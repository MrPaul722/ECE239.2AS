{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrPaul722/ECE239.2AS/blob/main/Language_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty9JG1yU8iQq"
      },
      "source": [
        "# Google Colab Setup\n",
        "\n",
        "Please run the code below to mount drive if you are running on colab.\n",
        "\n",
        "Please ignore if you are running on your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYjWAzEY8iQt",
        "outputId": "bd12d92a-6d9a-4ae4-a1c6-0eb5e45f4416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWXW_UXw8iQu",
        "outputId": "b6b4de63-7217-4c24-8d22-e017509c21be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ECE_239AS.2/Project3_skeleton\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/ECE_239AS.2/Project3_skeleton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAEyOQta8iQu"
      },
      "source": [
        "# Language Modeling and Transformers\n",
        "\n",
        "The project will consist of two broad parts.\n",
        "\n",
        "1. **Baseline Generative Language Model**: We will train a simple Bigram language model on the text data. We will use this model to generate a mini story.\n",
        "2. **Implementing Mini GPT**: We will implement a mini version of the GPT model layer by layer and attempt to train it on the text data. You will then load pretrained weights provided and generate a mini story."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3HpJ8Qu8iQu"
      },
      "source": [
        "## Some general instructions\n",
        "\n",
        "1. Please keep the name of layers consistent with what is requested in the `model.py` file for each layer, this helps us test in each function independently.\n",
        "2. Please check to see if the bias is to be set to false or true for all linear layers (it is mentioned in the doc string)\n",
        "3. As a general rule please read the docstring well, it contains information you will need to write the code.\n",
        "4. All configs are defined in `config.py` for the first part. While you are writing the code, do not change the values in the config file since we use them to test. Once you have passed all the tests please feel free to vary the parameter as you please.\n",
        "5. You will need to fill in `train.py` and run it to train the model. If you are running into memory issues please feel free to change the `batch_size` in the `config.py` file. If you are working on Colab please make sure to use the GPU runtime and feel free to copy over the training code to the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrb1WgXi8iQv",
        "outputId": "12603618-4787-4cdf-da8d-fc83d213ec24",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy torch tiktoken wandb einops # Install all required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ckg7TP-R8iQv"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hKjIFhc48iQv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "x6p_UqM68iQw"
      },
      "outputs": [],
      "source": [
        "from model import BigramLanguageModel, SingleHeadAttention, MultiHeadAttention, FeedForwardLayer, LayerNorm, TransformerLayer, MiniGPT\n",
        "from config import BigramConfig, MiniGPTConfig\n",
        "import tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gwOCwrD08iQw"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7ciPXO98iQw"
      },
      "outputs": [],
      "source": [
        "# If not provided, download from https://drive.google.com/file/d/1g09qUM9WibdfQVgkj6IAj8K2S3SGwc91/view?usp=sharing\n",
        "path_to_bigram_tester = \"/content/drive/MyDrive/ECE_239AS.2/Project3_skeleton/pretrained_models/MiniGPT/bigram_tester.pt\" # Load the bigram model with name bigram_tester.pt\n",
        "path_to_gpt_tester = \"/content/drive/MyDrive/ECE_239AS.2/Project3_skeleton/pretrained_models/MiniGPT/minigpt_tester.pt\" # Load the gpt model with name minigpt_tester.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcUEPtry8iQw"
      },
      "source": [
        "##  Bigram Language Model (10 points)\n",
        "\n",
        "A bigram language model is a type of probabilistic language model that predicts a word given the previous word in the sequence. The model is trained on a text corpus and learns the probability of a word given the previous word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTDBw1HT8iQw"
      },
      "source": [
        "### Implement the Bigram model (5 points)\n",
        "\n",
        "Please complete the `BigramLanguageModel` class in model.py. We will model a Bigram language model using a simple MLP with one hidden layer. The model will take in the previous word index and output the logits over the vocabulary for the next word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IkIT-C-c8iQx",
        "outputId": "753a5fa2-1ce7-49f8-8022-a167cc450f5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TEST CASE PASSED!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Test implementation for Bigram Language Model\n",
        "model = BigramLanguageModel(BigramConfig)\n",
        "tests.check_bigram(model, path_to_bigram_tester, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwtc9TLZ8iQx"
      },
      "source": [
        "### Training the Bigram Language Model (2.5 points)\n",
        "\n",
        "Complete the code in `train.py` to train the Bigram language model on the text data. Please provide plots for both the training and validation in the cell below.\n",
        "\n",
        "Some notes on the training process:\n",
        "\n",
        "1. You should be able to train the model slowly on your local machine.\n",
        "2. Training it on Colab will help with speed.\n",
        "3.  <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You should see it saturate to a value close to around 5-6 but as long as you see it decreasing then saturating you should be good.\n",
        "4. Please log the loss curves either on wandb, tensorboard or any other logger of your choice and please attach them below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tujKhn2K8iQx"
      },
      "outputs": [],
      "source": [
        "from train import solver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sSSFLBRY8iQx",
        "outputId": "63a78f3d-674f-43a5-a434-36b8308a30c1",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of trainable parameters: 3.27M\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">glamorous-universe-12</strong> at: <a href='https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3/runs/3mo71zob' target=\"_blank\">https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3/runs/3mo71zob</a><br> View project at: <a href='https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3' target=\"_blank\">https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250518_043809-3mo71zob/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/ECE_239AS.2/Project3_skeleton/wandb/run-20250518_052004-jnzpu99w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3/runs/jnzpu99w' target=\"_blank\">hearty-meadow-13</a></strong> to <a href='https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3' target=\"_blank\">https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3/runs/jnzpu99w' target=\"_blank\">https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3/runs/jnzpu99w</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 250100, Train Loss: 4.877995014190674 Eval Loss: 3.815951521396637\n",
            "Running evaluation at iteration 250200...\n",
            "Iteration 250200, Train Loss: 4.042382717132568 Eval Loss: 3.9448913979530333\n",
            "Running evaluation at iteration 250300...\n",
            "Iteration 250300, Train Loss: 4.452442169189453 Eval Loss: 3.8816947460174562\n",
            "Running evaluation at iteration 250400...\n",
            "Iteration 250400, Train Loss: 3.6545801162719727 Eval Loss: 3.9212216925621033\n",
            "Running evaluation at iteration 250500...\n",
            "Iteration 250500, Train Loss: 4.939520835876465 Eval Loss: 3.831873588562012\n",
            "Running evaluation at iteration 250600...\n",
            "Iteration 250600, Train Loss: 3.287876844406128 Eval Loss: 3.851326928138733\n",
            "Running evaluation at iteration 250700...\n",
            "Iteration 250700, Train Loss: 3.329890012741089 Eval Loss: 3.894927349090576\n",
            "Running evaluation at iteration 250800...\n",
            "Iteration 250800, Train Loss: 3.6534149646759033 Eval Loss: 3.85690212726593\n",
            "Running evaluation at iteration 250900...\n",
            "Iteration 250900, Train Loss: 4.082273960113525 Eval Loss: 3.8256999707221984\n",
            "Running evaluation at iteration 251000...\n",
            "Iteration 251000, Train Loss: 3.508448839187622 Eval Loss: 3.938881368637085\n",
            "Running evaluation at iteration 251100...\n",
            "Iteration 251100, Train Loss: 3.924231767654419 Eval Loss: 3.8656349778175354\n",
            "Running evaluation at iteration 251200...\n",
            "Iteration 251200, Train Loss: 4.304001808166504 Eval Loss: 3.9485334229469298\n",
            "Running evaluation at iteration 251300...\n",
            "Iteration 251300, Train Loss: 3.494633197784424 Eval Loss: 3.983271698951721\n",
            "Running evaluation at iteration 251400...\n",
            "Iteration 251400, Train Loss: 4.488081455230713 Eval Loss: 3.9220377516746523\n",
            "Running evaluation at iteration 251500...\n",
            "Iteration 251500, Train Loss: 3.5317156314849854 Eval Loss: 3.915010871887207\n",
            "Running evaluation at iteration 251600...\n",
            "Iteration 251600, Train Loss: 4.620927333831787 Eval Loss: 3.9282412934303284\n",
            "Running evaluation at iteration 251700...\n",
            "Iteration 251700, Train Loss: 3.4062626361846924 Eval Loss: 3.8055888056755065\n",
            "Running evaluation at iteration 251800...\n",
            "Iteration 251800, Train Loss: 3.43121075630188 Eval Loss: 3.8784522128105166\n",
            "Running evaluation at iteration 251900...\n",
            "Iteration 251900, Train Loss: 3.6404197216033936 Eval Loss: 3.8554407930374146\n",
            "Running evaluation at iteration 252000...\n",
            "Iteration 252000, Train Loss: 3.150602340698242 Eval Loss: 3.8490227365493777\n",
            "Running evaluation at iteration 252100...\n",
            "Iteration 252100, Train Loss: 4.059013366699219 Eval Loss: 3.826445155143738\n",
            "Running evaluation at iteration 252200...\n",
            "Iteration 252200, Train Loss: 4.0839924812316895 Eval Loss: 3.8594767355918886\n",
            "Running evaluation at iteration 252300...\n",
            "Iteration 252300, Train Loss: 3.716536045074463 Eval Loss: 3.969387753009796\n",
            "Running evaluation at iteration 252400...\n",
            "Iteration 252400, Train Loss: 3.5994653701782227 Eval Loss: 3.7859594893455504\n",
            "Running evaluation at iteration 252500...\n",
            "Iteration 252500, Train Loss: 3.572134017944336 Eval Loss: 3.879647822380066\n",
            "Running evaluation at iteration 252600...\n",
            "Iteration 252600, Train Loss: 3.645453453063965 Eval Loss: 3.9096872210502625\n",
            "Running evaluation at iteration 252700...\n",
            "Iteration 252700, Train Loss: 3.9955198764801025 Eval Loss: 3.8380031871795652\n",
            "Running evaluation at iteration 252800...\n",
            "Iteration 252800, Train Loss: 3.498868703842163 Eval Loss: 3.784247786998749\n",
            "Running evaluation at iteration 252900...\n",
            "Iteration 252900, Train Loss: 3.3099091053009033 Eval Loss: 3.9013077998161316\n",
            "Running evaluation at iteration 253000...\n",
            "Iteration 253000, Train Loss: 3.8300724029541016 Eval Loss: 3.9015359544754027\n",
            "Running evaluation at iteration 253100...\n",
            "Iteration 253100, Train Loss: 4.290009498596191 Eval Loss: 3.797109694480896\n",
            "Running evaluation at iteration 253200...\n",
            "Iteration 253200, Train Loss: 3.546046495437622 Eval Loss: 3.8338881659507753\n",
            "Running evaluation at iteration 253300...\n",
            "Iteration 253300, Train Loss: 4.263552665710449 Eval Loss: 3.8785050535202026\n",
            "Running evaluation at iteration 253400...\n",
            "Iteration 253400, Train Loss: 4.579208850860596 Eval Loss: 3.9665920090675355\n",
            "Running evaluation at iteration 253500...\n",
            "Iteration 253500, Train Loss: 4.013415336608887 Eval Loss: 3.8039195275306703\n",
            "Running evaluation at iteration 253600...\n",
            "Iteration 253600, Train Loss: 3.2970049381256104 Eval Loss: 3.905912778377533\n",
            "Running evaluation at iteration 253700...\n",
            "Iteration 253700, Train Loss: 4.447787284851074 Eval Loss: 3.8065832543373106\n",
            "Running evaluation at iteration 253800...\n",
            "Iteration 253800, Train Loss: 3.560181140899658 Eval Loss: 3.8326185083389284\n",
            "Running evaluation at iteration 253900...\n",
            "Iteration 253900, Train Loss: 4.997590065002441 Eval Loss: 3.9368736720085145\n",
            "Running evaluation at iteration 254000...\n",
            "Iteration 254000, Train Loss: 4.116478443145752 Eval Loss: 3.912733256816864\n",
            "Running evaluation at iteration 254100...\n",
            "Iteration 254100, Train Loss: 3.034808397293091 Eval Loss: 3.884576728343964\n",
            "Running evaluation at iteration 254200...\n",
            "Iteration 254200, Train Loss: 3.5815272331237793 Eval Loss: 3.8074171352386474\n",
            "Running evaluation at iteration 254300...\n",
            "Iteration 254300, Train Loss: 4.133163928985596 Eval Loss: 3.8929224944114686\n",
            "Running evaluation at iteration 254400...\n",
            "Iteration 254400, Train Loss: 4.201846122741699 Eval Loss: 3.9713366651535034\n",
            "Running evaluation at iteration 254500...\n",
            "Iteration 254500, Train Loss: 4.469066619873047 Eval Loss: 3.969432525634766\n",
            "Running evaluation at iteration 254600...\n",
            "Iteration 254600, Train Loss: 3.8218913078308105 Eval Loss: 3.945543622970581\n",
            "Running evaluation at iteration 254700...\n",
            "Iteration 254700, Train Loss: 3.0364866256713867 Eval Loss: 3.8623619627952577\n",
            "Running evaluation at iteration 254800...\n",
            "Iteration 254800, Train Loss: 3.6899025440216064 Eval Loss: 3.8753938364982603\n",
            "Running evaluation at iteration 254900...\n",
            "Iteration 254900, Train Loss: 4.224752426147461 Eval Loss: 3.9140519452095033\n",
            "Running evaluation at iteration 255000...\n",
            "Iteration 255000, Train Loss: 4.6360015869140625 Eval Loss: 3.9010147929191588\n",
            "Running evaluation at iteration 255100...\n",
            "Iteration 255100, Train Loss: 4.214844226837158 Eval Loss: 3.857447464466095\n",
            "Running evaluation at iteration 255200...\n",
            "Iteration 255200, Train Loss: 3.5729517936706543 Eval Loss: 3.9792434549331666\n",
            "Running evaluation at iteration 255300...\n",
            "Iteration 255300, Train Loss: 3.351743459701538 Eval Loss: 3.8564701008796693\n",
            "Running evaluation at iteration 255400...\n",
            "Iteration 255400, Train Loss: 4.295897960662842 Eval Loss: 3.885638313293457\n",
            "Running evaluation at iteration 255500...\n",
            "Iteration 255500, Train Loss: 4.17042875289917 Eval Loss: 3.924903039932251\n",
            "Running evaluation at iteration 255600...\n",
            "Iteration 255600, Train Loss: 4.3250932693481445 Eval Loss: 3.9140025997161865\n",
            "Running evaluation at iteration 255700...\n",
            "Iteration 255700, Train Loss: 4.38334321975708 Eval Loss: 3.8255522322654723\n",
            "Running evaluation at iteration 255800...\n",
            "Iteration 255800, Train Loss: 4.215127468109131 Eval Loss: 3.898286142349243\n",
            "Running evaluation at iteration 255900...\n",
            "Iteration 255900, Train Loss: 3.3305039405822754 Eval Loss: 3.8875205230712893\n",
            "Running evaluation at iteration 256000...\n",
            "Iteration 256000, Train Loss: 4.581331253051758 Eval Loss: 3.814852123260498\n",
            "Running evaluation at iteration 256100...\n",
            "Iteration 256100, Train Loss: 3.732187271118164 Eval Loss: 3.8369357442855834\n",
            "Running evaluation at iteration 256200...\n",
            "Iteration 256200, Train Loss: 3.9665846824645996 Eval Loss: 3.8214379858970644\n",
            "Running evaluation at iteration 256300...\n",
            "Iteration 256300, Train Loss: 4.48567533493042 Eval Loss: 3.8593515276908876\n",
            "Running evaluation at iteration 256400...\n",
            "Iteration 256400, Train Loss: 3.878464937210083 Eval Loss: 3.852514786720276\n",
            "Running evaluation at iteration 256500...\n",
            "Iteration 256500, Train Loss: 3.8018722534179688 Eval Loss: 3.8400657987594604\n",
            "Running evaluation at iteration 256600...\n",
            "Iteration 256600, Train Loss: 3.7458925247192383 Eval Loss: 3.8452962160110475\n",
            "Running evaluation at iteration 256700...\n",
            "Iteration 256700, Train Loss: 3.887394666671753 Eval Loss: 3.8564236545562744\n",
            "Running evaluation at iteration 256800...\n",
            "Iteration 256800, Train Loss: 4.603928565979004 Eval Loss: 3.799182550907135\n",
            "Running evaluation at iteration 256900...\n",
            "Iteration 256900, Train Loss: 4.689294815063477 Eval Loss: 3.7863112020492555\n",
            "Running evaluation at iteration 257000...\n",
            "Iteration 257000, Train Loss: 3.6321194171905518 Eval Loss: 3.9060793924331665\n",
            "Running evaluation at iteration 257100...\n",
            "Iteration 257100, Train Loss: 4.512197017669678 Eval Loss: 3.8813119459152223\n",
            "Running evaluation at iteration 257200...\n",
            "Iteration 257200, Train Loss: 3.7261829376220703 Eval Loss: 3.7971517848968506\n",
            "Running evaluation at iteration 257300...\n",
            "Iteration 257300, Train Loss: 3.831505298614502 Eval Loss: 3.9013514065742494\n",
            "Running evaluation at iteration 257400...\n",
            "Iteration 257400, Train Loss: 4.1493330001831055 Eval Loss: 3.8479398894309997\n",
            "Running evaluation at iteration 257500...\n",
            "Iteration 257500, Train Loss: 4.394733428955078 Eval Loss: 3.8142116928100585\n",
            "Running evaluation at iteration 257600...\n",
            "Iteration 257600, Train Loss: 3.732088565826416 Eval Loss: 3.8909839272499083\n",
            "Running evaluation at iteration 257700...\n",
            "Iteration 257700, Train Loss: 3.8214566707611084 Eval Loss: 3.775970323085785\n",
            "Running evaluation at iteration 257800...\n",
            "Iteration 257800, Train Loss: 3.954169988632202 Eval Loss: 3.8390072131156923\n",
            "Running evaluation at iteration 257900...\n",
            "Iteration 257900, Train Loss: 4.1948065757751465 Eval Loss: 3.9189450621604918\n",
            "Running evaluation at iteration 258000...\n",
            "Iteration 258000, Train Loss: 3.0900440216064453 Eval Loss: 3.828368377685547\n",
            "Running evaluation at iteration 258100...\n",
            "Iteration 258100, Train Loss: 3.3103559017181396 Eval Loss: 3.8932567596435548\n",
            "Running evaluation at iteration 258200...\n",
            "Iteration 258200, Train Loss: 3.71844220161438 Eval Loss: 3.8406354284286497\n",
            "Running evaluation at iteration 258300...\n",
            "Iteration 258300, Train Loss: 3.90354061126709 Eval Loss: 3.831818771362305\n",
            "Running evaluation at iteration 258400...\n",
            "Iteration 258400, Train Loss: 4.179542064666748 Eval Loss: 3.8639939641952514\n",
            "Running evaluation at iteration 258500...\n",
            "Iteration 258500, Train Loss: 3.326040267944336 Eval Loss: 3.8628410267829896\n",
            "Running evaluation at iteration 258600...\n",
            "Iteration 258600, Train Loss: 3.9911086559295654 Eval Loss: 3.8176130747795103\n",
            "Running evaluation at iteration 258700...\n",
            "Iteration 258700, Train Loss: 4.978914260864258 Eval Loss: 3.8031060814857485\n",
            "Running evaluation at iteration 258800...\n",
            "Iteration 258800, Train Loss: 3.456394672393799 Eval Loss: 3.7683301305770875\n",
            "Running evaluation at iteration 258900...\n",
            "Iteration 258900, Train Loss: 4.2856597900390625 Eval Loss: 3.8887912249565124\n",
            "Running evaluation at iteration 259000...\n",
            "Iteration 259000, Train Loss: 3.156796932220459 Eval Loss: 3.8652280497550966\n",
            "Running evaluation at iteration 259100...\n",
            "Iteration 259100, Train Loss: 3.976945161819458 Eval Loss: 3.8097030091285706\n",
            "Running evaluation at iteration 259200...\n",
            "Iteration 259200, Train Loss: 4.089101791381836 Eval Loss: 3.8187614130973815\n",
            "Running evaluation at iteration 259300...\n",
            "Iteration 259300, Train Loss: 4.287248611450195 Eval Loss: 3.836210091114044\n",
            "Running evaluation at iteration 259400...\n",
            "Iteration 259400, Train Loss: 3.812802791595459 Eval Loss: 3.906797137260437\n",
            "Running evaluation at iteration 259500...\n",
            "Iteration 259500, Train Loss: 4.346619129180908 Eval Loss: 3.8741632604599\n",
            "Running evaluation at iteration 259600...\n",
            "Iteration 259600, Train Loss: 3.8588991165161133 Eval Loss: 3.7918434286117555\n",
            "Running evaluation at iteration 259700...\n",
            "Iteration 259700, Train Loss: 3.4593687057495117 Eval Loss: 3.8496976613998415\n",
            "Running evaluation at iteration 259800...\n",
            "Iteration 259800, Train Loss: 3.8482582569122314 Eval Loss: 3.865969774723053\n",
            "Running evaluation at iteration 259900...\n",
            "Iteration 259900, Train Loss: 3.808929681777954 Eval Loss: 3.8864461731910707\n",
            "Running evaluation at iteration 260000...\n",
            "Iteration 260000, Train Loss: 5.148453712463379 Eval Loss: 3.915115852355957\n",
            "Running evaluation at iteration 260100...\n",
            "Iteration 260100, Train Loss: 2.8221707344055176 Eval Loss: 3.7910868239402773\n",
            "Running evaluation at iteration 260200...\n",
            "Iteration 260200, Train Loss: 4.113616466522217 Eval Loss: 3.965906283855438\n",
            "Running evaluation at iteration 260300...\n",
            "Iteration 260300, Train Loss: 4.503851413726807 Eval Loss: 3.8171311521530153\n",
            "Running evaluation at iteration 260400...\n",
            "Iteration 260400, Train Loss: 4.198598861694336 Eval Loss: 3.8743049097061157\n",
            "Running evaluation at iteration 260500...\n",
            "Iteration 260500, Train Loss: 3.3903355598449707 Eval Loss: 3.8512294483184815\n",
            "Running evaluation at iteration 260600...\n",
            "Iteration 260600, Train Loss: 3.0697951316833496 Eval Loss: 3.9175921869277954\n",
            "Running evaluation at iteration 260700...\n",
            "Iteration 260700, Train Loss: 3.7291595935821533 Eval Loss: 3.905145969390869\n",
            "Running evaluation at iteration 260800...\n",
            "Iteration 260800, Train Loss: 3.36871600151062 Eval Loss: 3.8698556351661684\n",
            "Running evaluation at iteration 260900...\n",
            "Iteration 260900, Train Loss: 3.9063286781311035 Eval Loss: 3.821949760913849\n",
            "Running evaluation at iteration 261000...\n",
            "Iteration 261000, Train Loss: 2.688408613204956 Eval Loss: 3.8998142862319947\n",
            "Running evaluation at iteration 261100...\n",
            "Iteration 261100, Train Loss: 3.427128553390503 Eval Loss: 3.8995959854125974\n",
            "Running evaluation at iteration 261200...\n",
            "Iteration 261200, Train Loss: 3.99762225151062 Eval Loss: 3.918987309932709\n",
            "Running evaluation at iteration 261300...\n",
            "Iteration 261300, Train Loss: 4.129319190979004 Eval Loss: 3.8305151653289795\n",
            "Running evaluation at iteration 261400...\n",
            "Iteration 261400, Train Loss: 3.543642997741699 Eval Loss: 3.851348421573639\n",
            "Running evaluation at iteration 261500...\n",
            "Iteration 261500, Train Loss: 4.330873012542725 Eval Loss: 3.9147291469573973\n",
            "Running evaluation at iteration 261600...\n",
            "Iteration 261600, Train Loss: 4.7352094650268555 Eval Loss: 3.8029205322265627\n",
            "Running evaluation at iteration 261700...\n",
            "Iteration 261700, Train Loss: 5.086754322052002 Eval Loss: 3.94659029006958\n",
            "Running evaluation at iteration 261800...\n",
            "Iteration 261800, Train Loss: 3.451502561569214 Eval Loss: 3.775440912246704\n",
            "Running evaluation at iteration 261900...\n",
            "Iteration 261900, Train Loss: 3.82716703414917 Eval Loss: 3.83566077709198\n",
            "Running evaluation at iteration 262000...\n",
            "Iteration 262000, Train Loss: 4.203597545623779 Eval Loss: 3.868599433898926\n",
            "Running evaluation at iteration 262100...\n",
            "Iteration 262100, Train Loss: 4.892086982727051 Eval Loss: 3.9432961320877076\n",
            "Running evaluation at iteration 262200...\n",
            "Iteration 262200, Train Loss: 4.1523261070251465 Eval Loss: 3.8212148118019105\n",
            "Running evaluation at iteration 262300...\n",
            "Iteration 262300, Train Loss: 4.203243255615234 Eval Loss: 3.909297113418579\n",
            "Running evaluation at iteration 262400...\n",
            "Iteration 262400, Train Loss: 4.937633514404297 Eval Loss: 3.8517142605781554\n",
            "Running evaluation at iteration 262500...\n",
            "Iteration 262500, Train Loss: 4.024489879608154 Eval Loss: 3.790773048400879\n",
            "Running evaluation at iteration 262600...\n",
            "Iteration 262600, Train Loss: 3.855802059173584 Eval Loss: 3.8320764446258546\n",
            "Running evaluation at iteration 262700...\n",
            "Iteration 262700, Train Loss: 4.671872138977051 Eval Loss: 3.9471353912353515\n",
            "Running evaluation at iteration 262800...\n",
            "Iteration 262800, Train Loss: 3.7560477256774902 Eval Loss: 3.9081421804428103\n",
            "Running evaluation at iteration 262900...\n",
            "Iteration 262900, Train Loss: 4.265124797821045 Eval Loss: 3.819241976737976\n",
            "Running evaluation at iteration 263000...\n",
            "Iteration 263000, Train Loss: 3.997765064239502 Eval Loss: 3.929991126060486\n",
            "Running evaluation at iteration 263100...\n",
            "Iteration 263100, Train Loss: 3.655681610107422 Eval Loss: 3.827102737426758\n",
            "Running evaluation at iteration 263200...\n",
            "Iteration 263200, Train Loss: 3.737245798110962 Eval Loss: 3.926103608608246\n",
            "Running evaluation at iteration 263300...\n",
            "Iteration 263300, Train Loss: 3.3412554264068604 Eval Loss: 3.903866968154907\n",
            "Running evaluation at iteration 263400...\n",
            "Iteration 263400, Train Loss: 4.616888046264648 Eval Loss: 3.8518283438682555\n",
            "Running evaluation at iteration 263500...\n",
            "Iteration 263500, Train Loss: 4.347986698150635 Eval Loss: 3.881886248588562\n",
            "Running evaluation at iteration 263600...\n",
            "Iteration 263600, Train Loss: 3.9905612468719482 Eval Loss: 3.851554324626923\n",
            "Running evaluation at iteration 263700...\n",
            "Iteration 263700, Train Loss: 3.7269656658172607 Eval Loss: 3.826744647026062\n",
            "Running evaluation at iteration 263800...\n",
            "Iteration 263800, Train Loss: 3.638082504272461 Eval Loss: 3.83742817401886\n",
            "Running evaluation at iteration 263900...\n",
            "Iteration 263900, Train Loss: 4.2981648445129395 Eval Loss: 3.920875563621521\n",
            "Running evaluation at iteration 264000...\n",
            "Iteration 264000, Train Loss: 4.149988174438477 Eval Loss: 3.9360093450546265\n",
            "Running evaluation at iteration 264100...\n",
            "Iteration 264100, Train Loss: 3.079773426055908 Eval Loss: 3.854880619049072\n",
            "Running evaluation at iteration 264200...\n",
            "Iteration 264200, Train Loss: 3.7561845779418945 Eval Loss: 3.9478339076042177\n",
            "Running evaluation at iteration 264300...\n",
            "Iteration 264300, Train Loss: 4.171789169311523 Eval Loss: 3.8982082653045653\n",
            "Running evaluation at iteration 264400...\n",
            "Iteration 264400, Train Loss: 3.6759910583496094 Eval Loss: 3.866635785102844\n",
            "Running evaluation at iteration 264500...\n",
            "Iteration 264500, Train Loss: 4.2435712814331055 Eval Loss: 3.9336490869522094\n",
            "Running evaluation at iteration 264600...\n",
            "Iteration 264600, Train Loss: 4.236515045166016 Eval Loss: 3.840908727645874\n",
            "Running evaluation at iteration 264700...\n",
            "Iteration 264700, Train Loss: 3.500988006591797 Eval Loss: 3.8330460739135743\n",
            "Running evaluation at iteration 264800...\n",
            "Iteration 264800, Train Loss: 4.409826278686523 Eval Loss: 3.8162381434440613\n",
            "Running evaluation at iteration 264900...\n",
            "Iteration 264900, Train Loss: 3.996511220932007 Eval Loss: 3.9118781328201293\n",
            "Running evaluation at iteration 265000...\n",
            "Iteration 265000, Train Loss: 3.727529764175415 Eval Loss: 4.011605315208435\n",
            "Running evaluation at iteration 265100...\n",
            "Iteration 265100, Train Loss: 3.3851914405822754 Eval Loss: 3.912337267398834\n",
            "Running evaluation at iteration 265200...\n",
            "Iteration 265200, Train Loss: 4.469385147094727 Eval Loss: 3.8188448476791383\n",
            "Running evaluation at iteration 265300...\n",
            "Iteration 265300, Train Loss: 4.8705220222473145 Eval Loss: 3.8161020922660827\n",
            "Running evaluation at iteration 265400...\n",
            "Iteration 265400, Train Loss: 4.071984767913818 Eval Loss: 3.862705638408661\n",
            "Running evaluation at iteration 265500...\n",
            "Iteration 265500, Train Loss: 4.5968708992004395 Eval Loss: 3.8300802731513977\n",
            "Running evaluation at iteration 265600...\n",
            "Iteration 265600, Train Loss: 2.4889137744903564 Eval Loss: 3.845635039806366\n",
            "Running evaluation at iteration 265700...\n",
            "Iteration 265700, Train Loss: 3.070786237716675 Eval Loss: 3.867521598339081\n",
            "Running evaluation at iteration 265800...\n",
            "Iteration 265800, Train Loss: 4.917908191680908 Eval Loss: 3.8473313784599306\n",
            "Running evaluation at iteration 265900...\n",
            "Iteration 265900, Train Loss: 4.1117730140686035 Eval Loss: 3.9151348185539248\n",
            "Running evaluation at iteration 266000...\n",
            "Iteration 266000, Train Loss: 3.9253015518188477 Eval Loss: 3.761128556728363\n",
            "Running evaluation at iteration 266100...\n",
            "Iteration 266100, Train Loss: 5.416745662689209 Eval Loss: 3.83471693277359\n",
            "Running evaluation at iteration 266200...\n",
            "Iteration 266200, Train Loss: 4.210788249969482 Eval Loss: 3.903467321395874\n",
            "Running evaluation at iteration 266300...\n",
            "Iteration 266300, Train Loss: 4.029459476470947 Eval Loss: 3.850402672290802\n",
            "Running evaluation at iteration 266400...\n",
            "Iteration 266400, Train Loss: 3.59633207321167 Eval Loss: 3.879882318973541\n",
            "Running evaluation at iteration 266500...\n",
            "Iteration 266500, Train Loss: 3.1000001430511475 Eval Loss: 3.9083316826820376\n",
            "Running evaluation at iteration 266600...\n",
            "Iteration 266600, Train Loss: 3.1735405921936035 Eval Loss: 3.7996205401420595\n",
            "Running evaluation at iteration 266700...\n",
            "Iteration 266700, Train Loss: 3.5885093212127686 Eval Loss: 3.8728928208351134\n",
            "Running evaluation at iteration 266800...\n",
            "Iteration 266800, Train Loss: 3.7258479595184326 Eval Loss: 3.8734058380126952\n",
            "Running evaluation at iteration 266900...\n",
            "Iteration 266900, Train Loss: 4.484811782836914 Eval Loss: 3.8423945140838622\n",
            "Running evaluation at iteration 267000...\n",
            "Iteration 267000, Train Loss: 3.352220058441162 Eval Loss: 3.893908369541168\n",
            "Running evaluation at iteration 267100...\n",
            "Iteration 267100, Train Loss: 4.108627796173096 Eval Loss: 3.8551627922058107\n",
            "Running evaluation at iteration 267200...\n",
            "Iteration 267200, Train Loss: 3.6706995964050293 Eval Loss: 3.7801481556892393\n",
            "Running evaluation at iteration 267300...\n",
            "Iteration 267300, Train Loss: 4.42166805267334 Eval Loss: 3.7796942806243896\n",
            "Running evaluation at iteration 267400...\n",
            "Iteration 267400, Train Loss: 4.2008562088012695 Eval Loss: 3.8669739389419555\n",
            "Running evaluation at iteration 267500...\n",
            "Iteration 267500, Train Loss: 3.144436836242676 Eval Loss: 3.7816201281547546\n",
            "Running evaluation at iteration 267600...\n",
            "Iteration 267600, Train Loss: 3.695103883743286 Eval Loss: 3.8747007036209107\n",
            "Running evaluation at iteration 267700...\n",
            "Iteration 267700, Train Loss: 2.691249132156372 Eval Loss: 3.8979380750656127\n",
            "Running evaluation at iteration 267800...\n",
            "Iteration 267800, Train Loss: 3.7986326217651367 Eval Loss: 3.848813683986664\n",
            "Running evaluation at iteration 267900...\n",
            "Iteration 267900, Train Loss: 3.1568105220794678 Eval Loss: 3.8769048738479612\n",
            "Running evaluation at iteration 268000...\n",
            "Iteration 268000, Train Loss: 3.3067498207092285 Eval Loss: 3.913824374675751\n",
            "Running evaluation at iteration 268100...\n",
            "Iteration 268100, Train Loss: 3.7403018474578857 Eval Loss: 3.8329937481880187\n",
            "Running evaluation at iteration 268200...\n",
            "Iteration 268200, Train Loss: 4.032825469970703 Eval Loss: 3.851282684803009\n",
            "Running evaluation at iteration 268300...\n",
            "Iteration 268300, Train Loss: 3.4527647495269775 Eval Loss: 3.929076955318451\n",
            "Running evaluation at iteration 268400...\n",
            "Iteration 268400, Train Loss: 4.54667329788208 Eval Loss: 3.7647914552688597\n",
            "Running evaluation at iteration 268500...\n",
            "Iteration 268500, Train Loss: 4.172966480255127 Eval Loss: 3.8234915494918824\n",
            "Running evaluation at iteration 268600...\n",
            "Iteration 268600, Train Loss: 3.895700216293335 Eval Loss: 3.789238533973694\n",
            "Running evaluation at iteration 268700...\n",
            "Iteration 268700, Train Loss: 4.2507452964782715 Eval Loss: 3.7666748571395874\n",
            "Running evaluation at iteration 268800...\n",
            "Iteration 268800, Train Loss: 4.314864158630371 Eval Loss: 3.8889239978790284\n",
            "Running evaluation at iteration 268900...\n",
            "Iteration 268900, Train Loss: 3.384990930557251 Eval Loss: 3.8319539761543275\n",
            "Running evaluation at iteration 269000...\n",
            "Iteration 269000, Train Loss: 3.9990975856781006 Eval Loss: 3.8832672595977784\n",
            "Running evaluation at iteration 269100...\n",
            "Iteration 269100, Train Loss: 4.769288063049316 Eval Loss: 3.8652304887771605\n",
            "Running evaluation at iteration 269200...\n",
            "Iteration 269200, Train Loss: 4.423909664154053 Eval Loss: 3.7607160925865175\n",
            "Running evaluation at iteration 269300...\n",
            "Iteration 269300, Train Loss: 4.243001461029053 Eval Loss: 3.8310075998306274\n",
            "Running evaluation at iteration 269400...\n",
            "Iteration 269400, Train Loss: 4.369420051574707 Eval Loss: 3.805315411090851\n",
            "Running evaluation at iteration 269500...\n",
            "Iteration 269500, Train Loss: 3.9447855949401855 Eval Loss: 3.9406169390678407\n",
            "Running evaluation at iteration 269600...\n",
            "Iteration 269600, Train Loss: 3.424266815185547 Eval Loss: 3.8630367302894593\n",
            "Running evaluation at iteration 269700...\n",
            "Iteration 269700, Train Loss: 3.9991893768310547 Eval Loss: 3.9304474210739135\n",
            "Running evaluation at iteration 269800...\n",
            "Iteration 269800, Train Loss: 3.580700397491455 Eval Loss: 3.8749952125549316\n",
            "Running evaluation at iteration 269900...\n",
            "Iteration 269900, Train Loss: 4.264603614807129 Eval Loss: 3.9577455615997312\n",
            "Running evaluation at iteration 270000...\n",
            "Iteration 270000, Train Loss: 3.6177964210510254 Eval Loss: 3.8379932951927187\n",
            "Running evaluation at iteration 270100...\n",
            "Iteration 270100, Train Loss: 4.02457857131958 Eval Loss: 3.937680196762085\n",
            "Running evaluation at iteration 270200...\n",
            "Iteration 270200, Train Loss: 2.7824647426605225 Eval Loss: 3.8639326214790346\n",
            "Running evaluation at iteration 270300...\n",
            "Iteration 270300, Train Loss: 3.3611862659454346 Eval Loss: 3.8601137042045592\n",
            "Running evaluation at iteration 270400...\n",
            "Iteration 270400, Train Loss: 3.5942821502685547 Eval Loss: 3.834087994098663\n",
            "Running evaluation at iteration 270500...\n",
            "Iteration 270500, Train Loss: 3.77060604095459 Eval Loss: 3.933773572444916\n",
            "Running evaluation at iteration 270600...\n",
            "Iteration 270600, Train Loss: 4.091219425201416 Eval Loss: 3.836357536315918\n",
            "Running evaluation at iteration 270700...\n",
            "Iteration 270700, Train Loss: 3.8264262676239014 Eval Loss: 3.924216933250427\n",
            "Running evaluation at iteration 270800...\n",
            "Iteration 270800, Train Loss: 4.095673561096191 Eval Loss: 3.922786605358124\n",
            "Running evaluation at iteration 270900...\n",
            "Iteration 270900, Train Loss: 4.856268882751465 Eval Loss: 3.7805555868148804\n",
            "Running evaluation at iteration 271000...\n",
            "Iteration 271000, Train Loss: 3.9032440185546875 Eval Loss: 3.8065564012527466\n",
            "Running evaluation at iteration 271100...\n",
            "Iteration 271100, Train Loss: 3.187810182571411 Eval Loss: 3.9264531421661375\n",
            "Running evaluation at iteration 271200...\n",
            "Iteration 271200, Train Loss: 4.194394111633301 Eval Loss: 3.9210157942771913\n",
            "Running evaluation at iteration 271300...\n",
            "Iteration 271300, Train Loss: 4.13791036605835 Eval Loss: 3.8568414568901064\n",
            "Running evaluation at iteration 271400...\n",
            "Iteration 271400, Train Loss: 4.124037265777588 Eval Loss: 3.8059588503837585\n",
            "Running evaluation at iteration 271500...\n",
            "Iteration 271500, Train Loss: 3.800849676132202 Eval Loss: 3.8255368280410766\n",
            "Running evaluation at iteration 271600...\n",
            "Iteration 271600, Train Loss: 3.5401957035064697 Eval Loss: 3.834332082271576\n",
            "Running evaluation at iteration 271700...\n",
            "Iteration 271700, Train Loss: 3.7948625087738037 Eval Loss: 3.7990074706077577\n",
            "Running evaluation at iteration 271800...\n",
            "Iteration 271800, Train Loss: 4.390905380249023 Eval Loss: 3.8133567643165587\n",
            "Running evaluation at iteration 271900...\n",
            "Iteration 271900, Train Loss: 3.0682873725891113 Eval Loss: 3.854620678424835\n",
            "Running evaluation at iteration 272000...\n",
            "Iteration 272000, Train Loss: 3.686265230178833 Eval Loss: 3.8146860885620115\n",
            "Running evaluation at iteration 272100...\n",
            "Iteration 272100, Train Loss: 3.382265090942383 Eval Loss: 3.8856320714950563\n",
            "Running evaluation at iteration 272200...\n",
            "Iteration 272200, Train Loss: 3.366145610809326 Eval Loss: 3.9259930658340454\n",
            "Running evaluation at iteration 272300...\n",
            "Iteration 272300, Train Loss: 4.249299049377441 Eval Loss: 3.8023280262947083\n",
            "Running evaluation at iteration 272400...\n",
            "Iteration 272400, Train Loss: 4.3620476722717285 Eval Loss: 3.810660572052002\n",
            "Running evaluation at iteration 272500...\n",
            "Iteration 272500, Train Loss: 4.533503532409668 Eval Loss: 3.9097752928733827\n",
            "Running evaluation at iteration 272600...\n",
            "Iteration 272600, Train Loss: 4.172613143920898 Eval Loss: 3.8321566438674926\n",
            "Running evaluation at iteration 272700...\n",
            "Iteration 272700, Train Loss: 4.045614719390869 Eval Loss: 3.9042108774185182\n",
            "Running evaluation at iteration 272800...\n",
            "Iteration 272800, Train Loss: 3.6401782035827637 Eval Loss: 3.7670160150527954\n",
            "Running evaluation at iteration 272900...\n",
            "Iteration 272900, Train Loss: 4.088723182678223 Eval Loss: 3.858026971817017\n",
            "Running evaluation at iteration 273000...\n",
            "Iteration 273000, Train Loss: 3.2900590896606445 Eval Loss: 3.8976124143600464\n",
            "Running evaluation at iteration 273100...\n",
            "Iteration 273100, Train Loss: 4.261396408081055 Eval Loss: 3.8039823079109194\n",
            "Running evaluation at iteration 273200...\n",
            "Iteration 273200, Train Loss: 3.6967508792877197 Eval Loss: 3.9206186509132386\n",
            "Running evaluation at iteration 273300...\n",
            "Iteration 273300, Train Loss: 4.601685523986816 Eval Loss: 3.8257540798187257\n",
            "Running evaluation at iteration 273400...\n",
            "Iteration 273400, Train Loss: 3.277686357498169 Eval Loss: 3.8906782031059266\n",
            "Running evaluation at iteration 273500...\n",
            "Iteration 273500, Train Loss: 4.562249660491943 Eval Loss: 3.844827833175659\n",
            "Running evaluation at iteration 273600...\n",
            "Iteration 273600, Train Loss: 3.5643622875213623 Eval Loss: 3.8342217230796813\n",
            "Running evaluation at iteration 273700...\n",
            "Iteration 273700, Train Loss: 4.7147064208984375 Eval Loss: 3.8637030792236327\n",
            "Running evaluation at iteration 273800...\n",
            "Iteration 273800, Train Loss: 3.747945785522461 Eval Loss: 3.7989582896232603\n",
            "Running evaluation at iteration 273900...\n",
            "Iteration 273900, Train Loss: 3.965104341506958 Eval Loss: 3.830718162059784\n",
            "Running evaluation at iteration 274000...\n",
            "Iteration 274000, Train Loss: 3.5427024364471436 Eval Loss: 3.919797160625458\n",
            "Running evaluation at iteration 274100...\n",
            "Iteration 274100, Train Loss: 4.22609806060791 Eval Loss: 3.9668178296089174\n",
            "Running evaluation at iteration 274200...\n",
            "Iteration 274200, Train Loss: 4.302051067352295 Eval Loss: 3.8488968634605407\n",
            "Running evaluation at iteration 274300...\n",
            "Iteration 274300, Train Loss: 4.164513111114502 Eval Loss: 3.826797490119934\n",
            "Running evaluation at iteration 274400...\n",
            "Iteration 274400, Train Loss: 3.487154245376587 Eval Loss: 3.8732715702056884\n",
            "Running evaluation at iteration 274500...\n",
            "Iteration 274500, Train Loss: 4.071191787719727 Eval Loss: 3.757939691543579\n",
            "Running evaluation at iteration 274600...\n",
            "Iteration 274600, Train Loss: 3.9959402084350586 Eval Loss: 3.875065050125122\n",
            "Running evaluation at iteration 274700...\n",
            "Iteration 274700, Train Loss: 4.1913652420043945 Eval Loss: 3.9271195125579834\n",
            "Running evaluation at iteration 274800...\n",
            "Iteration 274800, Train Loss: 3.734731912612915 Eval Loss: 3.8011253809928895\n",
            "Running evaluation at iteration 274900...\n",
            "Iteration 274900, Train Loss: 3.4338021278381348 Eval Loss: 3.883041944503784\n",
            "Running evaluation at iteration 275000...\n",
            "Iteration 275000, Train Loss: 3.49897837638855 Eval Loss: 3.8959778356552124\n",
            "Running evaluation at iteration 275100...\n",
            "Iteration 275100, Train Loss: 4.168200969696045 Eval Loss: 3.874625506401062\n",
            "Running evaluation at iteration 275200...\n",
            "Iteration 275200, Train Loss: 3.471169948577881 Eval Loss: 3.8421295857429505\n",
            "Running evaluation at iteration 275300...\n",
            "Iteration 275300, Train Loss: 3.5382299423217773 Eval Loss: 3.9557277083396913\n",
            "Running evaluation at iteration 275400...\n",
            "Iteration 275400, Train Loss: 3.5865023136138916 Eval Loss: 3.8181917786598207\n",
            "Running evaluation at iteration 275500...\n",
            "Iteration 275500, Train Loss: 3.262943744659424 Eval Loss: 3.8618662118911744\n",
            "Running evaluation at iteration 275600...\n",
            "Iteration 275600, Train Loss: 4.000628471374512 Eval Loss: 3.8471800875663758\n",
            "Running evaluation at iteration 275700...\n",
            "Iteration 275700, Train Loss: 3.1904397010803223 Eval Loss: 4.001934571266174\n",
            "Running evaluation at iteration 275800...\n",
            "Iteration 275800, Train Loss: 3.177940845489502 Eval Loss: 3.765304729938507\n",
            "Running evaluation at iteration 275900...\n",
            "Iteration 275900, Train Loss: 3.9843456745147705 Eval Loss: 3.83134446144104\n",
            "Running evaluation at iteration 276000...\n",
            "Iteration 276000, Train Loss: 3.3879623413085938 Eval Loss: 3.8329475474357606\n",
            "Running evaluation at iteration 276100...\n",
            "Iteration 276100, Train Loss: 3.6423094272613525 Eval Loss: 3.954471483230591\n",
            "Running evaluation at iteration 276200...\n",
            "Iteration 276200, Train Loss: 4.854017734527588 Eval Loss: 3.9155203080177308\n",
            "Running evaluation at iteration 276300...\n",
            "Iteration 276300, Train Loss: 3.848034381866455 Eval Loss: 3.8007560920715333\n",
            "Running evaluation at iteration 276400...\n",
            "Iteration 276400, Train Loss: 4.188615322113037 Eval Loss: 3.9282293343544006\n",
            "Running evaluation at iteration 276500...\n",
            "Iteration 276500, Train Loss: 3.277338981628418 Eval Loss: 3.820061559677124\n",
            "Running evaluation at iteration 276600...\n",
            "Iteration 276600, Train Loss: 3.8971376419067383 Eval Loss: 3.867951469421387\n",
            "Running evaluation at iteration 276700...\n",
            "Iteration 276700, Train Loss: 4.4220051765441895 Eval Loss: 3.8996320152282715\n",
            "Running evaluation at iteration 276800...\n",
            "Iteration 276800, Train Loss: 3.615574359893799 Eval Loss: 3.8331451535224916\n",
            "Running evaluation at iteration 276900...\n",
            "Iteration 276900, Train Loss: 4.847461700439453 Eval Loss: 3.771705057621002\n",
            "Running evaluation at iteration 277000...\n",
            "Iteration 277000, Train Loss: 3.57053804397583 Eval Loss: 3.8189332389831545\n",
            "Running evaluation at iteration 277100...\n",
            "Iteration 277100, Train Loss: 4.421146392822266 Eval Loss: 3.831235067844391\n",
            "Running evaluation at iteration 277200...\n",
            "Iteration 277200, Train Loss: 4.586974143981934 Eval Loss: 3.8145192575454714\n",
            "Running evaluation at iteration 277300...\n",
            "Iteration 277300, Train Loss: 3.3861806392669678 Eval Loss: 3.880289192199707\n",
            "Running evaluation at iteration 277400...\n",
            "Iteration 277400, Train Loss: 4.099294662475586 Eval Loss: 3.8543095850944518\n",
            "Running evaluation at iteration 277500...\n",
            "Iteration 277500, Train Loss: 3.7155942916870117 Eval Loss: 3.8228492641448977\n",
            "Running evaluation at iteration 277600...\n",
            "Iteration 277600, Train Loss: 4.104525566101074 Eval Loss: 3.8307325315475462\n",
            "Running evaluation at iteration 277700...\n",
            "Iteration 277700, Train Loss: 3.9395599365234375 Eval Loss: 3.7861790323257445\n",
            "Running evaluation at iteration 277800...\n",
            "Iteration 277800, Train Loss: 3.7698841094970703 Eval Loss: 3.7963217282295227\n",
            "Running evaluation at iteration 277900...\n",
            "Iteration 277900, Train Loss: 3.588810682296753 Eval Loss: 3.8664573001861573\n",
            "Running evaluation at iteration 278000...\n",
            "Iteration 278000, Train Loss: 4.696613788604736 Eval Loss: 3.8543039727211\n",
            "Running evaluation at iteration 278100...\n",
            "Iteration 278100, Train Loss: 3.6944150924682617 Eval Loss: 3.8730100440979003\n",
            "Running evaluation at iteration 278200...\n",
            "Iteration 278200, Train Loss: 4.2700042724609375 Eval Loss: 3.8798927068710327\n",
            "Running evaluation at iteration 278300...\n",
            "Iteration 278300, Train Loss: 3.3137590885162354 Eval Loss: 3.8859654760360716\n",
            "Running evaluation at iteration 278400...\n",
            "Iteration 278400, Train Loss: 3.858860969543457 Eval Loss: 3.907047681808472\n",
            "Running evaluation at iteration 278500...\n",
            "Iteration 278500, Train Loss: 3.2230279445648193 Eval Loss: 3.9519401741027833\n",
            "Running evaluation at iteration 278600...\n",
            "Iteration 278600, Train Loss: 3.978351354598999 Eval Loss: 3.7818260860443114\n",
            "Running evaluation at iteration 278700...\n",
            "Iteration 278700, Train Loss: 3.2005937099456787 Eval Loss: 3.878270571231842\n",
            "Running evaluation at iteration 278800...\n",
            "Iteration 278800, Train Loss: 3.425691604614258 Eval Loss: 3.8610736417770384\n",
            "Running evaluation at iteration 278900...\n",
            "Iteration 278900, Train Loss: 2.988013982772827 Eval Loss: 3.8080749893188477\n",
            "Running evaluation at iteration 279000...\n",
            "Iteration 279000, Train Loss: 4.082878112792969 Eval Loss: 3.874082441329956\n",
            "Running evaluation at iteration 279100...\n",
            "Iteration 279100, Train Loss: 3.1315486431121826 Eval Loss: 3.823709478378296\n",
            "Running evaluation at iteration 279200...\n",
            "Iteration 279200, Train Loss: 4.76693058013916 Eval Loss: 3.880174732208252\n",
            "Running evaluation at iteration 279300...\n",
            "Iteration 279300, Train Loss: 3.6858153343200684 Eval Loss: 3.854973728656769\n",
            "Running evaluation at iteration 279400...\n",
            "Iteration 279400, Train Loss: 3.3278136253356934 Eval Loss: 3.874106755256653\n",
            "Running evaluation at iteration 279500...\n",
            "Iteration 279500, Train Loss: 4.458742618560791 Eval Loss: 3.859884328842163\n",
            "Running evaluation at iteration 279600...\n",
            "Iteration 279600, Train Loss: 3.8474724292755127 Eval Loss: 3.8154617404937743\n",
            "Running evaluation at iteration 279700...\n",
            "Iteration 279700, Train Loss: 5.390757083892822 Eval Loss: 3.7943183541297913\n",
            "Running evaluation at iteration 279800...\n",
            "Iteration 279800, Train Loss: 3.2746994495391846 Eval Loss: 3.778587155342102\n",
            "Running evaluation at iteration 279900...\n",
            "Iteration 279900, Train Loss: 3.811971426010132 Eval Loss: 3.8558495473861694\n",
            "Running evaluation at iteration 280000...\n",
            "Iteration 280000, Train Loss: 4.135879039764404 Eval Loss: 3.9458797001838684\n",
            "Running evaluation at iteration 280100...\n",
            "Iteration 280100, Train Loss: 3.7454073429107666 Eval Loss: 3.8546054100990297\n",
            "Running evaluation at iteration 280200...\n",
            "Iteration 280200, Train Loss: 5.060819149017334 Eval Loss: 3.8977464175224306\n",
            "Running evaluation at iteration 280300...\n",
            "Iteration 280300, Train Loss: 4.280990123748779 Eval Loss: 3.944601123332977\n",
            "Running evaluation at iteration 280400...\n",
            "Iteration 280400, Train Loss: 3.00262713432312 Eval Loss: 3.802108862400055\n",
            "Running evaluation at iteration 280500...\n",
            "Iteration 280500, Train Loss: 5.088510036468506 Eval Loss: 3.8516931414604185\n",
            "Running evaluation at iteration 280600...\n",
            "Iteration 280600, Train Loss: 3.7052161693573 Eval Loss: 3.8584716081619264\n",
            "Running evaluation at iteration 280700...\n",
            "Iteration 280700, Train Loss: 3.5575616359710693 Eval Loss: 3.8908465266227723\n",
            "Running evaluation at iteration 280800...\n",
            "Iteration 280800, Train Loss: 4.178668022155762 Eval Loss: 3.777564830780029\n",
            "Running evaluation at iteration 280900...\n",
            "Iteration 280900, Train Loss: 3.7910425662994385 Eval Loss: 3.9268476510047914\n",
            "Running evaluation at iteration 281000...\n",
            "Iteration 281000, Train Loss: 3.851130247116089 Eval Loss: 3.9081195187568665\n",
            "Running evaluation at iteration 281100...\n",
            "Iteration 281100, Train Loss: 3.1159729957580566 Eval Loss: 3.8998564982414248\n",
            "Running evaluation at iteration 281200...\n",
            "Iteration 281200, Train Loss: 4.067344665527344 Eval Loss: 3.841747908592224\n",
            "Running evaluation at iteration 281300...\n",
            "Iteration 281300, Train Loss: 3.487922430038452 Eval Loss: 3.84820463180542\n",
            "Running evaluation at iteration 281400...\n",
            "Iteration 281400, Train Loss: 3.814878225326538 Eval Loss: 3.8841523814201353\n",
            "Running evaluation at iteration 281500...\n",
            "Iteration 281500, Train Loss: 3.582740545272827 Eval Loss: 3.8388957929611207\n",
            "Running evaluation at iteration 281600...\n",
            "Iteration 281600, Train Loss: 4.190997123718262 Eval Loss: 3.8018559885025023\n",
            "Running evaluation at iteration 281700...\n",
            "Iteration 281700, Train Loss: 4.066558361053467 Eval Loss: 3.8065913367271422\n",
            "Running evaluation at iteration 281800...\n",
            "Iteration 281800, Train Loss: 4.254111289978027 Eval Loss: 3.822046370506287\n",
            "Running evaluation at iteration 281900...\n",
            "Iteration 281900, Train Loss: 3.9579803943634033 Eval Loss: 3.883640241622925\n",
            "Running evaluation at iteration 282000...\n",
            "Iteration 282000, Train Loss: 4.235461711883545 Eval Loss: 3.8513417291641234\n",
            "Running evaluation at iteration 282100...\n",
            "Iteration 282100, Train Loss: 4.262096405029297 Eval Loss: 3.9126194167137145\n",
            "Running evaluation at iteration 282200...\n",
            "Iteration 282200, Train Loss: 3.295504570007324 Eval Loss: 3.9371142888069155\n",
            "Running evaluation at iteration 282300...\n",
            "Iteration 282300, Train Loss: 3.7247674465179443 Eval Loss: 3.824857692718506\n",
            "Running evaluation at iteration 282400...\n",
            "Iteration 282400, Train Loss: 4.346096992492676 Eval Loss: 3.867759680747986\n",
            "Running evaluation at iteration 282500...\n",
            "Iteration 282500, Train Loss: 3.871232748031616 Eval Loss: 3.7524442648887635\n",
            "Running evaluation at iteration 282600...\n",
            "Iteration 282600, Train Loss: 3.892822504043579 Eval Loss: 3.835518319606781\n",
            "Running evaluation at iteration 282700...\n",
            "Iteration 282700, Train Loss: 3.7946338653564453 Eval Loss: 3.8058184599876403\n",
            "Running evaluation at iteration 282800...\n",
            "Iteration 282800, Train Loss: 4.249973297119141 Eval Loss: 3.8537148690223693\n",
            "Running evaluation at iteration 282900...\n",
            "Iteration 282900, Train Loss: 3.89475154876709 Eval Loss: 3.8521331763267517\n",
            "Running evaluation at iteration 283000...\n",
            "Iteration 283000, Train Loss: 4.148258686065674 Eval Loss: 3.8432703113555906\n",
            "Running evaluation at iteration 283100...\n",
            "Iteration 283100, Train Loss: 4.270442485809326 Eval Loss: 3.7469541907310484\n",
            "Running evaluation at iteration 283200...\n",
            "Iteration 283200, Train Loss: 4.504560947418213 Eval Loss: 3.89464084148407\n",
            "Running evaluation at iteration 283300...\n",
            "Iteration 283300, Train Loss: 5.080873966217041 Eval Loss: 3.748901062011719\n",
            "Running evaluation at iteration 283400...\n",
            "Iteration 283400, Train Loss: 4.528857707977295 Eval Loss: 3.8765112233161925\n",
            "Running evaluation at iteration 283500...\n",
            "Iteration 283500, Train Loss: 4.346537113189697 Eval Loss: 3.8258249974250793\n",
            "Running evaluation at iteration 283600...\n",
            "Iteration 283600, Train Loss: 3.68146014213562 Eval Loss: 3.9572166633605956\n",
            "Running evaluation at iteration 283700...\n",
            "Iteration 283700, Train Loss: 3.5247011184692383 Eval Loss: 3.777852547168732\n",
            "Running evaluation at iteration 283800...\n",
            "Iteration 283800, Train Loss: 3.4123122692108154 Eval Loss: 3.8492102909088133\n",
            "Running evaluation at iteration 283900...\n",
            "Iteration 283900, Train Loss: 4.227543354034424 Eval Loss: 3.7894713711738586\n",
            "Running evaluation at iteration 284000...\n",
            "Iteration 284000, Train Loss: 3.8408355712890625 Eval Loss: 3.8342531371116637\n",
            "Running evaluation at iteration 284100...\n",
            "Iteration 284100, Train Loss: 3.218907356262207 Eval Loss: 3.9122747707366945\n",
            "Running evaluation at iteration 284200...\n",
            "Iteration 284200, Train Loss: 4.010530948638916 Eval Loss: 3.7714704346656798\n",
            "Running evaluation at iteration 284300...\n",
            "Iteration 284300, Train Loss: 4.829536437988281 Eval Loss: 3.78061842918396\n",
            "Running evaluation at iteration 284400...\n",
            "Iteration 284400, Train Loss: 3.628403663635254 Eval Loss: 3.939205150604248\n",
            "Running evaluation at iteration 284500...\n",
            "Iteration 284500, Train Loss: 3.3398406505584717 Eval Loss: 3.886414785385132\n",
            "Running evaluation at iteration 284600...\n",
            "Iteration 284600, Train Loss: 4.600186824798584 Eval Loss: 3.872455916404724\n",
            "Running evaluation at iteration 284700...\n",
            "Iteration 284700, Train Loss: 3.94252610206604 Eval Loss: 3.8750817251205443\n",
            "Running evaluation at iteration 284800...\n",
            "Iteration 284800, Train Loss: 3.9629201889038086 Eval Loss: 3.9307138991355894\n",
            "Running evaluation at iteration 284900...\n",
            "Iteration 284900, Train Loss: 3.203887939453125 Eval Loss: 3.8395596718788148\n",
            "Running evaluation at iteration 285000...\n",
            "Iteration 285000, Train Loss: 4.4720139503479 Eval Loss: 3.795608115196228\n",
            "Running evaluation at iteration 285100...\n",
            "Iteration 285100, Train Loss: 4.436995029449463 Eval Loss: 3.8723342633247375\n",
            "Running evaluation at iteration 285200...\n",
            "Iteration 285200, Train Loss: 3.226353645324707 Eval Loss: 3.8502591466903686\n",
            "Running evaluation at iteration 285300...\n",
            "Iteration 285300, Train Loss: 4.019537448883057 Eval Loss: 3.828664793968201\n",
            "Running evaluation at iteration 285400...\n",
            "Iteration 285400, Train Loss: 3.1104538440704346 Eval Loss: 3.7747138142585754\n",
            "Running evaluation at iteration 285500...\n",
            "Iteration 285500, Train Loss: 3.6272871494293213 Eval Loss: 3.849112868309021\n",
            "Running evaluation at iteration 285600...\n",
            "Iteration 285600, Train Loss: 5.199975967407227 Eval Loss: 3.856352274417877\n",
            "Running evaluation at iteration 285700...\n",
            "Iteration 285700, Train Loss: 4.335617542266846 Eval Loss: 3.8776063990592955\n",
            "Running evaluation at iteration 285800...\n",
            "Iteration 285800, Train Loss: 4.2664055824279785 Eval Loss: 3.816379871368408\n",
            "Running evaluation at iteration 285900...\n",
            "Iteration 285900, Train Loss: 3.041891574859619 Eval Loss: 3.8280997323989867\n",
            "Running evaluation at iteration 286000...\n",
            "Iteration 286000, Train Loss: 3.5677647590637207 Eval Loss: 3.859071729183197\n",
            "Running evaluation at iteration 286100...\n",
            "Iteration 286100, Train Loss: 4.713113307952881 Eval Loss: 3.831508741378784\n",
            "Running evaluation at iteration 286200...\n",
            "Iteration 286200, Train Loss: 4.789069652557373 Eval Loss: 3.9475401067733764\n",
            "Running evaluation at iteration 286300...\n",
            "Iteration 286300, Train Loss: 3.563135862350464 Eval Loss: 3.8063429975509644\n",
            "Running evaluation at iteration 286400...\n",
            "Iteration 286400, Train Loss: 3.246260404586792 Eval Loss: 3.9078301548957826\n",
            "Running evaluation at iteration 286500...\n",
            "Iteration 286500, Train Loss: 3.899714231491089 Eval Loss: 3.935532741546631\n",
            "Running evaluation at iteration 286600...\n",
            "Iteration 286600, Train Loss: 4.127096652984619 Eval Loss: 3.9061620807647706\n",
            "Running evaluation at iteration 286700...\n",
            "Iteration 286700, Train Loss: 4.268892765045166 Eval Loss: 3.7651400995254516\n",
            "Running evaluation at iteration 286800...\n",
            "Iteration 286800, Train Loss: 4.03140926361084 Eval Loss: 3.932686243057251\n",
            "Running evaluation at iteration 286900...\n",
            "Iteration 286900, Train Loss: 4.491800308227539 Eval Loss: 3.8953945660591125\n",
            "Running evaluation at iteration 287000...\n",
            "Iteration 287000, Train Loss: 4.53687047958374 Eval Loss: 3.847347846031189\n",
            "Running evaluation at iteration 287100...\n",
            "Iteration 287100, Train Loss: 3.9770395755767822 Eval Loss: 3.8063716292381287\n",
            "Running evaluation at iteration 287200...\n",
            "Iteration 287200, Train Loss: 4.052069187164307 Eval Loss: 3.912939305305481\n",
            "Running evaluation at iteration 287300...\n",
            "Iteration 287300, Train Loss: 3.5231873989105225 Eval Loss: 3.7934401273727416\n",
            "Running evaluation at iteration 287400...\n",
            "Iteration 287400, Train Loss: 3.3561501502990723 Eval Loss: 3.7237157583236695\n",
            "Running evaluation at iteration 287500...\n",
            "Iteration 287500, Train Loss: 3.076598644256592 Eval Loss: 3.9905413937568666\n",
            "Running evaluation at iteration 287600...\n",
            "Iteration 287600, Train Loss: 3.6661112308502197 Eval Loss: 3.817386634349823\n",
            "Running evaluation at iteration 287700...\n",
            "Iteration 287700, Train Loss: 4.154172420501709 Eval Loss: 3.8225016736984254\n",
            "Running evaluation at iteration 287800...\n",
            "Iteration 287800, Train Loss: 3.454263210296631 Eval Loss: 3.883209743499756\n",
            "Running evaluation at iteration 287900...\n",
            "Iteration 287900, Train Loss: 4.5443339347839355 Eval Loss: 3.9044767308235167\n",
            "Running evaluation at iteration 288000...\n",
            "Iteration 288000, Train Loss: 4.0511579513549805 Eval Loss: 3.793697934150696\n",
            "Running evaluation at iteration 288100...\n",
            "Iteration 288100, Train Loss: 3.9951534271240234 Eval Loss: 3.843446853160858\n",
            "Running evaluation at iteration 288200...\n",
            "Iteration 288200, Train Loss: 3.9200282096862793 Eval Loss: 3.8077466702461242\n",
            "Running evaluation at iteration 288300...\n",
            "Iteration 288300, Train Loss: 3.849940061569214 Eval Loss: 3.8715172624588012\n",
            "Running evaluation at iteration 288400...\n",
            "Iteration 288400, Train Loss: 3.9094395637512207 Eval Loss: 3.847521924972534\n",
            "Running evaluation at iteration 288500...\n",
            "Iteration 288500, Train Loss: 3.4856505393981934 Eval Loss: 3.8744903874397276\n",
            "Running evaluation at iteration 288600...\n",
            "Iteration 288600, Train Loss: 3.8069968223571777 Eval Loss: 3.9077212834358215\n",
            "Running evaluation at iteration 288700...\n",
            "Iteration 288700, Train Loss: 3.1237950325012207 Eval Loss: 3.8215300631523133\n",
            "Running evaluation at iteration 288800...\n",
            "Iteration 288800, Train Loss: 2.7833926677703857 Eval Loss: 3.8172652554512023\n",
            "Running evaluation at iteration 288900...\n",
            "Iteration 288900, Train Loss: 3.3026211261749268 Eval Loss: 3.865592448711395\n",
            "Running evaluation at iteration 289000...\n",
            "Iteration 289000, Train Loss: 5.300546646118164 Eval Loss: 3.921809494495392\n",
            "Running evaluation at iteration 289100...\n",
            "Iteration 289100, Train Loss: 4.114872932434082 Eval Loss: 3.8613815093040467\n",
            "Running evaluation at iteration 289200...\n",
            "Iteration 289200, Train Loss: 3.782606363296509 Eval Loss: 3.9717744159698487\n",
            "Running evaluation at iteration 289300...\n",
            "Iteration 289300, Train Loss: 4.307178974151611 Eval Loss: 3.835143883228302\n",
            "Running evaluation at iteration 289400...\n",
            "Iteration 289400, Train Loss: 4.8483991622924805 Eval Loss: 3.740295443534851\n",
            "Running evaluation at iteration 289500...\n",
            "Iteration 289500, Train Loss: 3.4864182472229004 Eval Loss: 3.833184735774994\n",
            "Running evaluation at iteration 289600...\n",
            "Iteration 289600, Train Loss: 3.569114923477173 Eval Loss: 3.8329692482948303\n",
            "Running evaluation at iteration 289700...\n",
            "Iteration 289700, Train Loss: 4.097794055938721 Eval Loss: 3.830974431037903\n",
            "Running evaluation at iteration 289800...\n",
            "Iteration 289800, Train Loss: 3.9889111518859863 Eval Loss: 3.8579283237457274\n",
            "Running evaluation at iteration 289900...\n",
            "Iteration 289900, Train Loss: 3.7244272232055664 Eval Loss: 3.8660865211486817\n",
            "Running evaluation at iteration 290000...\n",
            "Iteration 290000, Train Loss: 4.554084777832031 Eval Loss: 3.905050525665283\n",
            "Running evaluation at iteration 290100...\n",
            "Iteration 290100, Train Loss: 4.262946128845215 Eval Loss: 3.8856292510032655\n",
            "Running evaluation at iteration 290200...\n",
            "Iteration 290200, Train Loss: 3.9009299278259277 Eval Loss: 3.732467098236084\n",
            "Running evaluation at iteration 290300...\n",
            "Iteration 290300, Train Loss: 4.325344085693359 Eval Loss: 3.8028635716438295\n",
            "Running evaluation at iteration 290400...\n",
            "Iteration 290400, Train Loss: 3.303877592086792 Eval Loss: 3.8494368743896485\n",
            "Running evaluation at iteration 290500...\n",
            "Iteration 290500, Train Loss: 4.107168197631836 Eval Loss: 3.8610445165634157\n",
            "Running evaluation at iteration 290600...\n",
            "Iteration 290600, Train Loss: 3.2251272201538086 Eval Loss: 3.9409529519081117\n",
            "Running evaluation at iteration 290700...\n",
            "Iteration 290700, Train Loss: 3.9561638832092285 Eval Loss: 3.8672831892967223\n",
            "Running evaluation at iteration 290800...\n",
            "Iteration 290800, Train Loss: 3.556666612625122 Eval Loss: 3.8368679761886595\n",
            "Running evaluation at iteration 290900...\n",
            "Iteration 290900, Train Loss: 3.547302007675171 Eval Loss: 3.8868153357505797\n",
            "Running evaluation at iteration 291000...\n",
            "Iteration 291000, Train Loss: 3.96643328666687 Eval Loss: 3.8192335605621337\n",
            "Running evaluation at iteration 291100...\n",
            "Iteration 291100, Train Loss: 4.10197639465332 Eval Loss: 3.971417145729065\n",
            "Running evaluation at iteration 291200...\n",
            "Iteration 291200, Train Loss: 4.829665184020996 Eval Loss: 3.878794867992401\n",
            "Running evaluation at iteration 291300...\n",
            "Iteration 291300, Train Loss: 4.286308288574219 Eval Loss: 3.885605289936066\n",
            "Running evaluation at iteration 291400...\n",
            "Iteration 291400, Train Loss: 4.025659561157227 Eval Loss: 3.8581343126296996\n",
            "Running evaluation at iteration 291500...\n",
            "Iteration 291500, Train Loss: 4.575876712799072 Eval Loss: 3.8260918283462524\n",
            "Running evaluation at iteration 291600...\n",
            "Iteration 291600, Train Loss: 3.8702480792999268 Eval Loss: 3.783529875278473\n",
            "Running evaluation at iteration 291700...\n",
            "Iteration 291700, Train Loss: 3.7000510692596436 Eval Loss: 3.815964553356171\n",
            "Running evaluation at iteration 291800...\n",
            "Iteration 291800, Train Loss: 3.0030558109283447 Eval Loss: 3.9049725222587583\n",
            "Running evaluation at iteration 291900...\n",
            "Iteration 291900, Train Loss: 3.7553870677948 Eval Loss: 3.8649906706809998\n",
            "Running evaluation at iteration 292000...\n",
            "Iteration 292000, Train Loss: 3.8558740615844727 Eval Loss: 3.842290279865265\n",
            "Running evaluation at iteration 292100...\n",
            "Iteration 292100, Train Loss: 4.028903484344482 Eval Loss: 3.898923008441925\n",
            "Running evaluation at iteration 292200...\n",
            "Iteration 292200, Train Loss: 3.8914945125579834 Eval Loss: 3.897496173381805\n",
            "Running evaluation at iteration 292300...\n",
            "Iteration 292300, Train Loss: 4.490405559539795 Eval Loss: 3.8047663497924806\n",
            "Running evaluation at iteration 292400...\n",
            "Iteration 292400, Train Loss: 4.226485729217529 Eval Loss: 3.7728831696510317\n",
            "Running evaluation at iteration 292500...\n",
            "Iteration 292500, Train Loss: 4.1277265548706055 Eval Loss: 3.8898145008087157\n",
            "Running evaluation at iteration 292600...\n",
            "Iteration 292600, Train Loss: 3.1526196002960205 Eval Loss: 3.8232400798797608\n",
            "Running evaluation at iteration 292700...\n",
            "Iteration 292700, Train Loss: 4.444206714630127 Eval Loss: 3.8662963700294495\n",
            "Running evaluation at iteration 292800...\n",
            "Iteration 292800, Train Loss: 3.5739903450012207 Eval Loss: 3.8103060030937197\n",
            "Running evaluation at iteration 292900...\n",
            "Iteration 292900, Train Loss: 3.487464427947998 Eval Loss: 3.960358395576477\n",
            "Running evaluation at iteration 293000...\n",
            "Iteration 293000, Train Loss: 4.931085586547852 Eval Loss: 3.9248661613464355\n",
            "Running evaluation at iteration 293100...\n",
            "Iteration 293100, Train Loss: 3.391881227493286 Eval Loss: 3.749628882408142\n",
            "Running evaluation at iteration 293200...\n",
            "Iteration 293200, Train Loss: 3.6797773838043213 Eval Loss: 3.797553689479828\n",
            "Running evaluation at iteration 293300...\n",
            "Iteration 293300, Train Loss: 4.016801357269287 Eval Loss: 3.866027789115906\n",
            "Running evaluation at iteration 293400...\n",
            "Iteration 293400, Train Loss: 4.763498783111572 Eval Loss: 3.8497431921958922\n",
            "Running evaluation at iteration 293500...\n",
            "Iteration 293500, Train Loss: 3.8929262161254883 Eval Loss: 3.771294732093811\n",
            "Running evaluation at iteration 293600...\n",
            "Iteration 293600, Train Loss: 3.851912021636963 Eval Loss: 3.9206606483459474\n",
            "Running evaluation at iteration 293700...\n",
            "Iteration 293700, Train Loss: 4.417544841766357 Eval Loss: 3.879130382537842\n",
            "Running evaluation at iteration 293800...\n",
            "Iteration 293800, Train Loss: 3.6823863983154297 Eval Loss: 3.7786508440971374\n",
            "Running evaluation at iteration 293900...\n",
            "Iteration 293900, Train Loss: 4.507743835449219 Eval Loss: 3.9265497159957885\n",
            "Running evaluation at iteration 294000...\n",
            "Iteration 294000, Train Loss: 4.155948638916016 Eval Loss: 3.9143239855766296\n",
            "Running evaluation at iteration 294100...\n",
            "Iteration 294100, Train Loss: 4.5283355712890625 Eval Loss: 3.9086782455444338\n",
            "Running evaluation at iteration 294200...\n",
            "Iteration 294200, Train Loss: 3.890211343765259 Eval Loss: 3.8672754836082457\n",
            "Running evaluation at iteration 294300...\n",
            "Iteration 294300, Train Loss: 5.325672626495361 Eval Loss: 3.7520813727378846\n",
            "Running evaluation at iteration 294400...\n",
            "Iteration 294400, Train Loss: 3.810112237930298 Eval Loss: 3.7921172523498536\n",
            "Running evaluation at iteration 294500...\n",
            "Iteration 294500, Train Loss: 4.215163230895996 Eval Loss: 3.8969063329696656\n",
            "Running evaluation at iteration 294600...\n",
            "Iteration 294600, Train Loss: 4.503158092498779 Eval Loss: 3.7997949528694153\n",
            "Running evaluation at iteration 294700...\n",
            "Iteration 294700, Train Loss: 4.3213324546813965 Eval Loss: 3.9466472339630125\n",
            "Running evaluation at iteration 294800...\n",
            "Iteration 294800, Train Loss: 4.09822416305542 Eval Loss: 3.8432611346244814\n",
            "Running evaluation at iteration 294900...\n",
            "Iteration 294900, Train Loss: 4.183029651641846 Eval Loss: 3.7631369400024415\n",
            "Running evaluation at iteration 295000...\n",
            "Iteration 295000, Train Loss: 4.447353363037109 Eval Loss: 3.882171370983124\n",
            "Running evaluation at iteration 295100...\n",
            "Iteration 295100, Train Loss: 3.6726455688476562 Eval Loss: 3.7542557644844057\n",
            "Running evaluation at iteration 295200...\n",
            "Iteration 295200, Train Loss: 4.785696029663086 Eval Loss: 3.7711771869659425\n",
            "Running evaluation at iteration 295300...\n",
            "Iteration 295300, Train Loss: 3.7791504859924316 Eval Loss: 3.7811225986480714\n",
            "Running evaluation at iteration 295400...\n",
            "Iteration 295400, Train Loss: 3.639385223388672 Eval Loss: 3.8176053547859192\n",
            "Running evaluation at iteration 295500...\n",
            "Iteration 295500, Train Loss: 3.7699196338653564 Eval Loss: 3.9247206616401673\n",
            "Running evaluation at iteration 295600...\n",
            "Iteration 295600, Train Loss: 4.344643592834473 Eval Loss: 3.900794961452484\n",
            "Running evaluation at iteration 295700...\n",
            "Iteration 295700, Train Loss: 3.608729124069214 Eval Loss: 3.853542368412018\n",
            "Running evaluation at iteration 295800...\n",
            "Iteration 295800, Train Loss: 3.3499414920806885 Eval Loss: 3.8347522044181823\n",
            "Running evaluation at iteration 295900...\n",
            "Iteration 295900, Train Loss: 5.260003566741943 Eval Loss: 3.8620140528678895\n",
            "Running evaluation at iteration 296000...\n",
            "Iteration 296000, Train Loss: 4.128305912017822 Eval Loss: 3.919503448009491\n",
            "Running evaluation at iteration 296100...\n",
            "Iteration 296100, Train Loss: 4.057721138000488 Eval Loss: 3.8478593826293945\n",
            "Running evaluation at iteration 296200...\n",
            "Iteration 296200, Train Loss: 3.925628662109375 Eval Loss: 3.87820018529892\n",
            "Running evaluation at iteration 296300...\n",
            "Iteration 296300, Train Loss: 4.351273059844971 Eval Loss: 3.7684154653549196\n",
            "Running evaluation at iteration 296400...\n",
            "Iteration 296400, Train Loss: 3.0490615367889404 Eval Loss: 3.826803765296936\n",
            "Running evaluation at iteration 296500...\n",
            "Iteration 296500, Train Loss: 3.2767531871795654 Eval Loss: 3.934879870414734\n",
            "Running evaluation at iteration 296600...\n",
            "Iteration 296600, Train Loss: 3.6616313457489014 Eval Loss: 3.7482247447967527\n",
            "Running evaluation at iteration 296700...\n",
            "Iteration 296700, Train Loss: 4.235701084136963 Eval Loss: 3.8179233622550965\n",
            "Running evaluation at iteration 296800...\n",
            "Iteration 296800, Train Loss: 3.9636218547821045 Eval Loss: 3.803581292629242\n",
            "Running evaluation at iteration 296900...\n",
            "Iteration 296900, Train Loss: 4.037013530731201 Eval Loss: 3.8759608602523805\n",
            "Running evaluation at iteration 297000...\n",
            "Iteration 297000, Train Loss: 4.664402961730957 Eval Loss: 3.8337779688835143\n",
            "Running evaluation at iteration 297100...\n",
            "Iteration 297100, Train Loss: 4.09901762008667 Eval Loss: 3.758647141456604\n",
            "Running evaluation at iteration 297200...\n",
            "Iteration 297200, Train Loss: 4.2439069747924805 Eval Loss: 3.8416815972328187\n",
            "Running evaluation at iteration 297300...\n",
            "Iteration 297300, Train Loss: 4.307644367218018 Eval Loss: 3.8551318502426146\n",
            "Running evaluation at iteration 297400...\n",
            "Iteration 297400, Train Loss: 4.696140766143799 Eval Loss: 3.9034859347343445\n",
            "Running evaluation at iteration 297500...\n",
            "Iteration 297500, Train Loss: 3.1801764965057373 Eval Loss: 3.9079076981544496\n",
            "Running evaluation at iteration 297600...\n",
            "Iteration 297600, Train Loss: 2.8504908084869385 Eval Loss: 3.7825715470314027\n",
            "Running evaluation at iteration 297700...\n",
            "Iteration 297700, Train Loss: 4.390846252441406 Eval Loss: 3.897131061553955\n",
            "Running evaluation at iteration 297800...\n",
            "Iteration 297800, Train Loss: 4.151394844055176 Eval Loss: 3.8660792231559755\n",
            "Running evaluation at iteration 297900...\n",
            "Iteration 297900, Train Loss: 2.6890463829040527 Eval Loss: 3.928898797035217\n",
            "Running evaluation at iteration 298000...\n",
            "Iteration 298000, Train Loss: 4.230476379394531 Eval Loss: 3.8328431344032285\n",
            "Running evaluation at iteration 298100...\n",
            "Iteration 298100, Train Loss: 3.7988791465759277 Eval Loss: 3.704500322341919\n",
            "Running evaluation at iteration 298200...\n",
            "Iteration 298200, Train Loss: 4.229135036468506 Eval Loss: 3.8788787961006164\n",
            "Running evaluation at iteration 298300...\n",
            "Iteration 298300, Train Loss: 3.8675696849823 Eval Loss: 3.8755903816223145\n",
            "Running evaluation at iteration 298400...\n",
            "Iteration 298400, Train Loss: 5.089496612548828 Eval Loss: 3.8754716324806213\n",
            "Running evaluation at iteration 298500...\n",
            "Iteration 298500, Train Loss: 4.565023422241211 Eval Loss: 3.8256272864341736\n",
            "Running evaluation at iteration 298600...\n",
            "Iteration 298600, Train Loss: 3.4815285205841064 Eval Loss: 3.8395542120933532\n",
            "Running evaluation at iteration 298700...\n",
            "Iteration 298700, Train Loss: 3.9815642833709717 Eval Loss: 3.8710931444168093\n",
            "Running evaluation at iteration 298800...\n",
            "Iteration 298800, Train Loss: 4.070864677429199 Eval Loss: 3.80675986289978\n",
            "Running evaluation at iteration 298900...\n",
            "Iteration 298900, Train Loss: 4.777229309082031 Eval Loss: 3.9219257760047914\n",
            "Running evaluation at iteration 299000...\n",
            "Iteration 299000, Train Loss: 2.8596765995025635 Eval Loss: 3.840433852672577\n",
            "Running evaluation at iteration 299100...\n",
            "Iteration 299100, Train Loss: 4.809816360473633 Eval Loss: 3.8927289509773253\n",
            "Running evaluation at iteration 299200...\n",
            "Iteration 299200, Train Loss: 3.82084059715271 Eval Loss: 3.7989651584625244\n",
            "Running evaluation at iteration 299300...\n",
            "Iteration 299300, Train Loss: 3.571726083755493 Eval Loss: 3.755046889781952\n",
            "Running evaluation at iteration 299400...\n",
            "Iteration 299400, Train Loss: 3.597355365753174 Eval Loss: 3.7521216297149658\n",
            "Running evaluation at iteration 299500...\n",
            "Iteration 299500, Train Loss: 4.15086030960083 Eval Loss: 3.887060821056366\n",
            "Running evaluation at iteration 299600...\n",
            "Iteration 299600, Train Loss: 4.420989513397217 Eval Loss: 3.827408514022827\n",
            "Running evaluation at iteration 299700...\n",
            "Iteration 299700, Train Loss: 4.266473770141602 Eval Loss: 3.9284926152229307\n",
            "Running evaluation at iteration 299800...\n",
            "Iteration 299800, Train Loss: 3.715998649597168 Eval Loss: 3.9126320934295653\n",
            "Running evaluation at iteration 299900...\n",
            "Iteration 299900, Train Loss: 3.273790121078491 Eval Loss: 3.840093955993652\n",
            "Running evaluation at iteration 300000...\n",
            "Iteration 300000, Train Loss: 3.5154783725738525 Eval Loss: 3.8356170892715453\n",
            "Running evaluation at iteration 300100...\n",
            "Iteration 300100, Train Loss: 3.449708938598633 Eval Loss: 3.868792662620544\n",
            "Running evaluation at iteration 300200...\n",
            "Iteration 300200, Train Loss: 3.777242422103882 Eval Loss: 3.7562509059906004\n",
            "Running evaluation at iteration 300300...\n",
            "Iteration 300300, Train Loss: 3.84010910987854 Eval Loss: 3.773120651245117\n",
            "Running evaluation at iteration 300400...\n",
            "Iteration 300400, Train Loss: 4.034206390380859 Eval Loss: 3.8581044769287107\n",
            "Running evaluation at iteration 300500...\n",
            "Iteration 300500, Train Loss: 3.578354597091675 Eval Loss: 3.993736448287964\n",
            "Running evaluation at iteration 300600...\n",
            "Iteration 300600, Train Loss: 3.9441142082214355 Eval Loss: 3.929448149204254\n",
            "Running evaluation at iteration 300700...\n",
            "Iteration 300700, Train Loss: 4.06226110458374 Eval Loss: 3.9443382024765015\n",
            "Running evaluation at iteration 300800...\n",
            "Iteration 300800, Train Loss: 4.574286460876465 Eval Loss: 3.9098220300674438\n",
            "Running evaluation at iteration 300900...\n",
            "Iteration 300900, Train Loss: 5.012964248657227 Eval Loss: 3.846808032989502\n",
            "Running evaluation at iteration 301000...\n",
            "Iteration 301000, Train Loss: 3.9248239994049072 Eval Loss: 3.8796536540985107\n",
            "Running evaluation at iteration 301100...\n",
            "Iteration 301100, Train Loss: 3.32979154586792 Eval Loss: 3.8229815340042115\n",
            "Running evaluation at iteration 301200...\n",
            "Iteration 301200, Train Loss: 4.439754486083984 Eval Loss: 3.8274390840530397\n",
            "Running evaluation at iteration 301300...\n",
            "Iteration 301300, Train Loss: 4.079782962799072 Eval Loss: 3.811611876487732\n",
            "Running evaluation at iteration 301400...\n",
            "Iteration 301400, Train Loss: 4.025530815124512 Eval Loss: 3.850221285820007\n",
            "Running evaluation at iteration 301500...\n",
            "Iteration 301500, Train Loss: 3.159654140472412 Eval Loss: 3.8835492300987244\n",
            "Running evaluation at iteration 301600...\n",
            "Iteration 301600, Train Loss: 3.642190933227539 Eval Loss: 3.838243100643158\n",
            "Running evaluation at iteration 301700...\n",
            "Iteration 301700, Train Loss: 3.7753005027770996 Eval Loss: 3.896824929714203\n",
            "Running evaluation at iteration 301800...\n",
            "Iteration 301800, Train Loss: 3.083660364151001 Eval Loss: 3.9074717926979066\n",
            "Running evaluation at iteration 301900...\n",
            "Iteration 301900, Train Loss: 4.379411697387695 Eval Loss: 3.8563419556617737\n",
            "Running evaluation at iteration 302000...\n",
            "Iteration 302000, Train Loss: 4.496424198150635 Eval Loss: 3.8890329790115357\n",
            "Running evaluation at iteration 302100...\n",
            "Iteration 302100, Train Loss: 3.4203853607177734 Eval Loss: 3.7879071831703186\n",
            "Running evaluation at iteration 302200...\n",
            "Iteration 302200, Train Loss: 4.178873062133789 Eval Loss: 3.7989166712760927\n",
            "Running evaluation at iteration 302300...\n",
            "Iteration 302300, Train Loss: 3.8306777477264404 Eval Loss: 3.8895885157585144\n",
            "Running evaluation at iteration 302400...\n",
            "Iteration 302400, Train Loss: 2.9611127376556396 Eval Loss: 3.845774736404419\n",
            "Running evaluation at iteration 302500...\n",
            "Iteration 302500, Train Loss: 4.307132244110107 Eval Loss: 3.8768830347061156\n",
            "Running evaluation at iteration 302600...\n",
            "Iteration 302600, Train Loss: 3.0713021755218506 Eval Loss: 3.795616960525513\n",
            "Running evaluation at iteration 302700...\n",
            "Iteration 302700, Train Loss: 3.6459197998046875 Eval Loss: 3.812551782131195\n",
            "Running evaluation at iteration 302800...\n",
            "Iteration 302800, Train Loss: 4.118343353271484 Eval Loss: 3.858333387374878\n",
            "Running evaluation at iteration 302900...\n",
            "Iteration 302900, Train Loss: 5.191858291625977 Eval Loss: 3.8468446826934812\n",
            "Running evaluation at iteration 303000...\n",
            "Iteration 303000, Train Loss: 2.769113302230835 Eval Loss: 3.880240774154663\n",
            "Running evaluation at iteration 303100...\n",
            "Iteration 303100, Train Loss: 4.384653091430664 Eval Loss: 3.910385127067566\n",
            "Running evaluation at iteration 303200...\n",
            "Iteration 303200, Train Loss: 3.9367752075195312 Eval Loss: 3.8795044374465943\n",
            "Running evaluation at iteration 303300...\n",
            "Iteration 303300, Train Loss: 3.5896785259246826 Eval Loss: 3.871812708377838\n",
            "Running evaluation at iteration 303400...\n",
            "Iteration 303400, Train Loss: 3.179853916168213 Eval Loss: 3.9625669884681702\n",
            "Running evaluation at iteration 303500...\n",
            "Iteration 303500, Train Loss: 4.495228290557861 Eval Loss: 3.8648698687553407\n",
            "Running evaluation at iteration 303600...\n",
            "Iteration 303600, Train Loss: 3.6170308589935303 Eval Loss: 3.8806920742988584\n",
            "Running evaluation at iteration 303700...\n",
            "Iteration 303700, Train Loss: 4.33303689956665 Eval Loss: 3.8470701384544372\n",
            "Running evaluation at iteration 303800...\n",
            "Iteration 303800, Train Loss: 3.381078004837036 Eval Loss: 3.7994732570648195\n",
            "Running evaluation at iteration 303900...\n",
            "Iteration 303900, Train Loss: 3.7948975563049316 Eval Loss: 3.7958313536643984\n",
            "Running evaluation at iteration 304000...\n",
            "Iteration 304000, Train Loss: 3.8911044597625732 Eval Loss: 3.8066190838813783\n",
            "Running evaluation at iteration 304100...\n",
            "Iteration 304100, Train Loss: 3.7783854007720947 Eval Loss: 3.825704188346863\n",
            "Running evaluation at iteration 304200...\n",
            "Iteration 304200, Train Loss: 3.3105151653289795 Eval Loss: 3.8756892919540404\n",
            "Running evaluation at iteration 304300...\n",
            "Iteration 304300, Train Loss: 3.4875986576080322 Eval Loss: 3.8695953917503356\n",
            "Running evaluation at iteration 304400...\n",
            "Iteration 304400, Train Loss: 4.305349826812744 Eval Loss: 3.8316400909423827\n",
            "Running evaluation at iteration 304500...\n",
            "Iteration 304500, Train Loss: 5.425913333892822 Eval Loss: 3.9278694653511046\n",
            "Running evaluation at iteration 304600...\n",
            "Iteration 304600, Train Loss: 4.292057991027832 Eval Loss: 3.878156759738922\n",
            "Running evaluation at iteration 304700...\n",
            "Iteration 304700, Train Loss: 2.1346256732940674 Eval Loss: 3.9304946303367614\n",
            "Running evaluation at iteration 304800...\n",
            "Iteration 304800, Train Loss: 2.7677526473999023 Eval Loss: 3.7664775705337523\n",
            "Running evaluation at iteration 304900...\n",
            "Iteration 304900, Train Loss: 4.140112400054932 Eval Loss: 3.7270684123039244\n",
            "Running evaluation at iteration 305000...\n",
            "Iteration 305000, Train Loss: 3.3240318298339844 Eval Loss: 3.800786306858063\n",
            "Running evaluation at iteration 305100...\n",
            "Iteration 305100, Train Loss: 3.6221089363098145 Eval Loss: 3.8386220288276673\n",
            "Running evaluation at iteration 305200...\n",
            "Iteration 305200, Train Loss: 3.678391933441162 Eval Loss: 3.8291960763931274\n",
            "Running evaluation at iteration 305300...\n",
            "Iteration 305300, Train Loss: 4.432088851928711 Eval Loss: 3.8680090069770814\n",
            "Running evaluation at iteration 305400...\n",
            "Iteration 305400, Train Loss: 3.790956735610962 Eval Loss: 3.7244241857528686\n",
            "Running evaluation at iteration 305500...\n",
            "Iteration 305500, Train Loss: 4.333982467651367 Eval Loss: 3.947753291130066\n",
            "Running evaluation at iteration 305600...\n",
            "Iteration 305600, Train Loss: 3.755905866622925 Eval Loss: 3.8059358286857603\n",
            "Running evaluation at iteration 305700...\n",
            "Iteration 305700, Train Loss: 3.637338399887085 Eval Loss: 3.8690927624702454\n",
            "Running evaluation at iteration 305800...\n",
            "Iteration 305800, Train Loss: 4.184882640838623 Eval Loss: 3.9181052303314208\n",
            "Running evaluation at iteration 305900...\n",
            "Iteration 305900, Train Loss: 5.4993205070495605 Eval Loss: 3.821942584514618\n",
            "Running evaluation at iteration 306000...\n",
            "Iteration 306000, Train Loss: 4.73514986038208 Eval Loss: 3.7978412461280824\n",
            "Running evaluation at iteration 306100...\n",
            "Iteration 306100, Train Loss: 3.6328001022338867 Eval Loss: 3.8088725185394288\n",
            "Running evaluation at iteration 306200...\n",
            "Iteration 306200, Train Loss: 3.9509661197662354 Eval Loss: 3.896013031005859\n",
            "Running evaluation at iteration 306300...\n",
            "Iteration 306300, Train Loss: 3.2022576332092285 Eval Loss: 3.9156537008285524\n",
            "Running evaluation at iteration 306400...\n",
            "Iteration 306400, Train Loss: 5.1431660652160645 Eval Loss: 3.7627501916885375\n",
            "Running evaluation at iteration 306500...\n",
            "Iteration 306500, Train Loss: 3.355337142944336 Eval Loss: 3.8647755098342897\n",
            "Running evaluation at iteration 306600...\n",
            "Iteration 306600, Train Loss: 3.681333303451538 Eval Loss: 3.816434323787689\n",
            "Running evaluation at iteration 306700...\n",
            "Iteration 306700, Train Loss: 4.140695095062256 Eval Loss: 3.881171998977661\n",
            "Running evaluation at iteration 306800...\n",
            "Iteration 306800, Train Loss: 3.9170029163360596 Eval Loss: 3.920236327648163\n",
            "Running evaluation at iteration 306900...\n",
            "Iteration 306900, Train Loss: 4.48261022567749 Eval Loss: 3.891030504703522\n",
            "Running evaluation at iteration 307000...\n",
            "Iteration 307000, Train Loss: 4.659870624542236 Eval Loss: 3.7916300749778746\n",
            "Running evaluation at iteration 307100...\n",
            "Iteration 307100, Train Loss: 3.6741881370544434 Eval Loss: 3.826294538974762\n",
            "Running evaluation at iteration 307200...\n",
            "Iteration 307200, Train Loss: 3.33097767829895 Eval Loss: 3.7729882764816285\n",
            "Running evaluation at iteration 307300...\n",
            "Iteration 307300, Train Loss: 3.0237419605255127 Eval Loss: 3.785750455856323\n",
            "Running evaluation at iteration 307400...\n",
            "Iteration 307400, Train Loss: 3.803523540496826 Eval Loss: 3.9206039237976076\n",
            "Running evaluation at iteration 307500...\n",
            "Iteration 307500, Train Loss: 4.653358459472656 Eval Loss: 3.7770303297042847\n",
            "Running evaluation at iteration 307600...\n",
            "Iteration 307600, Train Loss: 3.486704111099243 Eval Loss: 3.8714281630516054\n",
            "Running evaluation at iteration 307700...\n",
            "Iteration 307700, Train Loss: 3.3775107860565186 Eval Loss: 3.8735741567611695\n",
            "Running evaluation at iteration 307800...\n",
            "Iteration 307800, Train Loss: 4.690700054168701 Eval Loss: 3.8350781846046447\n",
            "Running evaluation at iteration 307900...\n",
            "Iteration 307900, Train Loss: 3.925389289855957 Eval Loss: 3.8381496810913087\n",
            "Running evaluation at iteration 308000...\n",
            "Iteration 308000, Train Loss: 3.351483106613159 Eval Loss: 3.8517417311668396\n",
            "Running evaluation at iteration 308100...\n",
            "Iteration 308100, Train Loss: 4.153157711029053 Eval Loss: 3.9189457845687867\n",
            "Running evaluation at iteration 308200...\n",
            "Iteration 308200, Train Loss: 3.4399776458740234 Eval Loss: 3.893638370037079\n",
            "Running evaluation at iteration 308300...\n",
            "Iteration 308300, Train Loss: 3.7915279865264893 Eval Loss: 3.854211928844452\n",
            "Running evaluation at iteration 308400...\n",
            "Iteration 308400, Train Loss: 3.829089403152466 Eval Loss: 3.8067652082443235\n",
            "Running evaluation at iteration 308500...\n",
            "Iteration 308500, Train Loss: 4.366364002227783 Eval Loss: 3.7946245646476746\n",
            "Running evaluation at iteration 308600...\n",
            "Iteration 308600, Train Loss: 3.646761655807495 Eval Loss: 3.7831854224205017\n",
            "Running evaluation at iteration 308700...\n",
            "Iteration 308700, Train Loss: 3.870846748352051 Eval Loss: 3.842468514442444\n",
            "Running evaluation at iteration 308800...\n",
            "Iteration 308800, Train Loss: 4.14595365524292 Eval Loss: 3.7829513859748842\n",
            "Running evaluation at iteration 308900...\n",
            "Iteration 308900, Train Loss: 4.390745639801025 Eval Loss: 3.8892247557640074\n",
            "Running evaluation at iteration 309000...\n",
            "Iteration 309000, Train Loss: 3.345928192138672 Eval Loss: 3.8578645730018617\n",
            "Running evaluation at iteration 309100...\n",
            "Iteration 309100, Train Loss: 5.260471820831299 Eval Loss: 3.939249155521393\n",
            "Running evaluation at iteration 309200...\n",
            "Iteration 309200, Train Loss: 4.267492771148682 Eval Loss: 3.80762544631958\n",
            "Running evaluation at iteration 309300...\n",
            "Iteration 309300, Train Loss: 3.941162109375 Eval Loss: 3.8749386525154113\n",
            "Running evaluation at iteration 309400...\n",
            "Iteration 309400, Train Loss: 3.7648236751556396 Eval Loss: 3.7838018488883973\n",
            "Running evaluation at iteration 309500...\n",
            "Iteration 309500, Train Loss: 4.061917304992676 Eval Loss: 3.829552774429321\n",
            "Running evaluation at iteration 309600...\n",
            "Iteration 309600, Train Loss: 3.1334309577941895 Eval Loss: 3.9204052662849427\n",
            "Running evaluation at iteration 309700...\n",
            "Iteration 309700, Train Loss: 4.224548816680908 Eval Loss: 3.886608572006226\n",
            "Running evaluation at iteration 309800...\n",
            "Iteration 309800, Train Loss: 3.7075181007385254 Eval Loss: 3.869977889060974\n",
            "Running evaluation at iteration 309900...\n",
            "Iteration 309900, Train Loss: 2.4103784561157227 Eval Loss: 3.8562941575050353\n",
            "Running evaluation at iteration 310000...\n",
            "Iteration 310000, Train Loss: 3.395925998687744 Eval Loss: 3.8393778514862063\n",
            "Running evaluation at iteration 310100...\n",
            "Iteration 310100, Train Loss: 3.545179605484009 Eval Loss: 3.913846869468689\n",
            "Running evaluation at iteration 310200...\n",
            "Iteration 310200, Train Loss: 4.333834648132324 Eval Loss: 3.816048648357391\n",
            "Running evaluation at iteration 310300...\n",
            "Iteration 310300, Train Loss: 3.9783504009246826 Eval Loss: 3.8619008612632753\n",
            "Running evaluation at iteration 310400...\n",
            "Iteration 310400, Train Loss: 4.446540355682373 Eval Loss: 3.8666600680351255\n",
            "Running evaluation at iteration 310500...\n",
            "Iteration 310500, Train Loss: 4.4939775466918945 Eval Loss: 3.914829578399658\n",
            "Running evaluation at iteration 310600...\n",
            "Iteration 310600, Train Loss: 4.829360008239746 Eval Loss: 3.8994002985954284\n",
            "Running evaluation at iteration 310700...\n",
            "Iteration 310700, Train Loss: 4.7678656578063965 Eval Loss: 3.8137663888931272\n",
            "Running evaluation at iteration 310800...\n",
            "Iteration 310800, Train Loss: 4.498712062835693 Eval Loss: 3.876352896690369\n",
            "Running evaluation at iteration 310900...\n",
            "Iteration 310900, Train Loss: 3.8304946422576904 Eval Loss: 3.876741364002228\n",
            "Running evaluation at iteration 311000...\n",
            "Iteration 311000, Train Loss: 4.673996448516846 Eval Loss: 3.8178860473632814\n",
            "Running evaluation at iteration 311100...\n",
            "Iteration 311100, Train Loss: 3.595747947692871 Eval Loss: 3.8628760170936585\n",
            "Running evaluation at iteration 311200...\n",
            "Iteration 311200, Train Loss: 4.00236701965332 Eval Loss: 3.910050871372223\n",
            "Running evaluation at iteration 311300...\n",
            "Iteration 311300, Train Loss: 3.2342324256896973 Eval Loss: 3.9371363973617552\n",
            "Running evaluation at iteration 311400...\n",
            "Iteration 311400, Train Loss: 3.6232171058654785 Eval Loss: 3.7940333342552184\n",
            "Running evaluation at iteration 311500...\n",
            "Iteration 311500, Train Loss: 3.3688061237335205 Eval Loss: 3.8784299552440644\n",
            "Running evaluation at iteration 311600...\n",
            "Iteration 311600, Train Loss: 4.323586940765381 Eval Loss: 3.816803767681122\n",
            "Running evaluation at iteration 311700...\n",
            "Iteration 311700, Train Loss: 3.353715658187866 Eval Loss: 3.8686527895927427\n",
            "Running evaluation at iteration 311800...\n",
            "Iteration 311800, Train Loss: 3.129086494445801 Eval Loss: 3.7554407024383547\n",
            "Running evaluation at iteration 311900...\n",
            "Iteration 311900, Train Loss: 3.8667335510253906 Eval Loss: 3.873568069934845\n",
            "Running evaluation at iteration 312000...\n",
            "Iteration 312000, Train Loss: 3.718733787536621 Eval Loss: 3.7553266501426696\n",
            "Running evaluation at iteration 312100...\n",
            "Iteration 312100, Train Loss: 3.9270458221435547 Eval Loss: 3.8298830914497377\n",
            "Running evaluation at iteration 312200...\n",
            "Iteration 312200, Train Loss: 3.840297222137451 Eval Loss: 3.8589184927940368\n",
            "Running evaluation at iteration 312300...\n",
            "Iteration 312300, Train Loss: 3.9999985694885254 Eval Loss: 3.8790226650238036\n",
            "Running evaluation at iteration 312400...\n",
            "Iteration 312400, Train Loss: 3.089704990386963 Eval Loss: 3.8474575543403624\n",
            "Running evaluation at iteration 312500...\n",
            "Iteration 312500, Train Loss: 4.210317611694336 Eval Loss: 3.826407878398895\n",
            "Running evaluation at iteration 312600...\n",
            "Iteration 312600, Train Loss: 3.7802670001983643 Eval Loss: 3.865241048336029\n",
            "Running evaluation at iteration 312700...\n",
            "Iteration 312700, Train Loss: 3.7653605937957764 Eval Loss: 3.872554507255554\n",
            "Running evaluation at iteration 312800...\n",
            "Iteration 312800, Train Loss: 3.6927947998046875 Eval Loss: 3.8423946595191953\n",
            "Running evaluation at iteration 312900...\n",
            "Iteration 312900, Train Loss: 3.717073917388916 Eval Loss: 3.9504584097862243\n",
            "Running evaluation at iteration 313000...\n",
            "Iteration 313000, Train Loss: 4.2042622566223145 Eval Loss: 3.8122999024391175\n",
            "Running evaluation at iteration 313100...\n",
            "Iteration 313100, Train Loss: 5.268276691436768 Eval Loss: 3.9288588428497313\n",
            "Running evaluation at iteration 313200...\n",
            "Iteration 313200, Train Loss: 4.033191204071045 Eval Loss: 3.811326518058777\n",
            "Running evaluation at iteration 313300...\n",
            "Iteration 313300, Train Loss: 3.659456729888916 Eval Loss: 3.8174452877044676\n",
            "Running evaluation at iteration 313400...\n",
            "Iteration 313400, Train Loss: 4.000840187072754 Eval Loss: 3.8320118021965026\n",
            "Running evaluation at iteration 313500...\n",
            "Iteration 313500, Train Loss: 4.474546432495117 Eval Loss: 3.789591066837311\n",
            "Running evaluation at iteration 313600...\n",
            "Iteration 313600, Train Loss: 4.3037638664245605 Eval Loss: 3.7584363412857056\n",
            "Running evaluation at iteration 313700...\n",
            "Iteration 313700, Train Loss: 4.37164831161499 Eval Loss: 3.704374814033508\n",
            "Running evaluation at iteration 313800...\n",
            "Iteration 313800, Train Loss: 3.6455183029174805 Eval Loss: 3.858110771179199\n",
            "Running evaluation at iteration 313900...\n",
            "Iteration 313900, Train Loss: 4.111701488494873 Eval Loss: 3.8502446007728577\n",
            "Running evaluation at iteration 314000...\n",
            "Iteration 314000, Train Loss: 3.0195000171661377 Eval Loss: 3.8337525796890257\n",
            "Running evaluation at iteration 314100...\n",
            "Iteration 314100, Train Loss: 3.3101441860198975 Eval Loss: 3.8082486963272095\n",
            "Running evaluation at iteration 314200...\n",
            "Iteration 314200, Train Loss: 3.8438379764556885 Eval Loss: 3.8572824478149412\n",
            "Running evaluation at iteration 314300...\n",
            "Iteration 314300, Train Loss: 3.985435962677002 Eval Loss: 3.8368279457092287\n",
            "Running evaluation at iteration 314400...\n",
            "Iteration 314400, Train Loss: 3.670032024383545 Eval Loss: 3.844762964248657\n",
            "Running evaluation at iteration 314500...\n",
            "Iteration 314500, Train Loss: 4.494006156921387 Eval Loss: 3.8476468348503112\n",
            "Running evaluation at iteration 314600...\n",
            "Iteration 314600, Train Loss: 3.4464457035064697 Eval Loss: 3.824966883659363\n",
            "Running evaluation at iteration 314700...\n",
            "Iteration 314700, Train Loss: 3.930748462677002 Eval Loss: 3.826214916706085\n",
            "Running evaluation at iteration 314800...\n",
            "Iteration 314800, Train Loss: 3.757913827896118 Eval Loss: 3.830028483867645\n",
            "Running evaluation at iteration 314900...\n",
            "Iteration 314900, Train Loss: 4.112682342529297 Eval Loss: 3.866359293460846\n",
            "Running evaluation at iteration 315000...\n",
            "Iteration 315000, Train Loss: 3.7289230823516846 Eval Loss: 3.828086392879486\n",
            "Running evaluation at iteration 315100...\n",
            "Iteration 315100, Train Loss: 3.879002809524536 Eval Loss: 3.8084181332588196\n",
            "Running evaluation at iteration 315200...\n",
            "Iteration 315200, Train Loss: 4.425389289855957 Eval Loss: 3.908048412799835\n",
            "Running evaluation at iteration 315300...\n",
            "Iteration 315300, Train Loss: 4.247753620147705 Eval Loss: 3.8174758172035217\n",
            "Running evaluation at iteration 315400...\n",
            "Iteration 315400, Train Loss: 3.4649343490600586 Eval Loss: 3.8065996885299684\n",
            "Running evaluation at iteration 315500...\n",
            "Iteration 315500, Train Loss: 4.385531902313232 Eval Loss: 3.953305356502533\n",
            "Running evaluation at iteration 315600...\n",
            "Iteration 315600, Train Loss: 4.336575508117676 Eval Loss: 3.777971386909485\n",
            "Running evaluation at iteration 315700...\n",
            "Iteration 315700, Train Loss: 4.097057819366455 Eval Loss: 3.8270563316345214\n",
            "Running evaluation at iteration 315800...\n",
            "Iteration 315800, Train Loss: 4.18946647644043 Eval Loss: 3.879988372325897\n",
            "Running evaluation at iteration 315900...\n",
            "Iteration 315900, Train Loss: 4.793830394744873 Eval Loss: 3.828442807197571\n",
            "Running evaluation at iteration 316000...\n",
            "Iteration 316000, Train Loss: 5.11296272277832 Eval Loss: 3.794214837551117\n",
            "Running evaluation at iteration 316100...\n",
            "Iteration 316100, Train Loss: 3.3910787105560303 Eval Loss: 3.7726359081268313\n",
            "Running evaluation at iteration 316200...\n",
            "Iteration 316200, Train Loss: 3.356363296508789 Eval Loss: 3.821300129890442\n",
            "Running evaluation at iteration 316300...\n",
            "Iteration 316300, Train Loss: 3.906306505203247 Eval Loss: 3.841100652217865\n",
            "Running evaluation at iteration 316400...\n",
            "Iteration 316400, Train Loss: 4.292642593383789 Eval Loss: 3.8907543992996216\n",
            "Running evaluation at iteration 316500...\n",
            "Iteration 316500, Train Loss: 4.702773094177246 Eval Loss: 3.894954812526703\n",
            "Running evaluation at iteration 316600...\n",
            "Iteration 316600, Train Loss: 3.3078458309173584 Eval Loss: 3.8250735211372375\n",
            "Running evaluation at iteration 316700...\n",
            "Iteration 316700, Train Loss: 3.112792491912842 Eval Loss: 3.978310923576355\n",
            "Running evaluation at iteration 316800...\n",
            "Iteration 316800, Train Loss: 3.160707950592041 Eval Loss: 3.839955198764801\n",
            "Running evaluation at iteration 316900...\n",
            "Iteration 316900, Train Loss: 4.632735729217529 Eval Loss: 3.8914459419250487\n",
            "Running evaluation at iteration 317000...\n",
            "Iteration 317000, Train Loss: 3.988868474960327 Eval Loss: 3.7984323644638063\n",
            "Running evaluation at iteration 317100...\n",
            "Iteration 317100, Train Loss: 4.108371257781982 Eval Loss: 3.897732357978821\n",
            "Running evaluation at iteration 317200...\n",
            "Iteration 317200, Train Loss: 2.7371716499328613 Eval Loss: 3.8937167739868164\n",
            "Running evaluation at iteration 317300...\n",
            "Iteration 317300, Train Loss: 3.673454999923706 Eval Loss: 3.8319493746757507\n",
            "Running evaluation at iteration 317400...\n",
            "Iteration 317400, Train Loss: 4.391274452209473 Eval Loss: 3.848046433925629\n",
            "Running evaluation at iteration 317500...\n",
            "Iteration 317500, Train Loss: 4.523514747619629 Eval Loss: 3.915159139633179\n",
            "Running evaluation at iteration 317600...\n",
            "Iteration 317600, Train Loss: 3.6918277740478516 Eval Loss: 3.8353737425804137\n",
            "Running evaluation at iteration 317700...\n",
            "Iteration 317700, Train Loss: 3.597438097000122 Eval Loss: 3.872678666114807\n",
            "Running evaluation at iteration 317800...\n",
            "Iteration 317800, Train Loss: 3.4772942066192627 Eval Loss: 3.9318447661399842\n",
            "Running evaluation at iteration 317900...\n",
            "Iteration 317900, Train Loss: 3.4354660511016846 Eval Loss: 3.9011935663223265\n",
            "Running evaluation at iteration 318000...\n",
            "Iteration 318000, Train Loss: 3.1968820095062256 Eval Loss: 3.747833540439606\n",
            "Running evaluation at iteration 318100...\n",
            "Iteration 318100, Train Loss: 4.374004364013672 Eval Loss: 3.8818667769432067\n",
            "Running evaluation at iteration 318200...\n",
            "Iteration 318200, Train Loss: 3.57191801071167 Eval Loss: 3.7042738032341003\n",
            "Running evaluation at iteration 318300...\n",
            "Iteration 318300, Train Loss: 4.24835205078125 Eval Loss: 3.8926129055023195\n",
            "Running evaluation at iteration 318400...\n",
            "Iteration 318400, Train Loss: 3.797121047973633 Eval Loss: 3.883287811279297\n",
            "Running evaluation at iteration 318500...\n",
            "Iteration 318500, Train Loss: 4.180930137634277 Eval Loss: 3.768334398269653\n",
            "Running evaluation at iteration 318600...\n",
            "Iteration 318600, Train Loss: 3.2922096252441406 Eval Loss: 3.876135940551758\n",
            "Running evaluation at iteration 318700...\n",
            "Iteration 318700, Train Loss: 4.027380466461182 Eval Loss: 3.9340773963928224\n",
            "Running evaluation at iteration 318800...\n",
            "Iteration 318800, Train Loss: 4.132768154144287 Eval Loss: 3.8413032412528993\n",
            "Running evaluation at iteration 318900...\n",
            "Iteration 318900, Train Loss: 3.4902184009552 Eval Loss: 3.865074517726898\n",
            "Running evaluation at iteration 319000...\n",
            "Iteration 319000, Train Loss: 3.340932846069336 Eval Loss: 3.816665093898773\n",
            "Running evaluation at iteration 319100...\n",
            "Iteration 319100, Train Loss: 3.9544789791107178 Eval Loss: 3.764298872947693\n",
            "Running evaluation at iteration 319200...\n",
            "Iteration 319200, Train Loss: 4.530776023864746 Eval Loss: 3.8420337438583374\n",
            "Running evaluation at iteration 319300...\n",
            "Iteration 319300, Train Loss: 4.4398112297058105 Eval Loss: 3.8060324716567995\n",
            "Running evaluation at iteration 319400...\n",
            "Iteration 319400, Train Loss: 3.762909173965454 Eval Loss: 3.799948303699493\n",
            "Running evaluation at iteration 319500...\n",
            "Iteration 319500, Train Loss: 3.129042625427246 Eval Loss: 3.8384434628486632\n",
            "Running evaluation at iteration 319600...\n",
            "Iteration 319600, Train Loss: 3.5736045837402344 Eval Loss: 3.874338026046753\n",
            "Running evaluation at iteration 319700...\n",
            "Iteration 319700, Train Loss: 2.8774852752685547 Eval Loss: 3.905571401119232\n",
            "Running evaluation at iteration 319800...\n",
            "Iteration 319800, Train Loss: 3.49652099609375 Eval Loss: 3.792240083217621\n",
            "Running evaluation at iteration 319900...\n",
            "Iteration 319900, Train Loss: 3.949702024459839 Eval Loss: 3.8338181257247923\n",
            "Running evaluation at iteration 320000...\n",
            "Iteration 320000, Train Loss: 4.272029876708984 Eval Loss: 3.858767828941345\n",
            "Running evaluation at iteration 320100...\n",
            "Iteration 320100, Train Loss: 3.649030923843384 Eval Loss: 3.9250845408439634\n",
            "Running evaluation at iteration 320200...\n",
            "Iteration 320200, Train Loss: 3.6381967067718506 Eval Loss: 3.883445243835449\n",
            "Running evaluation at iteration 320300...\n",
            "Iteration 320300, Train Loss: 4.882465839385986 Eval Loss: 3.7414922976493834\n",
            "Running evaluation at iteration 320400...\n",
            "Iteration 320400, Train Loss: 4.001053333282471 Eval Loss: 3.898673689365387\n",
            "Running evaluation at iteration 320500...\n",
            "Iteration 320500, Train Loss: 3.9935426712036133 Eval Loss: 3.7775645899772643\n",
            "Running evaluation at iteration 320600...\n",
            "Iteration 320600, Train Loss: 3.8185882568359375 Eval Loss: 3.900325448513031\n",
            "Running evaluation at iteration 320700...\n",
            "Iteration 320700, Train Loss: 3.780263662338257 Eval Loss: 3.9178860545158387\n",
            "Running evaluation at iteration 320800...\n",
            "Iteration 320800, Train Loss: 4.444573402404785 Eval Loss: 3.837719752788544\n",
            "Running evaluation at iteration 320900...\n",
            "Iteration 320900, Train Loss: 3.610109806060791 Eval Loss: 3.889916710853577\n",
            "Running evaluation at iteration 321000...\n",
            "Iteration 321000, Train Loss: 4.346158981323242 Eval Loss: 3.891662213802338\n",
            "Running evaluation at iteration 321100...\n",
            "Iteration 321100, Train Loss: 3.324566602706909 Eval Loss: 3.950632016658783\n",
            "Running evaluation at iteration 321200...\n",
            "Iteration 321200, Train Loss: 3.535179853439331 Eval Loss: 3.8724252867698667\n",
            "Running evaluation at iteration 321300...\n",
            "Iteration 321300, Train Loss: 3.3648123741149902 Eval Loss: 3.813612525463104\n",
            "Running evaluation at iteration 321400...\n",
            "Iteration 321400, Train Loss: 2.8962318897247314 Eval Loss: 3.8522044944763185\n",
            "Running evaluation at iteration 321500...\n",
            "Iteration 321500, Train Loss: 4.105920314788818 Eval Loss: 3.9635420560836794\n",
            "Running evaluation at iteration 321600...\n",
            "Iteration 321600, Train Loss: 3.4819490909576416 Eval Loss: 3.8295026993751526\n",
            "Running evaluation at iteration 321700...\n",
            "Iteration 321700, Train Loss: 4.731988906860352 Eval Loss: 3.792301275730133\n",
            "Running evaluation at iteration 321800...\n",
            "Iteration 321800, Train Loss: 3.541316270828247 Eval Loss: 3.7307744336128237\n",
            "Running evaluation at iteration 321900...\n",
            "Iteration 321900, Train Loss: 3.6916344165802 Eval Loss: 3.8008884048461913\n",
            "Running evaluation at iteration 322000...\n",
            "Iteration 322000, Train Loss: 3.413835048675537 Eval Loss: 3.825761353969574\n",
            "Running evaluation at iteration 322100...\n",
            "Iteration 322100, Train Loss: 3.964995861053467 Eval Loss: 3.9489998626708984\n",
            "Running evaluation at iteration 322200...\n",
            "Iteration 322200, Train Loss: 4.070152759552002 Eval Loss: 3.8850816822052003\n",
            "Running evaluation at iteration 322300...\n",
            "Iteration 322300, Train Loss: 3.798712730407715 Eval Loss: 3.8643566584587097\n",
            "Running evaluation at iteration 322400...\n",
            "Iteration 322400, Train Loss: 4.742552757263184 Eval Loss: 3.8122904682159424\n",
            "Running evaluation at iteration 322500...\n",
            "Iteration 322500, Train Loss: 3.6428465843200684 Eval Loss: 3.9074959707260133\n",
            "Running evaluation at iteration 322600...\n",
            "Iteration 322600, Train Loss: 3.716451406478882 Eval Loss: 3.866318941116333\n",
            "Running evaluation at iteration 322700...\n",
            "Iteration 322700, Train Loss: 3.280226945877075 Eval Loss: 3.836654303073883\n",
            "Running evaluation at iteration 322800...\n",
            "Iteration 322800, Train Loss: 3.67691707611084 Eval Loss: 3.862770924568176\n",
            "Running evaluation at iteration 322900...\n",
            "Iteration 322900, Train Loss: 3.633380651473999 Eval Loss: 3.815143339633942\n",
            "Running evaluation at iteration 323000...\n",
            "Iteration 323000, Train Loss: 3.760244607925415 Eval Loss: 3.8807075691223143\n",
            "Running evaluation at iteration 323100...\n",
            "Iteration 323100, Train Loss: 3.741241693496704 Eval Loss: 3.8115580224990846\n",
            "Running evaluation at iteration 323200...\n",
            "Iteration 323200, Train Loss: 4.622021675109863 Eval Loss: 3.814559369087219\n",
            "Running evaluation at iteration 323300...\n",
            "Iteration 323300, Train Loss: 4.100736141204834 Eval Loss: 3.901933526992798\n",
            "Running evaluation at iteration 323400...\n",
            "Iteration 323400, Train Loss: 3.361199378967285 Eval Loss: 3.8781866931915285\n",
            "Running evaluation at iteration 323500...\n",
            "Iteration 323500, Train Loss: 3.81904673576355 Eval Loss: 3.8396185636520386\n",
            "Running evaluation at iteration 323600...\n",
            "Iteration 323600, Train Loss: 3.6177895069122314 Eval Loss: 3.947044563293457\n",
            "Running evaluation at iteration 323700...\n",
            "Iteration 323700, Train Loss: 3.6703920364379883 Eval Loss: 3.762222714424133\n",
            "Running evaluation at iteration 323800...\n",
            "Iteration 323800, Train Loss: 3.71354341506958 Eval Loss: 3.7969384574890137\n",
            "Running evaluation at iteration 323900...\n",
            "Iteration 323900, Train Loss: 3.9783871173858643 Eval Loss: 3.886629972457886\n",
            "Running evaluation at iteration 324000...\n",
            "Iteration 324000, Train Loss: 4.129088878631592 Eval Loss: 3.8515469646453857\n",
            "Running evaluation at iteration 324100...\n",
            "Iteration 324100, Train Loss: 3.533707857131958 Eval Loss: 3.8094459772109985\n",
            "Running evaluation at iteration 324200...\n",
            "Iteration 324200, Train Loss: 3.581252336502075 Eval Loss: 3.911011509895325\n",
            "Running evaluation at iteration 324300...\n",
            "Iteration 324300, Train Loss: 3.328500509262085 Eval Loss: 3.875200653076172\n",
            "Running evaluation at iteration 324400...\n",
            "Iteration 324400, Train Loss: 4.6171650886535645 Eval Loss: 3.878864376544952\n",
            "Running evaluation at iteration 324500...\n",
            "Iteration 324500, Train Loss: 4.316946983337402 Eval Loss: 3.8805355167388917\n",
            "Running evaluation at iteration 324600...\n",
            "Iteration 324600, Train Loss: 3.830594539642334 Eval Loss: 3.838510665893555\n",
            "Running evaluation at iteration 324700...\n",
            "Iteration 324700, Train Loss: 3.7218778133392334 Eval Loss: 3.8925984477996827\n",
            "Running evaluation at iteration 324800...\n",
            "Iteration 324800, Train Loss: 4.246767997741699 Eval Loss: 3.886558384895325\n",
            "Running evaluation at iteration 324900...\n",
            "Iteration 324900, Train Loss: 3.592074394226074 Eval Loss: 3.7926548385620116\n",
            "Running evaluation at iteration 325000...\n",
            "Iteration 325000, Train Loss: 5.040154457092285 Eval Loss: 3.8850825452804565\n",
            "Running evaluation at iteration 325100...\n",
            "Iteration 325100, Train Loss: 3.6999549865722656 Eval Loss: 3.8524191999435424\n",
            "Running evaluation at iteration 325200...\n",
            "Iteration 325200, Train Loss: 4.274501323699951 Eval Loss: 3.887011444568634\n",
            "Running evaluation at iteration 325300...\n",
            "Iteration 325300, Train Loss: 2.767380952835083 Eval Loss: 3.779050920009613\n",
            "Running evaluation at iteration 325400...\n",
            "Iteration 325400, Train Loss: 4.023245811462402 Eval Loss: 3.9189874482154847\n",
            "Running evaluation at iteration 325500...\n",
            "Iteration 325500, Train Loss: 5.02252197265625 Eval Loss: 3.7664351868629455\n",
            "Running evaluation at iteration 325600...\n",
            "Iteration 325600, Train Loss: 2.873990535736084 Eval Loss: 3.8559740781784058\n",
            "Running evaluation at iteration 325700...\n",
            "Iteration 325700, Train Loss: 4.069013595581055 Eval Loss: 3.7331158256530763\n",
            "Running evaluation at iteration 325800...\n",
            "Iteration 325800, Train Loss: 3.683382511138916 Eval Loss: 3.7747312784194946\n",
            "Running evaluation at iteration 325900...\n",
            "Iteration 325900, Train Loss: 3.737121820449829 Eval Loss: 3.88142005443573\n",
            "Running evaluation at iteration 326000...\n",
            "Iteration 326000, Train Loss: 4.168610572814941 Eval Loss: 3.7454472184181213\n",
            "Running evaluation at iteration 326100...\n",
            "Iteration 326100, Train Loss: 3.568816661834717 Eval Loss: 3.865784969329834\n",
            "Running evaluation at iteration 326200...\n",
            "Iteration 326200, Train Loss: 4.75473165512085 Eval Loss: 3.812940676212311\n",
            "Running evaluation at iteration 326300...\n",
            "Iteration 326300, Train Loss: 3.3859660625457764 Eval Loss: 3.9269872879981995\n",
            "Running evaluation at iteration 326400...\n",
            "Iteration 326400, Train Loss: 3.3761167526245117 Eval Loss: 3.840038363933563\n",
            "Running evaluation at iteration 326500...\n",
            "Iteration 326500, Train Loss: 3.151754140853882 Eval Loss: 3.8464498138427734\n",
            "Running evaluation at iteration 326600...\n",
            "Iteration 326600, Train Loss: 4.188939094543457 Eval Loss: 3.829546790122986\n",
            "Running evaluation at iteration 326700...\n",
            "Iteration 326700, Train Loss: 4.361784934997559 Eval Loss: 3.7841845107078553\n",
            "Running evaluation at iteration 326800...\n",
            "Iteration 326800, Train Loss: 2.8487253189086914 Eval Loss: 3.8157912397384646\n",
            "Running evaluation at iteration 326900...\n",
            "Iteration 326900, Train Loss: 3.6406564712524414 Eval Loss: 3.87136705160141\n",
            "Running evaluation at iteration 327000...\n",
            "Iteration 327000, Train Loss: 3.408651351928711 Eval Loss: 3.891055159568787\n",
            "Running evaluation at iteration 327100...\n",
            "Iteration 327100, Train Loss: 4.202785968780518 Eval Loss: 3.771389706134796\n",
            "Running evaluation at iteration 327200...\n",
            "Iteration 327200, Train Loss: 3.685426950454712 Eval Loss: 3.879806740283966\n",
            "Running evaluation at iteration 327300...\n",
            "Iteration 327300, Train Loss: 4.047879219055176 Eval Loss: 3.7220202803611757\n",
            "Running evaluation at iteration 327400...\n",
            "Iteration 327400, Train Loss: 4.9669108390808105 Eval Loss: 3.8873029565811157\n",
            "Running evaluation at iteration 327500...\n",
            "Iteration 327500, Train Loss: 3.576594829559326 Eval Loss: 3.8903364515304566\n",
            "Running evaluation at iteration 327600...\n",
            "Iteration 327600, Train Loss: 3.742356061935425 Eval Loss: 3.9079892230033875\n",
            "Running evaluation at iteration 327700...\n",
            "Iteration 327700, Train Loss: 3.2367312908172607 Eval Loss: 3.771594922542572\n",
            "Running evaluation at iteration 327800...\n",
            "Iteration 327800, Train Loss: 3.9458730220794678 Eval Loss: 3.891315677165985\n",
            "Running evaluation at iteration 327900...\n",
            "Iteration 327900, Train Loss: 3.4127933979034424 Eval Loss: 3.8357579588890074\n",
            "Running evaluation at iteration 328000...\n",
            "Iteration 328000, Train Loss: 4.605864524841309 Eval Loss: 3.919684228897095\n",
            "Running evaluation at iteration 328100...\n",
            "Iteration 328100, Train Loss: 3.8945248126983643 Eval Loss: 3.8572510743141173\n",
            "Running evaluation at iteration 328200...\n",
            "Iteration 328200, Train Loss: 3.8343393802642822 Eval Loss: 3.7670210289955137\n",
            "Running evaluation at iteration 328300...\n",
            "Iteration 328300, Train Loss: 4.520251274108887 Eval Loss: 3.854598443508148\n",
            "Running evaluation at iteration 328400...\n",
            "Iteration 328400, Train Loss: 4.697937488555908 Eval Loss: 3.8113895082473754\n",
            "Running evaluation at iteration 328500...\n",
            "Iteration 328500, Train Loss: 3.8440606594085693 Eval Loss: 3.824371049404144\n",
            "Running evaluation at iteration 328600...\n",
            "Iteration 328600, Train Loss: 3.759328842163086 Eval Loss: 3.921464190483093\n",
            "Running evaluation at iteration 328700...\n",
            "Iteration 328700, Train Loss: 3.758803129196167 Eval Loss: 3.84021543264389\n",
            "Running evaluation at iteration 328800...\n",
            "Iteration 328800, Train Loss: 4.012636184692383 Eval Loss: 3.8789950251579284\n",
            "Running evaluation at iteration 328900...\n",
            "Iteration 328900, Train Loss: 3.3845837116241455 Eval Loss: 3.920514645576477\n",
            "Running evaluation at iteration 329000...\n",
            "Iteration 329000, Train Loss: 3.7561163902282715 Eval Loss: 3.864067850112915\n",
            "Running evaluation at iteration 329100...\n",
            "Iteration 329100, Train Loss: 4.167053699493408 Eval Loss: 3.8056449151039122\n",
            "Running evaluation at iteration 329200...\n",
            "Iteration 329200, Train Loss: 3.8963682651519775 Eval Loss: 3.791205279827118\n",
            "Running evaluation at iteration 329300...\n",
            "Iteration 329300, Train Loss: 4.245057582855225 Eval Loss: 3.8562451291084288\n",
            "Running evaluation at iteration 329400...\n",
            "Iteration 329400, Train Loss: 3.1622910499572754 Eval Loss: 3.776566174030304\n",
            "Running evaluation at iteration 329500...\n",
            "Iteration 329500, Train Loss: 3.0851376056671143 Eval Loss: 3.7365268087387085\n",
            "Running evaluation at iteration 329600...\n",
            "Iteration 329600, Train Loss: 3.43013596534729 Eval Loss: 3.9211225843429567\n",
            "Running evaluation at iteration 329700...\n",
            "Iteration 329700, Train Loss: 2.8218488693237305 Eval Loss: 3.7724916791915892\n",
            "Running evaluation at iteration 329800...\n",
            "Iteration 329800, Train Loss: 4.1712164878845215 Eval Loss: 3.7729389452934265\n",
            "Running evaluation at iteration 329900...\n",
            "Iteration 329900, Train Loss: 3.3045599460601807 Eval Loss: 3.863456447124481\n",
            "Running evaluation at iteration 330000...\n",
            "Iteration 330000, Train Loss: 3.804548740386963 Eval Loss: 3.7454882502555846\n",
            "Running evaluation at iteration 330100...\n",
            "Iteration 330100, Train Loss: 4.188842296600342 Eval Loss: 3.7997334384918213\n",
            "Running evaluation at iteration 330200...\n",
            "Iteration 330200, Train Loss: 3.5937983989715576 Eval Loss: 3.8820118355751037\n",
            "Running evaluation at iteration 330300...\n",
            "Iteration 330300, Train Loss: 4.8481597900390625 Eval Loss: 3.793314003944397\n",
            "Running evaluation at iteration 330400...\n",
            "Iteration 330400, Train Loss: 3.9405264854431152 Eval Loss: 3.8395739507675173\n",
            "Running evaluation at iteration 330500...\n",
            "Iteration 330500, Train Loss: 4.111834526062012 Eval Loss: 3.874265367984772\n",
            "Running evaluation at iteration 330600...\n",
            "Iteration 330600, Train Loss: 4.281952857971191 Eval Loss: 3.7838082766532897\n",
            "Running evaluation at iteration 330700...\n",
            "Iteration 330700, Train Loss: 4.538008689880371 Eval Loss: 3.908934683799744\n",
            "Running evaluation at iteration 330800...\n",
            "Iteration 330800, Train Loss: 4.016322135925293 Eval Loss: 3.802714331150055\n",
            "Running evaluation at iteration 330900...\n",
            "Iteration 330900, Train Loss: 3.7632362842559814 Eval Loss: 3.838481395244598\n",
            "Running evaluation at iteration 331000...\n",
            "Iteration 331000, Train Loss: 5.067500591278076 Eval Loss: 3.7943516159057618\n",
            "Running evaluation at iteration 331100...\n",
            "Iteration 331100, Train Loss: 4.492430210113525 Eval Loss: 3.888201744556427\n",
            "Running evaluation at iteration 331200...\n",
            "Iteration 331200, Train Loss: 4.678470134735107 Eval Loss: 3.8391979050636293\n",
            "Running evaluation at iteration 331300...\n",
            "Iteration 331300, Train Loss: 3.683492660522461 Eval Loss: 3.8908069849014284\n",
            "Running evaluation at iteration 331400...\n",
            "Iteration 331400, Train Loss: 4.190544605255127 Eval Loss: 3.8204186391830444\n",
            "Running evaluation at iteration 331500...\n",
            "Iteration 331500, Train Loss: 4.0303544998168945 Eval Loss: 3.8069329190254213\n",
            "Running evaluation at iteration 331600...\n",
            "Iteration 331600, Train Loss: 3.4583699703216553 Eval Loss: 3.7429520106315612\n",
            "Running evaluation at iteration 331700...\n",
            "Iteration 331700, Train Loss: 4.036538124084473 Eval Loss: 3.7804335379600524\n",
            "Running evaluation at iteration 331800...\n",
            "Iteration 331800, Train Loss: 3.793911933898926 Eval Loss: 3.882527232170105\n",
            "Running evaluation at iteration 331900...\n",
            "Iteration 331900, Train Loss: 5.25314474105835 Eval Loss: 3.8881749510765076\n",
            "Running evaluation at iteration 332000...\n",
            "Iteration 332000, Train Loss: 3.3598692417144775 Eval Loss: 3.8534253668785095\n",
            "Running evaluation at iteration 332100...\n",
            "Iteration 332100, Train Loss: 4.398698806762695 Eval Loss: 3.8895918774604796\n",
            "Running evaluation at iteration 332200...\n",
            "Iteration 332200, Train Loss: 4.015622615814209 Eval Loss: 3.7756454968452453\n",
            "Running evaluation at iteration 332300...\n",
            "Iteration 332300, Train Loss: 4.191267490386963 Eval Loss: 3.8553184723854064\n",
            "Running evaluation at iteration 332400...\n",
            "Iteration 332400, Train Loss: 3.4283061027526855 Eval Loss: 3.921435215473175\n",
            "Running evaluation at iteration 332500...\n",
            "Iteration 332500, Train Loss: 3.1937146186828613 Eval Loss: 3.8265715622901917\n",
            "Running evaluation at iteration 332600...\n",
            "Iteration 332600, Train Loss: 3.9813249111175537 Eval Loss: 3.8688676238059996\n",
            "Running evaluation at iteration 332700...\n",
            "Iteration 332700, Train Loss: 3.5752875804901123 Eval Loss: 3.836965067386627\n",
            "Running evaluation at iteration 332800...\n",
            "Iteration 332800, Train Loss: 3.900606632232666 Eval Loss: 3.800031533241272\n",
            "Running evaluation at iteration 332900...\n",
            "Iteration 332900, Train Loss: 3.777357816696167 Eval Loss: 3.8019395875930786\n",
            "Running evaluation at iteration 333000...\n",
            "Iteration 333000, Train Loss: 3.753063440322876 Eval Loss: 3.8537774324417113\n",
            "Running evaluation at iteration 333100...\n",
            "Iteration 333100, Train Loss: 4.090578556060791 Eval Loss: 3.8401677584648133\n",
            "Running evaluation at iteration 333200...\n",
            "Iteration 333200, Train Loss: 4.14257287979126 Eval Loss: 3.841083188056946\n",
            "Running evaluation at iteration 333300...\n",
            "Iteration 333300, Train Loss: 3.690326690673828 Eval Loss: 3.7809458589553833\n",
            "Running evaluation at iteration 333400...\n",
            "Iteration 333400, Train Loss: 4.108953952789307 Eval Loss: 3.846406934261322\n",
            "Running evaluation at iteration 333500...\n",
            "Iteration 333500, Train Loss: 3.9399781227111816 Eval Loss: 3.8592095804214477\n",
            "Running evaluation at iteration 333600...\n",
            "Iteration 333600, Train Loss: 3.76973819732666 Eval Loss: 3.8512426948547365\n",
            "Running evaluation at iteration 333700...\n",
            "Iteration 333700, Train Loss: 4.427250862121582 Eval Loss: 3.8393106269836426\n",
            "Running evaluation at iteration 333800...\n",
            "Iteration 333800, Train Loss: 3.148607015609741 Eval Loss: 3.8643426966667174\n",
            "Running evaluation at iteration 333900...\n",
            "Iteration 333900, Train Loss: 4.1222991943359375 Eval Loss: 3.8307414627075196\n",
            "Running evaluation at iteration 334000...\n",
            "Iteration 334000, Train Loss: 3.650472640991211 Eval Loss: 3.9230552744865417\n",
            "Running evaluation at iteration 334100...\n",
            "Iteration 334100, Train Loss: 3.228428602218628 Eval Loss: 3.8776085901260378\n",
            "Running evaluation at iteration 334200...\n",
            "Iteration 334200, Train Loss: 3.6009132862091064 Eval Loss: 3.830330128669739\n",
            "Running evaluation at iteration 334300...\n",
            "Iteration 334300, Train Loss: 3.3385169506073 Eval Loss: 3.73289785861969\n",
            "Running evaluation at iteration 334400...\n",
            "Iteration 334400, Train Loss: 3.463578462600708 Eval Loss: 3.8668238520622253\n",
            "Running evaluation at iteration 334500...\n",
            "Iteration 334500, Train Loss: 3.399293899536133 Eval Loss: 3.8491537523269654\n",
            "Running evaluation at iteration 334600...\n",
            "Iteration 334600, Train Loss: 3.5068931579589844 Eval Loss: 3.8757883501052857\n",
            "Running evaluation at iteration 334700...\n",
            "Iteration 334700, Train Loss: 3.555368423461914 Eval Loss: 3.7426558542251587\n",
            "Running evaluation at iteration 334800...\n",
            "Iteration 334800, Train Loss: 3.069924831390381 Eval Loss: 3.810354859828949\n",
            "Running evaluation at iteration 334900...\n",
            "Iteration 334900, Train Loss: 3.6075072288513184 Eval Loss: 3.764666345119476\n",
            "Running evaluation at iteration 335000...\n",
            "Iteration 335000, Train Loss: 4.12507963180542 Eval Loss: 3.767434334754944\n",
            "Running evaluation at iteration 335100...\n",
            "Iteration 335100, Train Loss: 3.1982085704803467 Eval Loss: 3.879810538291931\n",
            "Running evaluation at iteration 335200...\n",
            "Iteration 335200, Train Loss: 3.5200917720794678 Eval Loss: 3.89753520488739\n",
            "Running evaluation at iteration 335300...\n",
            "Iteration 335300, Train Loss: 3.907627820968628 Eval Loss: 3.9048719429969787\n",
            "Running evaluation at iteration 335400...\n",
            "Iteration 335400, Train Loss: 3.458380937576294 Eval Loss: 3.781257333755493\n",
            "Running evaluation at iteration 335500...\n",
            "Iteration 335500, Train Loss: 4.99656867980957 Eval Loss: 3.7945575952529906\n",
            "Running evaluation at iteration 335600...\n",
            "Iteration 335600, Train Loss: 3.613426685333252 Eval Loss: 3.8195006322860716\n",
            "Running evaluation at iteration 335700...\n",
            "Iteration 335700, Train Loss: 3.859116315841675 Eval Loss: 3.8352945327758787\n",
            "Running evaluation at iteration 335800...\n",
            "Iteration 335800, Train Loss: 2.9985005855560303 Eval Loss: 3.8679302859306337\n",
            "Running evaluation at iteration 335900...\n",
            "Iteration 335900, Train Loss: 4.259885787963867 Eval Loss: 3.85364884853363\n",
            "Running evaluation at iteration 336000...\n",
            "Iteration 336000, Train Loss: 3.8009822368621826 Eval Loss: 3.8421969175338746\n",
            "Running evaluation at iteration 336100...\n",
            "Iteration 336100, Train Loss: 4.01528263092041 Eval Loss: 3.874215066432953\n",
            "Running evaluation at iteration 336200...\n",
            "Iteration 336200, Train Loss: 4.336178779602051 Eval Loss: 3.8989621686935423\n",
            "Running evaluation at iteration 336300...\n",
            "Iteration 336300, Train Loss: 3.7913801670074463 Eval Loss: 3.7854651761054994\n",
            "Running evaluation at iteration 336400...\n",
            "Iteration 336400, Train Loss: 3.5461206436157227 Eval Loss: 3.7671491050720216\n",
            "Running evaluation at iteration 336500...\n",
            "Iteration 336500, Train Loss: 5.465851306915283 Eval Loss: 3.8527713942527773\n",
            "Running evaluation at iteration 336600...\n",
            "Iteration 336600, Train Loss: 4.3482770919799805 Eval Loss: 3.776196925640106\n",
            "Running evaluation at iteration 336700...\n",
            "Iteration 336700, Train Loss: 4.243077278137207 Eval Loss: 3.943895878791809\n",
            "Running evaluation at iteration 336800...\n",
            "Iteration 336800, Train Loss: 3.205839157104492 Eval Loss: 3.7909158062934876\n",
            "Running evaluation at iteration 336900...\n",
            "Iteration 336900, Train Loss: 3.540534019470215 Eval Loss: 3.9118078327178956\n",
            "Running evaluation at iteration 337000...\n",
            "Iteration 337000, Train Loss: 3.9992003440856934 Eval Loss: 3.882481667995453\n",
            "Running evaluation at iteration 337100...\n",
            "Iteration 337100, Train Loss: 3.296919107437134 Eval Loss: 3.953622877597809\n",
            "Running evaluation at iteration 337200...\n",
            "Iteration 337200, Train Loss: 4.096334934234619 Eval Loss: 3.838930847644806\n",
            "Running evaluation at iteration 337300...\n",
            "Iteration 337300, Train Loss: 3.918647527694702 Eval Loss: 3.8367895913124084\n",
            "Running evaluation at iteration 337400...\n",
            "Iteration 337400, Train Loss: 3.551208019256592 Eval Loss: 3.781055681705475\n",
            "Running evaluation at iteration 337500...\n",
            "Iteration 337500, Train Loss: 3.037605047225952 Eval Loss: 3.7326862359046937\n",
            "Running evaluation at iteration 337600...\n",
            "Iteration 337600, Train Loss: 3.544903039932251 Eval Loss: 3.8961301469802856\n",
            "Running evaluation at iteration 337700...\n",
            "Iteration 337700, Train Loss: 3.808673858642578 Eval Loss: 3.7804166865348816\n",
            "Running evaluation at iteration 337800...\n",
            "Iteration 337800, Train Loss: 4.749091625213623 Eval Loss: 3.866031789779663\n",
            "Running evaluation at iteration 337900...\n",
            "Iteration 337900, Train Loss: 4.416062831878662 Eval Loss: 3.8430650281906127\n",
            "Running evaluation at iteration 338000...\n",
            "Iteration 338000, Train Loss: 5.131985187530518 Eval Loss: 3.73459276676178\n",
            "Running evaluation at iteration 338100...\n",
            "Iteration 338100, Train Loss: 3.273682117462158 Eval Loss: 3.917673661708832\n",
            "Running evaluation at iteration 338200...\n",
            "Iteration 338200, Train Loss: 3.7425920963287354 Eval Loss: 3.9266630482673643\n",
            "Running evaluation at iteration 338300...\n",
            "Iteration 338300, Train Loss: 4.175065517425537 Eval Loss: 3.873352415561676\n",
            "Running evaluation at iteration 338400...\n",
            "Iteration 338400, Train Loss: 4.579278469085693 Eval Loss: 3.9338131451606753\n",
            "Running evaluation at iteration 338500...\n",
            "Iteration 338500, Train Loss: 3.4183297157287598 Eval Loss: 3.7898212599754335\n",
            "Running evaluation at iteration 338600...\n",
            "Iteration 338600, Train Loss: 2.8105015754699707 Eval Loss: 3.8686480808258055\n",
            "Running evaluation at iteration 338700...\n",
            "Iteration 338700, Train Loss: 4.145353317260742 Eval Loss: 3.847192978858948\n",
            "Running evaluation at iteration 338800...\n",
            "Iteration 338800, Train Loss: 4.391176700592041 Eval Loss: 3.7673653984069824\n",
            "Running evaluation at iteration 338900...\n",
            "Iteration 338900, Train Loss: 2.818545341491699 Eval Loss: 3.719067769050598\n",
            "Running evaluation at iteration 339000...\n",
            "Iteration 339000, Train Loss: 4.4392619132995605 Eval Loss: 3.857890160083771\n",
            "Running evaluation at iteration 339100...\n",
            "Iteration 339100, Train Loss: 3.4885096549987793 Eval Loss: 3.7596780014038087\n",
            "Running evaluation at iteration 339200...\n",
            "Iteration 339200, Train Loss: 2.7261147499084473 Eval Loss: 3.822010192871094\n",
            "Running evaluation at iteration 339300...\n",
            "Iteration 339300, Train Loss: 3.9466702938079834 Eval Loss: 3.8469466757774353\n",
            "Running evaluation at iteration 339400...\n",
            "Iteration 339400, Train Loss: 5.005343437194824 Eval Loss: 3.8847358059883117\n",
            "Running evaluation at iteration 339500...\n",
            "Iteration 339500, Train Loss: 4.249578475952148 Eval Loss: 3.808424892425537\n",
            "Running evaluation at iteration 339600...\n",
            "Iteration 339600, Train Loss: 4.25734281539917 Eval Loss: 3.8130062770843507\n",
            "Running evaluation at iteration 339700...\n",
            "Iteration 339700, Train Loss: 4.199685573577881 Eval Loss: 3.7469746041297913\n",
            "Running evaluation at iteration 339800...\n",
            "Iteration 339800, Train Loss: 3.061396837234497 Eval Loss: 3.8255662393569945\n",
            "Running evaluation at iteration 339900...\n",
            "Iteration 339900, Train Loss: 4.9904255867004395 Eval Loss: 3.799877419471741\n",
            "Running evaluation at iteration 340000...\n",
            "Iteration 340000, Train Loss: 4.718041896820068 Eval Loss: 3.862085859775543\n",
            "Running evaluation at iteration 340100...\n",
            "Iteration 340100, Train Loss: 2.917720317840576 Eval Loss: 3.8770507049560545\n",
            "Running evaluation at iteration 340200...\n",
            "Iteration 340200, Train Loss: 4.610703945159912 Eval Loss: 3.8241566491127013\n",
            "Running evaluation at iteration 340300...\n",
            "Iteration 340300, Train Loss: 4.3883466720581055 Eval Loss: 3.757676913738251\n",
            "Running evaluation at iteration 340400...\n",
            "Iteration 340400, Train Loss: 3.988494873046875 Eval Loss: 3.923404026031494\n",
            "Running evaluation at iteration 340500...\n",
            "Iteration 340500, Train Loss: 4.074003219604492 Eval Loss: 3.819639337062836\n",
            "Running evaluation at iteration 340600...\n",
            "Iteration 340600, Train Loss: 3.770907402038574 Eval Loss: 3.751285970211029\n",
            "Running evaluation at iteration 340700...\n",
            "Iteration 340700, Train Loss: 4.01663875579834 Eval Loss: 3.7801616311073305\n",
            "Running evaluation at iteration 340800...\n",
            "Iteration 340800, Train Loss: 4.173245906829834 Eval Loss: 3.9188240694999696\n",
            "Running evaluation at iteration 340900...\n",
            "Iteration 340900, Train Loss: 4.751729488372803 Eval Loss: 3.682249460220337\n",
            "Running evaluation at iteration 341000...\n",
            "Iteration 341000, Train Loss: 4.034977436065674 Eval Loss: 3.85231404542923\n",
            "Running evaluation at iteration 341100...\n",
            "Iteration 341100, Train Loss: 3.7533228397369385 Eval Loss: 3.8718667817115784\n",
            "Running evaluation at iteration 341200...\n",
            "Iteration 341200, Train Loss: 3.871173620223999 Eval Loss: 3.7700264382362367\n",
            "Running evaluation at iteration 341300...\n",
            "Iteration 341300, Train Loss: 3.7152390480041504 Eval Loss: 3.936797471046448\n",
            "Running evaluation at iteration 341400...\n",
            "Iteration 341400, Train Loss: 3.52811861038208 Eval Loss: 3.882131989002228\n",
            "Running evaluation at iteration 341500...\n",
            "Iteration 341500, Train Loss: 3.7190089225769043 Eval Loss: 3.7794019412994384\n",
            "Running evaluation at iteration 341600...\n",
            "Iteration 341600, Train Loss: 3.9566054344177246 Eval Loss: 3.675906376838684\n",
            "Running evaluation at iteration 341700...\n",
            "Iteration 341700, Train Loss: 3.470653772354126 Eval Loss: 3.8059945249557496\n",
            "Running evaluation at iteration 341800...\n",
            "Iteration 341800, Train Loss: 3.2304184436798096 Eval Loss: 3.9160954332351685\n",
            "Running evaluation at iteration 341900...\n",
            "Iteration 341900, Train Loss: 3.0324606895446777 Eval Loss: 3.879979741573334\n",
            "Running evaluation at iteration 342000...\n",
            "Iteration 342000, Train Loss: 3.903698444366455 Eval Loss: 3.7499690008163453\n",
            "Running evaluation at iteration 342100...\n",
            "Iteration 342100, Train Loss: 4.032530307769775 Eval Loss: 3.780332350730896\n",
            "Running evaluation at iteration 342200...\n",
            "Iteration 342200, Train Loss: 4.803630828857422 Eval Loss: 3.8284105801582338\n",
            "Running evaluation at iteration 342300...\n",
            "Iteration 342300, Train Loss: 4.094568252563477 Eval Loss: 3.831205499172211\n",
            "Running evaluation at iteration 342400...\n",
            "Iteration 342400, Train Loss: 3.7532413005828857 Eval Loss: 3.8014969038963318\n",
            "Running evaluation at iteration 342500...\n",
            "Iteration 342500, Train Loss: 4.43437385559082 Eval Loss: 3.8249931645393374\n",
            "Running evaluation at iteration 342600...\n",
            "Iteration 342600, Train Loss: 4.110557556152344 Eval Loss: 3.7784617495536805\n",
            "Running evaluation at iteration 342700...\n",
            "Iteration 342700, Train Loss: 2.992271900177002 Eval Loss: 3.793636910915375\n",
            "Running evaluation at iteration 342800...\n",
            "Iteration 342800, Train Loss: 4.102432727813721 Eval Loss: 3.821038689613342\n",
            "Running evaluation at iteration 342900...\n",
            "Iteration 342900, Train Loss: 4.434395790100098 Eval Loss: 3.840487916469574\n",
            "Running evaluation at iteration 343000...\n",
            "Iteration 343000, Train Loss: 3.862314462661743 Eval Loss: 3.8445227217674254\n",
            "Running evaluation at iteration 343100...\n",
            "Iteration 343100, Train Loss: 3.3633980751037598 Eval Loss: 3.766454734802246\n",
            "Running evaluation at iteration 343200...\n",
            "Iteration 343200, Train Loss: 4.717112064361572 Eval Loss: 3.8320769095420837\n",
            "Running evaluation at iteration 343300...\n",
            "Iteration 343300, Train Loss: 3.4317851066589355 Eval Loss: 3.821125636100769\n",
            "Running evaluation at iteration 343400...\n",
            "Iteration 343400, Train Loss: 4.0831618309021 Eval Loss: 3.878287949562073\n",
            "Running evaluation at iteration 343500...\n",
            "Iteration 343500, Train Loss: 3.7500462532043457 Eval Loss: 3.8122982239723204\n",
            "Running evaluation at iteration 343600...\n",
            "Iteration 343600, Train Loss: 4.243794918060303 Eval Loss: 3.9457232332229615\n",
            "Running evaluation at iteration 343700...\n",
            "Iteration 343700, Train Loss: 3.84765625 Eval Loss: 3.8464743661880494\n",
            "Running evaluation at iteration 343800...\n",
            "Iteration 343800, Train Loss: 3.590430974960327 Eval Loss: 3.813254294395447\n",
            "Running evaluation at iteration 343900...\n",
            "Iteration 343900, Train Loss: 4.324901580810547 Eval Loss: 3.9168476557731626\n",
            "Running evaluation at iteration 344000...\n",
            "Iteration 344000, Train Loss: 4.3039679527282715 Eval Loss: 3.856572449207306\n",
            "Running evaluation at iteration 344100...\n",
            "Iteration 344100, Train Loss: 3.96919846534729 Eval Loss: 3.7821381068229676\n",
            "Running evaluation at iteration 344200...\n",
            "Iteration 344200, Train Loss: 2.981295108795166 Eval Loss: 3.8464613246917723\n",
            "Running evaluation at iteration 344300...\n",
            "Iteration 344300, Train Loss: 4.4132232666015625 Eval Loss: 3.8223727226257322\n",
            "Running evaluation at iteration 344400...\n",
            "Iteration 344400, Train Loss: 2.9459476470947266 Eval Loss: 3.8025408172607422\n",
            "Running evaluation at iteration 344500...\n",
            "Iteration 344500, Train Loss: 3.785303831100464 Eval Loss: 3.8207854437828064\n",
            "Running evaluation at iteration 344600...\n",
            "Iteration 344600, Train Loss: 2.759777307510376 Eval Loss: 3.858985359668732\n",
            "Running evaluation at iteration 344700...\n",
            "Iteration 344700, Train Loss: 4.987319469451904 Eval Loss: 3.841986849308014\n",
            "Running evaluation at iteration 344800...\n",
            "Iteration 344800, Train Loss: 3.9082467555999756 Eval Loss: 3.8298909187316896\n",
            "Running evaluation at iteration 344900...\n",
            "Iteration 344900, Train Loss: 3.27994704246521 Eval Loss: 3.8093215012550354\n",
            "Running evaluation at iteration 345000...\n",
            "Iteration 345000, Train Loss: 3.848759174346924 Eval Loss: 3.8702588868141174\n",
            "Running evaluation at iteration 345100...\n",
            "Iteration 345100, Train Loss: 4.459071159362793 Eval Loss: 3.7604386687278746\n",
            "Running evaluation at iteration 345200...\n",
            "Iteration 345200, Train Loss: 4.478940963745117 Eval Loss: 3.7360861110687256\n",
            "Running evaluation at iteration 345300...\n",
            "Iteration 345300, Train Loss: 4.65248441696167 Eval Loss: 3.8727869033813476\n",
            "Running evaluation at iteration 345400...\n",
            "Iteration 345400, Train Loss: 4.372534275054932 Eval Loss: 3.8680032014846804\n",
            "Running evaluation at iteration 345500...\n",
            "Iteration 345500, Train Loss: 3.776505708694458 Eval Loss: 3.7612176036834715\n",
            "Running evaluation at iteration 345600...\n",
            "Iteration 345600, Train Loss: 3.65149188041687 Eval Loss: 3.7493392205238343\n",
            "Running evaluation at iteration 345700...\n",
            "Iteration 345700, Train Loss: 4.128128528594971 Eval Loss: 3.892109069824219\n",
            "Running evaluation at iteration 345800...\n",
            "Iteration 345800, Train Loss: 3.468386650085449 Eval Loss: 3.8298655414581297\n",
            "Running evaluation at iteration 345900...\n",
            "Iteration 345900, Train Loss: 3.8652961254119873 Eval Loss: 3.8560550141334535\n",
            "Running evaluation at iteration 346000...\n",
            "Iteration 346000, Train Loss: 3.2770707607269287 Eval Loss: 3.8146897625923155\n",
            "Running evaluation at iteration 346100...\n",
            "Iteration 346100, Train Loss: 3.7222518920898438 Eval Loss: 3.8455548095703125\n",
            "Running evaluation at iteration 346200...\n",
            "Iteration 346200, Train Loss: 3.625882148742676 Eval Loss: 3.873115141391754\n",
            "Running evaluation at iteration 346300...\n",
            "Iteration 346300, Train Loss: 3.573622703552246 Eval Loss: 3.828447368144989\n",
            "Running evaluation at iteration 346400...\n",
            "Iteration 346400, Train Loss: 3.8661794662475586 Eval Loss: 3.855111017227173\n",
            "Running evaluation at iteration 346500...\n",
            "Iteration 346500, Train Loss: 3.0637078285217285 Eval Loss: 3.808409616947174\n",
            "Running evaluation at iteration 346600...\n",
            "Iteration 346600, Train Loss: 4.221829414367676 Eval Loss: 3.868229548931122\n",
            "Running evaluation at iteration 346700...\n",
            "Iteration 346700, Train Loss: 3.2916131019592285 Eval Loss: 3.757339334487915\n",
            "Running evaluation at iteration 346800...\n",
            "Iteration 346800, Train Loss: 5.012300491333008 Eval Loss: 3.7286343431472777\n",
            "Running evaluation at iteration 346900...\n",
            "Iteration 346900, Train Loss: 4.611632347106934 Eval Loss: 3.7880643701553343\n",
            "Running evaluation at iteration 347000...\n",
            "Iteration 347000, Train Loss: 2.995692014694214 Eval Loss: 3.773206579685211\n",
            "Running evaluation at iteration 347100...\n",
            "Iteration 347100, Train Loss: 4.18733024597168 Eval Loss: 3.881901950836182\n",
            "Running evaluation at iteration 347200...\n",
            "Iteration 347200, Train Loss: 4.2141313552856445 Eval Loss: 3.821231601238251\n",
            "Running evaluation at iteration 347300...\n",
            "Iteration 347300, Train Loss: 3.894334316253662 Eval Loss: 3.8292553353309633\n",
            "Running evaluation at iteration 347400...\n",
            "Iteration 347400, Train Loss: 4.031919956207275 Eval Loss: 3.809038531780243\n",
            "Running evaluation at iteration 347500...\n",
            "Iteration 347500, Train Loss: 4.6428093910217285 Eval Loss: 3.7410184788703917\n",
            "Running evaluation at iteration 347600...\n",
            "Iteration 347600, Train Loss: 3.170621395111084 Eval Loss: 3.942370207309723\n",
            "Running evaluation at iteration 347700...\n",
            "Iteration 347700, Train Loss: 2.7778329849243164 Eval Loss: 3.818269271850586\n",
            "Running evaluation at iteration 347800...\n",
            "Iteration 347800, Train Loss: 3.9413931369781494 Eval Loss: 3.799250659942627\n",
            "Running evaluation at iteration 347900...\n",
            "Iteration 347900, Train Loss: 3.4936304092407227 Eval Loss: 3.9166449236869814\n",
            "Running evaluation at iteration 348000...\n",
            "Iteration 348000, Train Loss: 3.986119270324707 Eval Loss: 3.8593744111061095\n",
            "Running evaluation at iteration 348100...\n",
            "Iteration 348100, Train Loss: 4.301286697387695 Eval Loss: 3.823244788646698\n",
            "Running evaluation at iteration 348200...\n",
            "Iteration 348200, Train Loss: 2.9348621368408203 Eval Loss: 3.6980386090278627\n",
            "Running evaluation at iteration 348300...\n",
            "Iteration 348300, Train Loss: 3.776298761367798 Eval Loss: 3.804501516819\n",
            "Running evaluation at iteration 348400...\n",
            "Iteration 348400, Train Loss: 2.9535953998565674 Eval Loss: 3.862737035751343\n",
            "Running evaluation at iteration 348500...\n",
            "Iteration 348500, Train Loss: 4.019966125488281 Eval Loss: 3.86451575756073\n",
            "Running evaluation at iteration 348600...\n",
            "Iteration 348600, Train Loss: 3.066767930984497 Eval Loss: 3.8330387091636657\n",
            "Running evaluation at iteration 348700...\n",
            "Iteration 348700, Train Loss: 3.681978225708008 Eval Loss: 3.925212235450745\n",
            "Running evaluation at iteration 348800...\n",
            "Iteration 348800, Train Loss: 5.325784206390381 Eval Loss: 3.8370242714881897\n",
            "Running evaluation at iteration 348900...\n",
            "Iteration 348900, Train Loss: 4.786204814910889 Eval Loss: 3.897484505176544\n",
            "Running evaluation at iteration 349000...\n",
            "Iteration 349000, Train Loss: 4.067824840545654 Eval Loss: 3.825234591960907\n",
            "Running evaluation at iteration 349100...\n",
            "Iteration 349100, Train Loss: 3.232694387435913 Eval Loss: 3.862758677005768\n",
            "Running evaluation at iteration 349200...\n",
            "Iteration 349200, Train Loss: 3.8835322856903076 Eval Loss: 3.8602982425689696\n",
            "Running evaluation at iteration 349300...\n",
            "Iteration 349300, Train Loss: 5.237638473510742 Eval Loss: 3.878055372238159\n",
            "Running evaluation at iteration 349400...\n",
            "Iteration 349400, Train Loss: 4.399929046630859 Eval Loss: 3.827057132720947\n",
            "Running evaluation at iteration 349500...\n",
            "Iteration 349500, Train Loss: 3.478123188018799 Eval Loss: 3.7901210594177246\n",
            "Running evaluation at iteration 349600...\n",
            "Iteration 349600, Train Loss: 4.038539886474609 Eval Loss: 3.7312596344947817\n",
            "Running evaluation at iteration 349700...\n",
            "Iteration 349700, Train Loss: 3.189004898071289 Eval Loss: 3.7955307269096377\n",
            "Running evaluation at iteration 349800...\n",
            "Iteration 349800, Train Loss: 3.76009202003479 Eval Loss: 3.874227373600006\n",
            "Running evaluation at iteration 349900...\n",
            "Iteration 349900, Train Loss: 3.370965003967285 Eval Loss: 3.808569061756134\n",
            "Running evaluation at iteration 350000...\n",
            "Iteration 350000, Train Loss: 2.591804265975952 Eval Loss: 3.935220856666565\n",
            "Running evaluation at iteration 350100...\n",
            "Iteration 350100, Train Loss: 4.092303276062012 Eval Loss: 3.7258296036720275\n",
            "Running evaluation at iteration 350200...\n",
            "Iteration 350200, Train Loss: 3.958516836166382 Eval Loss: 3.7866384267807005\n",
            "Running evaluation at iteration 350300...\n",
            "Iteration 350300, Train Loss: 2.9121294021606445 Eval Loss: 3.779990906715393\n",
            "Running evaluation at iteration 350400...\n",
            "Iteration 350400, Train Loss: 3.4685516357421875 Eval Loss: 3.834153015613556\n",
            "Running evaluation at iteration 350500...\n",
            "Iteration 350500, Train Loss: 3.3763301372528076 Eval Loss: 3.8359265303611756\n",
            "Running evaluation at iteration 350600...\n",
            "Iteration 350600, Train Loss: 3.4114890098571777 Eval Loss: 3.7581120204925536\n",
            "Running evaluation at iteration 350700...\n",
            "Iteration 350700, Train Loss: 3.594212532043457 Eval Loss: 3.9716608572006225\n",
            "Running evaluation at iteration 350800...\n",
            "Iteration 350800, Train Loss: 4.464038372039795 Eval Loss: 3.8858585429191588\n",
            "Running evaluation at iteration 350900...\n",
            "Iteration 350900, Train Loss: 3.6923012733459473 Eval Loss: 3.804769718647003\n",
            "Running evaluation at iteration 351000...\n",
            "Iteration 351000, Train Loss: 4.049347877502441 Eval Loss: 3.780631000995636\n",
            "Running evaluation at iteration 351100...\n",
            "Iteration 351100, Train Loss: 3.7337968349456787 Eval Loss: 3.867844033241272\n",
            "Running evaluation at iteration 351200...\n",
            "Iteration 351200, Train Loss: 3.4360508918762207 Eval Loss: 3.805497705936432\n",
            "Running evaluation at iteration 351300...\n",
            "Iteration 351300, Train Loss: 4.642496109008789 Eval Loss: 3.7672291016578674\n",
            "Running evaluation at iteration 351400...\n",
            "Iteration 351400, Train Loss: 3.632408380508423 Eval Loss: 3.846913051605225\n",
            "Running evaluation at iteration 351500...\n",
            "Iteration 351500, Train Loss: 3.930414915084839 Eval Loss: 3.845410921573639\n",
            "Running evaluation at iteration 351600...\n",
            "Iteration 351600, Train Loss: 3.7004590034484863 Eval Loss: 3.8250409030914305\n",
            "Running evaluation at iteration 351700...\n",
            "Iteration 351700, Train Loss: 3.888258457183838 Eval Loss: 3.801776783466339\n",
            "Running evaluation at iteration 351800...\n",
            "Iteration 351800, Train Loss: 3.636993169784546 Eval Loss: 3.7892199468612673\n",
            "Running evaluation at iteration 351900...\n",
            "Iteration 351900, Train Loss: 3.596684217453003 Eval Loss: 3.8288200974464415\n",
            "Running evaluation at iteration 352000...\n",
            "Iteration 352000, Train Loss: 3.0248513221740723 Eval Loss: 3.8609818148612978\n",
            "Running evaluation at iteration 352100...\n",
            "Iteration 352100, Train Loss: 3.7655818462371826 Eval Loss: 3.864644477367401\n",
            "Running evaluation at iteration 352200...\n",
            "Iteration 352200, Train Loss: 3.4622018337249756 Eval Loss: 3.7955627965927126\n",
            "Running evaluation at iteration 352300...\n",
            "Iteration 352300, Train Loss: 3.689617395401001 Eval Loss: 3.894835441112518\n",
            "Running evaluation at iteration 352400...\n",
            "Iteration 352400, Train Loss: 4.392534255981445 Eval Loss: 3.9356575536727907\n",
            "Running evaluation at iteration 352500...\n",
            "Iteration 352500, Train Loss: 3.6936845779418945 Eval Loss: 3.8491112875938414\n",
            "Running evaluation at iteration 352600...\n",
            "Iteration 352600, Train Loss: 3.2177610397338867 Eval Loss: 3.8031442737579346\n",
            "Running evaluation at iteration 352700...\n",
            "Iteration 352700, Train Loss: 4.263515949249268 Eval Loss: 3.846297540664673\n",
            "Running evaluation at iteration 352800...\n",
            "Iteration 352800, Train Loss: 4.005986213684082 Eval Loss: 3.8228470921516418\n",
            "Running evaluation at iteration 352900...\n",
            "Iteration 352900, Train Loss: 3.1957476139068604 Eval Loss: 3.7543869638442993\n",
            "Running evaluation at iteration 353000...\n",
            "Iteration 353000, Train Loss: 3.989061117172241 Eval Loss: 3.9198532485961914\n",
            "Running evaluation at iteration 353100...\n",
            "Iteration 353100, Train Loss: 3.5267741680145264 Eval Loss: 3.816516921520233\n",
            "Running evaluation at iteration 353200...\n",
            "Iteration 353200, Train Loss: 3.6462457180023193 Eval Loss: 3.805564558506012\n",
            "Running evaluation at iteration 353300...\n",
            "Iteration 353300, Train Loss: 3.4281792640686035 Eval Loss: 3.83990816116333\n",
            "Running evaluation at iteration 353400...\n",
            "Iteration 353400, Train Loss: 3.745837688446045 Eval Loss: 3.8364500832557678\n",
            "Running evaluation at iteration 353500...\n",
            "Iteration 353500, Train Loss: 3.641972541809082 Eval Loss: 3.8170639729499816\n",
            "Running evaluation at iteration 353600...\n",
            "Iteration 353600, Train Loss: 2.905219078063965 Eval Loss: 3.817266993522644\n",
            "Running evaluation at iteration 353700...\n",
            "Iteration 353700, Train Loss: 3.8200302124023438 Eval Loss: 3.7971408057212828\n",
            "Running evaluation at iteration 353800...\n",
            "Iteration 353800, Train Loss: 2.9944980144500732 Eval Loss: 3.862370352745056\n",
            "Running evaluation at iteration 353900...\n",
            "Iteration 353900, Train Loss: 3.433173418045044 Eval Loss: 3.8099752140045164\n",
            "Running evaluation at iteration 354000...\n",
            "Iteration 354000, Train Loss: 3.795983076095581 Eval Loss: 3.9388711214065553\n",
            "Running evaluation at iteration 354100...\n",
            "Iteration 354100, Train Loss: 4.3108601570129395 Eval Loss: 3.8167366075515745\n",
            "Running evaluation at iteration 354200...\n",
            "Iteration 354200, Train Loss: 4.2382493019104 Eval Loss: 3.8366944193840027\n",
            "Running evaluation at iteration 354300...\n",
            "Iteration 354300, Train Loss: 3.2266998291015625 Eval Loss: 3.778217606544495\n",
            "Running evaluation at iteration 354400...\n",
            "Iteration 354400, Train Loss: 4.055691719055176 Eval Loss: 3.8103614449501038\n",
            "Running evaluation at iteration 354500...\n",
            "Iteration 354500, Train Loss: 3.7556233406066895 Eval Loss: 3.8505952024459837\n",
            "Running evaluation at iteration 354600...\n",
            "Iteration 354600, Train Loss: 3.8215394020080566 Eval Loss: 3.8600820875167847\n",
            "Running evaluation at iteration 354700...\n",
            "Iteration 354700, Train Loss: 5.164979934692383 Eval Loss: 3.80572455406189\n",
            "Running evaluation at iteration 354800...\n",
            "Iteration 354800, Train Loss: 4.1547627449035645 Eval Loss: 3.847763397693634\n",
            "Running evaluation at iteration 354900...\n",
            "Iteration 354900, Train Loss: 3.7010881900787354 Eval Loss: 3.8855119562149047\n",
            "Running evaluation at iteration 355000...\n",
            "Iteration 355000, Train Loss: 4.252023220062256 Eval Loss: 3.8069115900993347\n",
            "Running evaluation at iteration 355100...\n",
            "Iteration 355100, Train Loss: 3.465033769607544 Eval Loss: 3.845856750011444\n",
            "Running evaluation at iteration 355200...\n",
            "Iteration 355200, Train Loss: 3.007643461227417 Eval Loss: 3.8422415399551393\n",
            "Running evaluation at iteration 355300...\n",
            "Iteration 355300, Train Loss: 3.3679358959198 Eval Loss: 3.950054864883423\n",
            "Running evaluation at iteration 355400...\n",
            "Iteration 355400, Train Loss: 3.7721381187438965 Eval Loss: 3.7638068866729735\n",
            "Running evaluation at iteration 355500...\n",
            "Iteration 355500, Train Loss: 4.021978855133057 Eval Loss: 3.804528648853302\n",
            "Running evaluation at iteration 355600...\n",
            "Iteration 355600, Train Loss: 3.6314873695373535 Eval Loss: 3.8063329601287843\n",
            "Running evaluation at iteration 355700...\n",
            "Iteration 355700, Train Loss: 4.313716888427734 Eval Loss: 3.783299307823181\n",
            "Running evaluation at iteration 355800...\n",
            "Iteration 355800, Train Loss: 4.236009120941162 Eval Loss: 3.7693409633636477\n",
            "Running evaluation at iteration 355900...\n",
            "Iteration 355900, Train Loss: 3.0865938663482666 Eval Loss: 3.836937818527222\n",
            "Running evaluation at iteration 356000...\n",
            "Iteration 356000, Train Loss: 3.2007036209106445 Eval Loss: 3.8394489526748656\n",
            "Running evaluation at iteration 356100...\n",
            "Iteration 356100, Train Loss: 4.894376277923584 Eval Loss: 3.8409873747825625\n",
            "Running evaluation at iteration 356200...\n",
            "Iteration 356200, Train Loss: 3.057974100112915 Eval Loss: 3.7964094352722166\n",
            "Running evaluation at iteration 356300...\n",
            "Iteration 356300, Train Loss: 4.442963600158691 Eval Loss: 3.7761490082740785\n",
            "Running evaluation at iteration 356400...\n",
            "Iteration 356400, Train Loss: 3.7654879093170166 Eval Loss: 3.889178478717804\n",
            "Running evaluation at iteration 356500...\n",
            "Iteration 356500, Train Loss: 4.1764702796936035 Eval Loss: 3.8702857160568236\n",
            "Running evaluation at iteration 356600...\n",
            "Iteration 356600, Train Loss: 3.435441493988037 Eval Loss: 3.854603974819183\n",
            "Running evaluation at iteration 356700...\n",
            "Iteration 356700, Train Loss: 3.2875075340270996 Eval Loss: 3.8939979600906374\n",
            "Running evaluation at iteration 356800...\n",
            "Iteration 356800, Train Loss: 3.211493730545044 Eval Loss: 3.7871690702438356\n",
            "Running evaluation at iteration 356900...\n",
            "Iteration 356900, Train Loss: 3.987757444381714 Eval Loss: 3.7543558502197265\n",
            "Running evaluation at iteration 357000...\n",
            "Iteration 357000, Train Loss: 4.170685768127441 Eval Loss: 3.934617702960968\n",
            "Running evaluation at iteration 357100...\n",
            "Iteration 357100, Train Loss: 3.9388294219970703 Eval Loss: 3.896265730857849\n",
            "Running evaluation at iteration 357200...\n",
            "Iteration 357200, Train Loss: 4.209774017333984 Eval Loss: 3.77545756816864\n",
            "Running evaluation at iteration 357300...\n",
            "Iteration 357300, Train Loss: 3.3168411254882812 Eval Loss: 3.8054735445976258\n",
            "Running evaluation at iteration 357400...\n",
            "Iteration 357400, Train Loss: 3.6647591590881348 Eval Loss: 3.8301607584953308\n",
            "Running evaluation at iteration 357500...\n",
            "Iteration 357500, Train Loss: 3.844714403152466 Eval Loss: 3.82848916053772\n",
            "Running evaluation at iteration 357600...\n",
            "Iteration 357600, Train Loss: 3.5933406352996826 Eval Loss: 3.840140187740326\n",
            "Running evaluation at iteration 357700...\n",
            "Iteration 357700, Train Loss: 4.513533115386963 Eval Loss: 3.9303247356414794\n",
            "Running evaluation at iteration 357800...\n",
            "Iteration 357800, Train Loss: 3.803274154663086 Eval Loss: 3.8081438541412354\n",
            "Running evaluation at iteration 357900...\n",
            "Iteration 357900, Train Loss: 3.821718454360962 Eval Loss: 3.8202362751960752\n",
            "Running evaluation at iteration 358000...\n",
            "Iteration 358000, Train Loss: 4.134345531463623 Eval Loss: 3.721251344680786\n",
            "Running evaluation at iteration 358100...\n",
            "Iteration 358100, Train Loss: 3.913020133972168 Eval Loss: 3.8630133771896364\n",
            "Running evaluation at iteration 358200...\n",
            "Iteration 358200, Train Loss: 4.619206428527832 Eval Loss: 3.768019142150879\n",
            "Running evaluation at iteration 358300...\n",
            "Iteration 358300, Train Loss: 4.142547130584717 Eval Loss: 3.8390577626228333\n",
            "Running evaluation at iteration 358400...\n",
            "Iteration 358400, Train Loss: 3.995727062225342 Eval Loss: 3.8938236331939695\n",
            "Running evaluation at iteration 358500...\n",
            "Iteration 358500, Train Loss: 4.351215839385986 Eval Loss: 3.74508740901947\n",
            "Running evaluation at iteration 358600...\n",
            "Iteration 358600, Train Loss: 3.1102991104125977 Eval Loss: 3.8130759620666503\n",
            "Running evaluation at iteration 358700...\n",
            "Iteration 358700, Train Loss: 4.25944185256958 Eval Loss: 3.8793840432167053\n",
            "Running evaluation at iteration 358800...\n",
            "Iteration 358800, Train Loss: 3.6813790798187256 Eval Loss: 3.9469537115097046\n",
            "Running evaluation at iteration 358900...\n",
            "Iteration 358900, Train Loss: 2.974696159362793 Eval Loss: 3.798117663860321\n",
            "Running evaluation at iteration 359000...\n",
            "Iteration 359000, Train Loss: 3.9381957054138184 Eval Loss: 3.7876407408714297\n",
            "Running evaluation at iteration 359100...\n",
            "Iteration 359100, Train Loss: 3.1984965801239014 Eval Loss: 3.753359067440033\n",
            "Running evaluation at iteration 359200...\n",
            "Iteration 359200, Train Loss: 3.525141477584839 Eval Loss: 3.8373716497421264\n",
            "Running evaluation at iteration 359300...\n",
            "Iteration 359300, Train Loss: 4.7028069496154785 Eval Loss: 3.822249298095703\n",
            "Running evaluation at iteration 359400...\n",
            "Iteration 359400, Train Loss: 3.6978659629821777 Eval Loss: 3.839266951084137\n",
            "Running evaluation at iteration 359500...\n",
            "Iteration 359500, Train Loss: 3.4769082069396973 Eval Loss: 3.83314435005188\n",
            "Running evaluation at iteration 359600...\n",
            "Iteration 359600, Train Loss: 4.118281364440918 Eval Loss: 3.763165383338928\n",
            "Running evaluation at iteration 359700...\n",
            "Iteration 359700, Train Loss: 3.646280527114868 Eval Loss: 3.858704061508179\n",
            "Running evaluation at iteration 359800...\n",
            "Iteration 359800, Train Loss: 3.308291435241699 Eval Loss: 3.8193705701828002\n",
            "Running evaluation at iteration 359900...\n",
            "Iteration 359900, Train Loss: 3.9206137657165527 Eval Loss: 3.7998220038414003\n",
            "Running evaluation at iteration 360000...\n",
            "Iteration 360000, Train Loss: 3.8679628372192383 Eval Loss: 3.839396617412567\n",
            "Running evaluation at iteration 360100...\n",
            "Iteration 360100, Train Loss: 4.161355495452881 Eval Loss: 3.742891564369202\n",
            "Running evaluation at iteration 360200...\n",
            "Iteration 360200, Train Loss: 4.274443626403809 Eval Loss: 3.8564578318595886\n",
            "Running evaluation at iteration 360300...\n",
            "Iteration 360300, Train Loss: 3.9604504108428955 Eval Loss: 3.777991952896118\n",
            "Running evaluation at iteration 360400...\n",
            "Iteration 360400, Train Loss: 4.089498043060303 Eval Loss: 3.842803318500519\n",
            "Running evaluation at iteration 360500...\n",
            "Iteration 360500, Train Loss: 4.454481601715088 Eval Loss: 3.826740357875824\n",
            "Running evaluation at iteration 360600...\n",
            "Iteration 360600, Train Loss: 4.0766401290893555 Eval Loss: 3.8383449649810792\n",
            "Running evaluation at iteration 360700...\n",
            "Iteration 360700, Train Loss: 3.594224452972412 Eval Loss: 3.82381049156189\n",
            "Running evaluation at iteration 360800...\n",
            "Iteration 360800, Train Loss: 4.627099990844727 Eval Loss: 3.8048579573631285\n",
            "Running evaluation at iteration 360900...\n",
            "Iteration 360900, Train Loss: 3.724863052368164 Eval Loss: 3.6875469946861266\n",
            "Running evaluation at iteration 361000...\n",
            "Iteration 361000, Train Loss: 3.7786004543304443 Eval Loss: 3.8072487616539004\n",
            "Running evaluation at iteration 361100...\n",
            "Iteration 361100, Train Loss: 3.5219812393188477 Eval Loss: 3.741399736404419\n",
            "Running evaluation at iteration 361200...\n",
            "Iteration 361200, Train Loss: 4.481555461883545 Eval Loss: 3.7526098799705507\n",
            "Running evaluation at iteration 361300...\n",
            "Iteration 361300, Train Loss: 4.139856338500977 Eval Loss: 3.859938476085663\n",
            "Running evaluation at iteration 361400...\n",
            "Iteration 361400, Train Loss: 3.7326998710632324 Eval Loss: 3.815279619693756\n",
            "Running evaluation at iteration 361500...\n",
            "Iteration 361500, Train Loss: 3.9141387939453125 Eval Loss: 3.844488711357117\n",
            "Running evaluation at iteration 361600...\n",
            "Iteration 361600, Train Loss: 3.486386775970459 Eval Loss: 3.8665164637565614\n",
            "Running evaluation at iteration 361700...\n",
            "Iteration 361700, Train Loss: 3.5298140048980713 Eval Loss: 3.724142553806305\n",
            "Running evaluation at iteration 361800...\n",
            "Iteration 361800, Train Loss: 3.683156967163086 Eval Loss: 3.8670402503013612\n",
            "Running evaluation at iteration 361900...\n",
            "Iteration 361900, Train Loss: 4.6895575523376465 Eval Loss: 3.885282700061798\n",
            "Running evaluation at iteration 362000...\n",
            "Iteration 362000, Train Loss: 4.240801811218262 Eval Loss: 3.817682285308838\n",
            "Running evaluation at iteration 362100...\n",
            "Iteration 362100, Train Loss: 3.3695015907287598 Eval Loss: 3.827908847332001\n",
            "Running evaluation at iteration 362200...\n",
            "Iteration 362200, Train Loss: 4.02012300491333 Eval Loss: 3.771102638244629\n",
            "Running evaluation at iteration 362300...\n",
            "Iteration 362300, Train Loss: 3.9552009105682373 Eval Loss: 3.759031643867493\n",
            "Running evaluation at iteration 362400...\n",
            "Iteration 362400, Train Loss: 3.9789392948150635 Eval Loss: 3.7797444033622742\n",
            "Running evaluation at iteration 362500...\n",
            "Iteration 362500, Train Loss: 3.559497356414795 Eval Loss: 3.86065144777298\n",
            "Running evaluation at iteration 362600...\n",
            "Iteration 362600, Train Loss: 3.747278928756714 Eval Loss: 3.8682490038871764\n",
            "Running evaluation at iteration 362700...\n",
            "Iteration 362700, Train Loss: 3.7028942108154297 Eval Loss: 3.8216059494018553\n",
            "Running evaluation at iteration 362800...\n",
            "Iteration 362800, Train Loss: 3.8873281478881836 Eval Loss: 3.8314162611961367\n",
            "Running evaluation at iteration 362900...\n",
            "Iteration 362900, Train Loss: 3.3886160850524902 Eval Loss: 3.7317326927185057\n",
            "Running evaluation at iteration 363000...\n",
            "Iteration 363000, Train Loss: 3.865926504135132 Eval Loss: 3.80868376493454\n",
            "Running evaluation at iteration 363100...\n",
            "Iteration 363100, Train Loss: 3.664442300796509 Eval Loss: 3.721082503795624\n",
            "Running evaluation at iteration 363200...\n",
            "Iteration 363200, Train Loss: 3.1517586708068848 Eval Loss: 3.7179937624931334\n",
            "Running evaluation at iteration 363300...\n",
            "Iteration 363300, Train Loss: 4.258741855621338 Eval Loss: 3.9084808111190794\n",
            "Running evaluation at iteration 363400...\n",
            "Iteration 363400, Train Loss: 3.0016510486602783 Eval Loss: 3.8873590278625487\n",
            "Running evaluation at iteration 363500...\n",
            "Iteration 363500, Train Loss: 4.228294372558594 Eval Loss: 3.861690490245819\n",
            "Running evaluation at iteration 363600...\n",
            "Iteration 363600, Train Loss: 3.5861194133758545 Eval Loss: 3.8979087972640993\n",
            "Running evaluation at iteration 363700...\n",
            "Iteration 363700, Train Loss: 4.0483832359313965 Eval Loss: 3.807875938415527\n",
            "Running evaluation at iteration 363800...\n",
            "Iteration 363800, Train Loss: 4.123329162597656 Eval Loss: 3.8350460147857666\n",
            "Running evaluation at iteration 363900...\n",
            "Iteration 363900, Train Loss: 3.740480899810791 Eval Loss: 3.908570103645325\n",
            "Running evaluation at iteration 364000...\n",
            "Iteration 364000, Train Loss: 4.764997959136963 Eval Loss: 3.8452453899383543\n",
            "Running evaluation at iteration 364100...\n",
            "Iteration 364100, Train Loss: 4.212584495544434 Eval Loss: 3.719497699737549\n",
            "Running evaluation at iteration 364200...\n",
            "Iteration 364200, Train Loss: 4.83075475692749 Eval Loss: 3.820701720714569\n",
            "Running evaluation at iteration 364300...\n",
            "Iteration 364300, Train Loss: 4.28974723815918 Eval Loss: 3.792380177974701\n",
            "Running evaluation at iteration 364400...\n",
            "Iteration 364400, Train Loss: 3.625544548034668 Eval Loss: 3.822835030555725\n",
            "Running evaluation at iteration 364500...\n",
            "Iteration 364500, Train Loss: 3.753852128982544 Eval Loss: 3.6308148550987243\n",
            "Running evaluation at iteration 364600...\n",
            "Iteration 364600, Train Loss: 3.7441511154174805 Eval Loss: 3.924711890220642\n",
            "Running evaluation at iteration 364700...\n",
            "Iteration 364700, Train Loss: 3.8414528369903564 Eval Loss: 3.912761318683624\n",
            "Running evaluation at iteration 364800...\n",
            "Iteration 364800, Train Loss: 3.3343162536621094 Eval Loss: 3.859255430698395\n",
            "Running evaluation at iteration 364900...\n",
            "Iteration 364900, Train Loss: 2.8557472229003906 Eval Loss: 3.819164125919342\n",
            "Running evaluation at iteration 365000...\n",
            "Iteration 365000, Train Loss: 4.382450580596924 Eval Loss: 3.8550460815429686\n",
            "Running evaluation at iteration 365100...\n",
            "Iteration 365100, Train Loss: 3.5559885501861572 Eval Loss: 3.8127241587638854\n",
            "Running evaluation at iteration 365200...\n",
            "Iteration 365200, Train Loss: 3.110736608505249 Eval Loss: 3.752694079875946\n",
            "Running evaluation at iteration 365300...\n",
            "Iteration 365300, Train Loss: 3.3768351078033447 Eval Loss: 3.824980573654175\n",
            "Running evaluation at iteration 365400...\n",
            "Iteration 365400, Train Loss: 4.336681842803955 Eval Loss: 3.7421413373947146\n",
            "Running evaluation at iteration 365500...\n",
            "Iteration 365500, Train Loss: 4.079319477081299 Eval Loss: 3.8781331515312196\n",
            "Running evaluation at iteration 365600...\n",
            "Iteration 365600, Train Loss: 5.054018974304199 Eval Loss: 3.8281410193443297\n",
            "Running evaluation at iteration 365700...\n",
            "Iteration 365700, Train Loss: 3.5262701511383057 Eval Loss: 3.865710277557373\n",
            "Running evaluation at iteration 365800...\n",
            "Iteration 365800, Train Loss: 4.1113104820251465 Eval Loss: 3.79768474817276\n",
            "Running evaluation at iteration 365900...\n",
            "Iteration 365900, Train Loss: 4.298410892486572 Eval Loss: 3.79628931760788\n",
            "Running evaluation at iteration 366000...\n",
            "Iteration 366000, Train Loss: 3.0846996307373047 Eval Loss: 3.845587043762207\n",
            "Running evaluation at iteration 366100...\n",
            "Iteration 366100, Train Loss: 3.65812087059021 Eval Loss: 3.9085034847259523\n",
            "Running evaluation at iteration 366200...\n",
            "Iteration 366200, Train Loss: 2.957766056060791 Eval Loss: 3.937950601577759\n",
            "Running evaluation at iteration 366300...\n",
            "Iteration 366300, Train Loss: 3.988436460494995 Eval Loss: 3.7970514488220215\n",
            "Running evaluation at iteration 366400...\n",
            "Iteration 366400, Train Loss: 4.163521766662598 Eval Loss: 3.720979652404785\n",
            "Running evaluation at iteration 366500...\n",
            "Iteration 366500, Train Loss: 3.7243268489837646 Eval Loss: 3.898929376602173\n",
            "Running evaluation at iteration 366600...\n",
            "Iteration 366600, Train Loss: 4.551746845245361 Eval Loss: 3.8553299570083617\n",
            "Running evaluation at iteration 366700...\n",
            "Iteration 366700, Train Loss: 3.69808030128479 Eval Loss: 3.8325503873825073\n",
            "Running evaluation at iteration 366800...\n",
            "Iteration 366800, Train Loss: 4.209001064300537 Eval Loss: 3.851570928096771\n",
            "Running evaluation at iteration 366900...\n",
            "Iteration 366900, Train Loss: 4.565006256103516 Eval Loss: 3.769846441745758\n",
            "Running evaluation at iteration 367000...\n",
            "Iteration 367000, Train Loss: 4.60311222076416 Eval Loss: 3.903606035709381\n",
            "Running evaluation at iteration 367100...\n",
            "Iteration 367100, Train Loss: 3.220247507095337 Eval Loss: 3.859999032020569\n",
            "Running evaluation at iteration 367200...\n",
            "Iteration 367200, Train Loss: 3.947265386581421 Eval Loss: 3.809448230266571\n",
            "Running evaluation at iteration 367300...\n",
            "Iteration 367300, Train Loss: 3.6623997688293457 Eval Loss: 3.7291921162605286\n",
            "Running evaluation at iteration 367400...\n",
            "Iteration 367400, Train Loss: 3.163553476333618 Eval Loss: 3.8490191674232483\n",
            "Running evaluation at iteration 367500...\n",
            "Iteration 367500, Train Loss: 3.6168246269226074 Eval Loss: 3.765797688961029\n",
            "Running evaluation at iteration 367600...\n",
            "Iteration 367600, Train Loss: 3.5091750621795654 Eval Loss: 3.850095546245575\n",
            "Running evaluation at iteration 367700...\n",
            "Iteration 367700, Train Loss: 3.874581813812256 Eval Loss: 3.7966135144233704\n",
            "Running evaluation at iteration 367800...\n",
            "Iteration 367800, Train Loss: 2.7045884132385254 Eval Loss: 3.8265596508979796\n",
            "Running evaluation at iteration 367900...\n",
            "Iteration 367900, Train Loss: 4.947113513946533 Eval Loss: 3.8087230706214905\n",
            "Running evaluation at iteration 368000...\n",
            "Iteration 368000, Train Loss: 3.8282413482666016 Eval Loss: 3.827000741958618\n",
            "Running evaluation at iteration 368100...\n",
            "Iteration 368100, Train Loss: 3.6305348873138428 Eval Loss: 3.8498594427108763\n",
            "Running evaluation at iteration 368200...\n",
            "Iteration 368200, Train Loss: 4.608152866363525 Eval Loss: 3.8881745290756227\n",
            "Running evaluation at iteration 368300...\n",
            "Iteration 368300, Train Loss: 3.7377731800079346 Eval Loss: 3.924393653869629\n",
            "Running evaluation at iteration 368400...\n",
            "Iteration 368400, Train Loss: 2.762693405151367 Eval Loss: 3.8058211755752565\n",
            "Running evaluation at iteration 368500...\n",
            "Iteration 368500, Train Loss: 4.999850749969482 Eval Loss: 3.7904481530189513\n",
            "Running evaluation at iteration 368600...\n",
            "Iteration 368600, Train Loss: 3.958515167236328 Eval Loss: 3.8681737279891966\n",
            "Running evaluation at iteration 368700...\n",
            "Iteration 368700, Train Loss: 4.170029163360596 Eval Loss: 3.8062691831588746\n",
            "Running evaluation at iteration 368800...\n",
            "Iteration 368800, Train Loss: 3.8607590198516846 Eval Loss: 3.810484538078308\n",
            "Running evaluation at iteration 368900...\n",
            "Iteration 368900, Train Loss: 3.839632511138916 Eval Loss: 3.8099566078186036\n",
            "Running evaluation at iteration 369000...\n",
            "Iteration 369000, Train Loss: 3.4786605834960938 Eval Loss: 3.800758709907532\n",
            "Running evaluation at iteration 369100...\n",
            "Iteration 369100, Train Loss: 3.4981415271759033 Eval Loss: 3.7256489109992983\n",
            "Running evaluation at iteration 369200...\n",
            "Iteration 369200, Train Loss: 3.3760461807250977 Eval Loss: 3.8079006600379945\n",
            "Running evaluation at iteration 369300...\n",
            "Iteration 369300, Train Loss: 3.97458815574646 Eval Loss: 3.79589382648468\n",
            "Running evaluation at iteration 369400...\n",
            "Iteration 369400, Train Loss: 3.9194302558898926 Eval Loss: 3.786459925174713\n",
            "Running evaluation at iteration 369500...\n",
            "Iteration 369500, Train Loss: 4.183889865875244 Eval Loss: 3.681906898021698\n",
            "Running evaluation at iteration 369600...\n",
            "Iteration 369600, Train Loss: 4.24904727935791 Eval Loss: 3.8311910438537597\n",
            "Running evaluation at iteration 369700...\n",
            "Iteration 369700, Train Loss: 3.1741271018981934 Eval Loss: 3.882091007232666\n",
            "Running evaluation at iteration 369800...\n",
            "Iteration 369800, Train Loss: 3.027244806289673 Eval Loss: 3.765366575717926\n",
            "Running evaluation at iteration 369900...\n",
            "Iteration 369900, Train Loss: 3.610384464263916 Eval Loss: 3.746340618133545\n",
            "Running evaluation at iteration 370000...\n",
            "Iteration 370000, Train Loss: 3.5167901515960693 Eval Loss: 3.8306415939331053\n",
            "Running evaluation at iteration 370100...\n",
            "Iteration 370100, Train Loss: 4.471695899963379 Eval Loss: 3.8264345049858095\n",
            "Running evaluation at iteration 370200...\n",
            "Iteration 370200, Train Loss: 3.3017449378967285 Eval Loss: 3.8014916658401487\n",
            "Running evaluation at iteration 370300...\n",
            "Iteration 370300, Train Loss: 4.263462066650391 Eval Loss: 3.8425462317466734\n",
            "Running evaluation at iteration 370400...\n",
            "Iteration 370400, Train Loss: 3.730801582336426 Eval Loss: 3.818675136566162\n",
            "Running evaluation at iteration 370500...\n",
            "Iteration 370500, Train Loss: 4.138307094573975 Eval Loss: 3.8210678482055664\n",
            "Running evaluation at iteration 370600...\n",
            "Iteration 370600, Train Loss: 2.917541980743408 Eval Loss: 3.8891843008995055\n",
            "Running evaluation at iteration 370700...\n",
            "Iteration 370700, Train Loss: 3.8872885704040527 Eval Loss: 3.852622799873352\n",
            "Running evaluation at iteration 370800...\n",
            "Iteration 370800, Train Loss: 3.6925101280212402 Eval Loss: 3.8408041620254516\n",
            "Running evaluation at iteration 370900...\n",
            "Iteration 370900, Train Loss: 4.559697151184082 Eval Loss: 3.888563416004181\n",
            "Running evaluation at iteration 371000...\n",
            "Iteration 371000, Train Loss: 4.643396377563477 Eval Loss: 3.896172000169754\n",
            "Running evaluation at iteration 371100...\n",
            "Iteration 371100, Train Loss: 4.151939392089844 Eval Loss: 3.764961974620819\n",
            "Running evaluation at iteration 371200...\n",
            "Iteration 371200, Train Loss: 4.493783950805664 Eval Loss: 3.7423260045051574\n",
            "Running evaluation at iteration 371300...\n",
            "Iteration 371300, Train Loss: 3.269744873046875 Eval Loss: 3.9022636938095094\n",
            "Running evaluation at iteration 371400...\n",
            "Iteration 371400, Train Loss: 3.6084299087524414 Eval Loss: 3.802981240749359\n",
            "Running evaluation at iteration 371500...\n",
            "Iteration 371500, Train Loss: 5.3012213706970215 Eval Loss: 3.826196839809418\n",
            "Running evaluation at iteration 371600...\n",
            "Iteration 371600, Train Loss: 3.554330587387085 Eval Loss: 3.764058609008789\n",
            "Running evaluation at iteration 371700...\n",
            "Iteration 371700, Train Loss: 3.956857442855835 Eval Loss: 3.8175719594955444\n",
            "Running evaluation at iteration 371800...\n",
            "Iteration 371800, Train Loss: 3.9779632091522217 Eval Loss: 3.836585268974304\n",
            "Running evaluation at iteration 371900...\n",
            "Iteration 371900, Train Loss: 3.611245632171631 Eval Loss: 3.7604232668876647\n",
            "Running evaluation at iteration 372000...\n",
            "Iteration 372000, Train Loss: 3.9059362411499023 Eval Loss: 3.811296560764313\n",
            "Running evaluation at iteration 372100...\n",
            "Iteration 372100, Train Loss: 3.4961957931518555 Eval Loss: 3.8140725970268248\n",
            "Running evaluation at iteration 372200...\n",
            "Iteration 372200, Train Loss: 2.8339428901672363 Eval Loss: 3.8044869780540465\n",
            "Running evaluation at iteration 372300...\n",
            "Iteration 372300, Train Loss: 3.675621509552002 Eval Loss: 3.7850863242149355\n",
            "Running evaluation at iteration 372400...\n",
            "Iteration 372400, Train Loss: 3.5085458755493164 Eval Loss: 3.8585009217262267\n",
            "Running evaluation at iteration 372500...\n",
            "Iteration 372500, Train Loss: 3.6445493698120117 Eval Loss: 3.837015404701233\n",
            "Running evaluation at iteration 372600...\n",
            "Iteration 372600, Train Loss: 3.7605535984039307 Eval Loss: 3.7865616130828856\n",
            "Running evaluation at iteration 372700...\n",
            "Iteration 372700, Train Loss: 3.2216861248016357 Eval Loss: 3.808859655857086\n",
            "Running evaluation at iteration 372800...\n",
            "Iteration 372800, Train Loss: 4.03065299987793 Eval Loss: 3.8403790736198427\n",
            "Running evaluation at iteration 372900...\n",
            "Iteration 372900, Train Loss: 3.478163242340088 Eval Loss: 3.798533036708832\n",
            "Running evaluation at iteration 373000...\n",
            "Iteration 373000, Train Loss: 3.5587332248687744 Eval Loss: 3.8003558921813965\n",
            "Running evaluation at iteration 373100...\n",
            "Iteration 373100, Train Loss: 4.285533905029297 Eval Loss: 3.8735556173324586\n",
            "Running evaluation at iteration 373200...\n",
            "Iteration 373200, Train Loss: 2.8969852924346924 Eval Loss: 3.8943978548049927\n",
            "Running evaluation at iteration 373300...\n",
            "Iteration 373300, Train Loss: 3.715813636779785 Eval Loss: 3.756282186508179\n",
            "Running evaluation at iteration 373400...\n",
            "Iteration 373400, Train Loss: 5.09005880355835 Eval Loss: 3.7030396604537965\n",
            "Running evaluation at iteration 373500...\n",
            "Iteration 373500, Train Loss: 4.297720909118652 Eval Loss: 3.8723803329467774\n",
            "Running evaluation at iteration 373600...\n",
            "Iteration 373600, Train Loss: 3.9452004432678223 Eval Loss: 3.830685420036316\n",
            "Running evaluation at iteration 373700...\n",
            "Iteration 373700, Train Loss: 3.982853889465332 Eval Loss: 3.8246171474456787\n",
            "Running evaluation at iteration 373800...\n",
            "Iteration 373800, Train Loss: 4.335219383239746 Eval Loss: 3.702064652442932\n",
            "Running evaluation at iteration 373900...\n",
            "Iteration 373900, Train Loss: 3.7250804901123047 Eval Loss: 3.7973132848739626\n",
            "Running evaluation at iteration 374000...\n",
            "Iteration 374000, Train Loss: 3.415853500366211 Eval Loss: 3.7976343417167664\n",
            "Running evaluation at iteration 374100...\n",
            "Iteration 374100, Train Loss: 5.093603610992432 Eval Loss: 3.865061807632446\n",
            "Running evaluation at iteration 374200...\n",
            "Iteration 374200, Train Loss: 4.222838878631592 Eval Loss: 3.8321145510673524\n",
            "Running evaluation at iteration 374300...\n",
            "Iteration 374300, Train Loss: 3.451744556427002 Eval Loss: 3.9440693187713625\n",
            "Running evaluation at iteration 374400...\n",
            "Iteration 374400, Train Loss: 3.947584867477417 Eval Loss: 3.8403434228897093\n",
            "Running evaluation at iteration 374500...\n",
            "Iteration 374500, Train Loss: 3.5526742935180664 Eval Loss: 3.862430822849274\n",
            "Running evaluation at iteration 374600...\n",
            "Iteration 374600, Train Loss: 3.4718375205993652 Eval Loss: 3.8606357860565184\n",
            "Running evaluation at iteration 374700...\n",
            "Iteration 374700, Train Loss: 4.131500244140625 Eval Loss: 3.802154092788696\n",
            "Running evaluation at iteration 374800...\n",
            "Iteration 374800, Train Loss: 3.352437973022461 Eval Loss: 3.840983865261078\n",
            "Running evaluation at iteration 374900...\n",
            "Iteration 374900, Train Loss: 4.1681599617004395 Eval Loss: 3.911598036289215\n",
            "Running evaluation at iteration 375000...\n",
            "Iteration 375000, Train Loss: 4.33513069152832 Eval Loss: 3.8890384817123413\n",
            "Running evaluation at iteration 375100...\n",
            "Iteration 375100, Train Loss: 3.228850841522217 Eval Loss: 3.8303358125686646\n",
            "Running evaluation at iteration 375200...\n",
            "Iteration 375200, Train Loss: 3.568079948425293 Eval Loss: 3.770134584903717\n",
            "Running evaluation at iteration 375300...\n",
            "Iteration 375300, Train Loss: 3.6248977184295654 Eval Loss: 3.8077273297309877\n",
            "Running evaluation at iteration 375400...\n",
            "Iteration 375400, Train Loss: 4.290402889251709 Eval Loss: 3.6927057099342346\n",
            "Running evaluation at iteration 375500...\n",
            "Iteration 375500, Train Loss: 3.9413115978240967 Eval Loss: 3.7164044976234436\n",
            "Running evaluation at iteration 375600...\n",
            "Iteration 375600, Train Loss: 4.692209720611572 Eval Loss: 3.8611400365829467\n",
            "Running evaluation at iteration 375700...\n",
            "Iteration 375700, Train Loss: 4.625736236572266 Eval Loss: 3.771811001300812\n",
            "Running evaluation at iteration 375800...\n",
            "Iteration 375800, Train Loss: 3.537416934967041 Eval Loss: 3.854011836051941\n",
            "Running evaluation at iteration 375900...\n",
            "Iteration 375900, Train Loss: 3.1280086040496826 Eval Loss: 3.7975468158721926\n",
            "Running evaluation at iteration 376000...\n",
            "Iteration 376000, Train Loss: 4.174130439758301 Eval Loss: 3.83184225320816\n",
            "Running evaluation at iteration 376100...\n",
            "Iteration 376100, Train Loss: 4.349648475646973 Eval Loss: 3.8553997111320495\n",
            "Running evaluation at iteration 376200...\n",
            "Iteration 376200, Train Loss: 3.3220112323760986 Eval Loss: 3.841668050289154\n",
            "Running evaluation at iteration 376300...\n",
            "Iteration 376300, Train Loss: 3.9224276542663574 Eval Loss: 3.8232986617088316\n",
            "Running evaluation at iteration 376400...\n",
            "Iteration 376400, Train Loss: 3.5760300159454346 Eval Loss: 3.797237160205841\n",
            "Running evaluation at iteration 376500...\n",
            "Iteration 376500, Train Loss: 5.412684440612793 Eval Loss: 3.8621052169799803\n",
            "Running evaluation at iteration 376600...\n",
            "Iteration 376600, Train Loss: 3.6947786808013916 Eval Loss: 3.8829523801803587\n",
            "Running evaluation at iteration 376700...\n",
            "Iteration 376700, Train Loss: 4.096382141113281 Eval Loss: 3.785808475017548\n",
            "Running evaluation at iteration 376800...\n",
            "Iteration 376800, Train Loss: 4.219296932220459 Eval Loss: 3.787027313709259\n",
            "Running evaluation at iteration 376900...\n",
            "Iteration 376900, Train Loss: 3.06084942817688 Eval Loss: 3.77955007314682\n",
            "Running evaluation at iteration 377000...\n",
            "Iteration 377000, Train Loss: 3.771209955215454 Eval Loss: 3.7981264281272886\n",
            "Running evaluation at iteration 377100...\n",
            "Iteration 377100, Train Loss: 3.7164947986602783 Eval Loss: 3.762443380355835\n",
            "Running evaluation at iteration 377200...\n",
            "Iteration 377200, Train Loss: 4.293725967407227 Eval Loss: 3.794592649936676\n",
            "Running evaluation at iteration 377300...\n",
            "Iteration 377300, Train Loss: 3.1943485736846924 Eval Loss: 3.8121451044082644\n",
            "Running evaluation at iteration 377400...\n",
            "Iteration 377400, Train Loss: 3.44661808013916 Eval Loss: 3.9202964305877686\n",
            "Running evaluation at iteration 377500...\n",
            "Iteration 377500, Train Loss: 3.329359531402588 Eval Loss: 3.7971590185165405\n",
            "Running evaluation at iteration 377600...\n",
            "Iteration 377600, Train Loss: 3.8069751262664795 Eval Loss: 3.851687822341919\n",
            "Running evaluation at iteration 377700...\n",
            "Iteration 377700, Train Loss: 3.8918049335479736 Eval Loss: 3.771961488723755\n",
            "Running evaluation at iteration 377800...\n",
            "Iteration 377800, Train Loss: 3.5819637775421143 Eval Loss: 3.706191132068634\n",
            "Running evaluation at iteration 377900...\n",
            "Iteration 377900, Train Loss: 5.526927471160889 Eval Loss: 3.8546066880226135\n",
            "Running evaluation at iteration 378000...\n",
            "Iteration 378000, Train Loss: 3.667334794998169 Eval Loss: 3.90926078081131\n",
            "Running evaluation at iteration 378100...\n",
            "Iteration 378100, Train Loss: 3.8648502826690674 Eval Loss: 3.882601761817932\n",
            "Running evaluation at iteration 378200...\n",
            "Iteration 378200, Train Loss: 4.98561429977417 Eval Loss: 3.844984073638916\n",
            "Running evaluation at iteration 378300...\n",
            "Iteration 378300, Train Loss: 3.8024985790252686 Eval Loss: 3.85607857465744\n",
            "Running evaluation at iteration 378400...\n",
            "Iteration 378400, Train Loss: 3.3153254985809326 Eval Loss: 3.7963838171958924\n",
            "Running evaluation at iteration 378500...\n",
            "Iteration 378500, Train Loss: 4.515340328216553 Eval Loss: 3.764057891368866\n",
            "Running evaluation at iteration 378600...\n",
            "Iteration 378600, Train Loss: 4.504929065704346 Eval Loss: 3.8293686270713807\n",
            "Running evaluation at iteration 378700...\n",
            "Iteration 378700, Train Loss: 3.5216286182403564 Eval Loss: 3.8120076203346254\n",
            "Running evaluation at iteration 378800...\n",
            "Iteration 378800, Train Loss: 3.860422134399414 Eval Loss: 3.906108407974243\n",
            "Running evaluation at iteration 378900...\n",
            "Iteration 378900, Train Loss: 4.1242241859436035 Eval Loss: 3.8860269832611083\n",
            "Running evaluation at iteration 379000...\n",
            "Iteration 379000, Train Loss: 3.392289400100708 Eval Loss: 3.892616512775421\n",
            "Running evaluation at iteration 379100...\n",
            "Iteration 379100, Train Loss: 4.1031599044799805 Eval Loss: 3.826276111602783\n",
            "Running evaluation at iteration 379200...\n",
            "Iteration 379200, Train Loss: 3.3365731239318848 Eval Loss: 3.7427750039100647\n",
            "Running evaluation at iteration 379300...\n",
            "Iteration 379300, Train Loss: 4.097874641418457 Eval Loss: 3.857539038658142\n",
            "Running evaluation at iteration 379400...\n",
            "Iteration 379400, Train Loss: 3.462001085281372 Eval Loss: 3.844783117771149\n",
            "Running evaluation at iteration 379500...\n",
            "Iteration 379500, Train Loss: 4.393067359924316 Eval Loss: 3.85309353351593\n",
            "Running evaluation at iteration 379600...\n",
            "Iteration 379600, Train Loss: 3.2026262283325195 Eval Loss: 3.7520397114753723\n",
            "Running evaluation at iteration 379700...\n",
            "Iteration 379700, Train Loss: 3.6440746784210205 Eval Loss: 3.869472506046295\n",
            "Running evaluation at iteration 379800...\n",
            "Iteration 379800, Train Loss: 4.746629238128662 Eval Loss: 3.8756399512290955\n",
            "Running evaluation at iteration 379900...\n",
            "Iteration 379900, Train Loss: 4.208471298217773 Eval Loss: 3.8047398734092712\n",
            "Running evaluation at iteration 380000...\n",
            "Iteration 380000, Train Loss: 3.0243451595306396 Eval Loss: 3.7811267042160033\n",
            "Running evaluation at iteration 380100...\n",
            "Iteration 380100, Train Loss: 3.6033363342285156 Eval Loss: 3.7715428686141967\n",
            "Running evaluation at iteration 380200...\n",
            "Iteration 380200, Train Loss: 3.598113536834717 Eval Loss: 3.8454809379577637\n",
            "Running evaluation at iteration 380300...\n",
            "Iteration 380300, Train Loss: 3.8414158821105957 Eval Loss: 3.918307456970215\n",
            "Running evaluation at iteration 380400...\n",
            "Iteration 380400, Train Loss: 3.3539528846740723 Eval Loss: 3.815563490390778\n",
            "Running evaluation at iteration 380500...\n",
            "Iteration 380500, Train Loss: 3.416186809539795 Eval Loss: 3.7597801804542543\n",
            "Running evaluation at iteration 380600...\n",
            "Iteration 380600, Train Loss: 3.39953875541687 Eval Loss: 3.784816598892212\n",
            "Running evaluation at iteration 380700...\n",
            "Iteration 380700, Train Loss: 4.166050910949707 Eval Loss: 3.8744904828071594\n",
            "Running evaluation at iteration 380800...\n",
            "Iteration 380800, Train Loss: 3.467665910720825 Eval Loss: 3.776239318847656\n",
            "Running evaluation at iteration 380900...\n",
            "Iteration 380900, Train Loss: 3.528721809387207 Eval Loss: 3.7871512627601622\n",
            "Running evaluation at iteration 381000...\n",
            "Iteration 381000, Train Loss: 4.274084091186523 Eval Loss: 3.822825536727905\n",
            "Running evaluation at iteration 381100...\n",
            "Iteration 381100, Train Loss: 3.978057861328125 Eval Loss: 3.853662095069885\n",
            "Running evaluation at iteration 381200...\n",
            "Iteration 381200, Train Loss: 3.6220364570617676 Eval Loss: 3.6939242386817934\n",
            "Running evaluation at iteration 381300...\n",
            "Iteration 381300, Train Loss: 4.394725799560547 Eval Loss: 3.8669047379493713\n",
            "Running evaluation at iteration 381400...\n",
            "Iteration 381400, Train Loss: 3.2634170055389404 Eval Loss: 3.7455412673950197\n",
            "Running evaluation at iteration 381500...\n",
            "Iteration 381500, Train Loss: 3.826890707015991 Eval Loss: 3.790443685054779\n",
            "Running evaluation at iteration 381600...\n",
            "Iteration 381600, Train Loss: 3.947359323501587 Eval Loss: 3.826171314716339\n",
            "Running evaluation at iteration 381700...\n",
            "Iteration 381700, Train Loss: 3.6300745010375977 Eval Loss: 3.8280303359031675\n",
            "Running evaluation at iteration 381800...\n",
            "Iteration 381800, Train Loss: 4.127127647399902 Eval Loss: 3.8058515071868895\n",
            "Running evaluation at iteration 381900...\n",
            "Iteration 381900, Train Loss: 4.639325141906738 Eval Loss: 3.8218875885009767\n",
            "Running evaluation at iteration 382000...\n",
            "Iteration 382000, Train Loss: 3.5243303775787354 Eval Loss: 3.814822995662689\n",
            "Running evaluation at iteration 382100...\n",
            "Iteration 382100, Train Loss: 4.251571178436279 Eval Loss: 3.769965124130249\n",
            "Running evaluation at iteration 382200...\n",
            "Iteration 382200, Train Loss: 3.916896343231201 Eval Loss: 3.794571404457092\n",
            "Running evaluation at iteration 382300...\n",
            "Iteration 382300, Train Loss: 2.653806686401367 Eval Loss: 3.7494134163856505\n",
            "Running evaluation at iteration 382400...\n",
            "Iteration 382400, Train Loss: 3.949838161468506 Eval Loss: 3.8487190580368043\n",
            "Running evaluation at iteration 382500...\n",
            "Iteration 382500, Train Loss: 3.621013641357422 Eval Loss: 3.786965525150299\n",
            "Running evaluation at iteration 382600...\n",
            "Iteration 382600, Train Loss: 3.8723931312561035 Eval Loss: 3.7446395897865297\n",
            "Running evaluation at iteration 382700...\n",
            "Iteration 382700, Train Loss: 3.207446575164795 Eval Loss: 3.833865313529968\n",
            "Running evaluation at iteration 382800...\n",
            "Iteration 382800, Train Loss: 2.995025157928467 Eval Loss: 3.8433800959587097\n",
            "Running evaluation at iteration 382900...\n",
            "Iteration 382900, Train Loss: 3.2222399711608887 Eval Loss: 3.852113516330719\n",
            "Running evaluation at iteration 383000...\n",
            "Iteration 383000, Train Loss: 3.5856027603149414 Eval Loss: 3.7588262033462523\n",
            "Running evaluation at iteration 383100...\n",
            "Iteration 383100, Train Loss: 4.079165458679199 Eval Loss: 3.8422901940345766\n",
            "Running evaluation at iteration 383200...\n",
            "Iteration 383200, Train Loss: 3.884145975112915 Eval Loss: 3.863274545669556\n",
            "Running evaluation at iteration 383300...\n",
            "Iteration 383300, Train Loss: 3.313117504119873 Eval Loss: 3.829272780418396\n",
            "Running evaluation at iteration 383400...\n",
            "Iteration 383400, Train Loss: 3.2495315074920654 Eval Loss: 3.823716084957123\n",
            "Running evaluation at iteration 383500...\n",
            "Iteration 383500, Train Loss: 4.027736663818359 Eval Loss: 3.7607086157798766\n",
            "Running evaluation at iteration 383600...\n",
            "Iteration 383600, Train Loss: 3.579427719116211 Eval Loss: 3.917504222393036\n",
            "Running evaluation at iteration 383700...\n",
            "Iteration 383700, Train Loss: 3.4364571571350098 Eval Loss: 3.920891134738922\n",
            "Running evaluation at iteration 383800...\n",
            "Iteration 383800, Train Loss: 4.194208145141602 Eval Loss: 3.8534491944313047\n",
            "Running evaluation at iteration 383900...\n",
            "Iteration 383900, Train Loss: 4.231236934661865 Eval Loss: 3.7819059920310973\n",
            "Running evaluation at iteration 384000...\n",
            "Iteration 384000, Train Loss: 3.2547597885131836 Eval Loss: 3.8173422026634216\n",
            "Running evaluation at iteration 384100...\n",
            "Iteration 384100, Train Loss: 4.045529365539551 Eval Loss: 3.7833075547218322\n",
            "Running evaluation at iteration 384200...\n",
            "Iteration 384200, Train Loss: 3.7225141525268555 Eval Loss: 3.808651850223541\n",
            "Running evaluation at iteration 384300...\n",
            "Iteration 384300, Train Loss: 4.0912322998046875 Eval Loss: 3.8174764776229857\n",
            "Running evaluation at iteration 384400...\n",
            "Iteration 384400, Train Loss: 3.363982915878296 Eval Loss: 3.833480486869812\n",
            "Running evaluation at iteration 384500...\n",
            "Iteration 384500, Train Loss: 4.127287864685059 Eval Loss: 3.813769760131836\n",
            "Running evaluation at iteration 384600...\n",
            "Iteration 384600, Train Loss: 3.277921676635742 Eval Loss: 3.7804967260360716\n",
            "Running evaluation at iteration 384700...\n",
            "Iteration 384700, Train Loss: 3.6903154850006104 Eval Loss: 3.829018657207489\n",
            "Running evaluation at iteration 384800...\n",
            "Iteration 384800, Train Loss: 4.199382305145264 Eval Loss: 3.7970741605758667\n",
            "Running evaluation at iteration 384900...\n",
            "Iteration 384900, Train Loss: 4.154862880706787 Eval Loss: 3.8307119679450987\n",
            "Running evaluation at iteration 385000...\n",
            "Iteration 385000, Train Loss: 3.3319387435913086 Eval Loss: 3.7544976902008056\n",
            "Running evaluation at iteration 385100...\n",
            "Iteration 385100, Train Loss: 4.201822280883789 Eval Loss: 3.83089359998703\n",
            "Running evaluation at iteration 385200...\n",
            "Iteration 385200, Train Loss: 3.986786127090454 Eval Loss: 3.9103709125518797\n",
            "Running evaluation at iteration 385300...\n",
            "Iteration 385300, Train Loss: 2.6830525398254395 Eval Loss: 3.802512238025665\n",
            "Running evaluation at iteration 385400...\n",
            "Iteration 385400, Train Loss: 4.419612407684326 Eval Loss: 3.871768441200256\n",
            "Running evaluation at iteration 385500...\n",
            "Iteration 385500, Train Loss: 3.3366851806640625 Eval Loss: 3.7916142892837525\n",
            "Running evaluation at iteration 385600...\n",
            "Iteration 385600, Train Loss: 3.9145891666412354 Eval Loss: 3.7976851582527162\n",
            "Running evaluation at iteration 385700...\n",
            "Iteration 385700, Train Loss: 3.8712124824523926 Eval Loss: 3.9182705974578855\n",
            "Running evaluation at iteration 385800...\n",
            "Iteration 385800, Train Loss: 3.6840872764587402 Eval Loss: 3.8374363446235655\n",
            "Running evaluation at iteration 385900...\n",
            "Iteration 385900, Train Loss: 3.536604166030884 Eval Loss: 3.993571343421936\n",
            "Running evaluation at iteration 386000...\n",
            "Iteration 386000, Train Loss: 3.8363847732543945 Eval Loss: 3.818029313087463\n",
            "Running evaluation at iteration 386100...\n",
            "Iteration 386100, Train Loss: 3.7887043952941895 Eval Loss: 3.7444726490974425\n",
            "Running evaluation at iteration 386200...\n",
            "Iteration 386200, Train Loss: 3.8603057861328125 Eval Loss: 3.8334174156188965\n",
            "Running evaluation at iteration 386300...\n",
            "Iteration 386300, Train Loss: 4.569178104400635 Eval Loss: 3.8269964170455935\n",
            "Running evaluation at iteration 386400...\n",
            "Iteration 386400, Train Loss: 3.8417181968688965 Eval Loss: 3.77158483505249\n",
            "Running evaluation at iteration 386500...\n",
            "Iteration 386500, Train Loss: 3.3032217025756836 Eval Loss: 3.8786931300163268\n",
            "Running evaluation at iteration 386600...\n",
            "Iteration 386600, Train Loss: 4.111178398132324 Eval Loss: 3.828443379402161\n",
            "Running evaluation at iteration 386700...\n",
            "Iteration 386700, Train Loss: 3.4424655437469482 Eval Loss: 3.776401731967926\n",
            "Running evaluation at iteration 386800...\n",
            "Iteration 386800, Train Loss: 3.881202459335327 Eval Loss: 3.8378548717498777\n",
            "Running evaluation at iteration 386900...\n",
            "Iteration 386900, Train Loss: 3.858726978302002 Eval Loss: 3.855964150428772\n",
            "Running evaluation at iteration 387000...\n",
            "Iteration 387000, Train Loss: 3.7860305309295654 Eval Loss: 3.813881859779358\n",
            "Running evaluation at iteration 387100...\n",
            "Iteration 387100, Train Loss: 3.049468755722046 Eval Loss: 3.8334638023376466\n",
            "Running evaluation at iteration 387200...\n",
            "Iteration 387200, Train Loss: 4.607105731964111 Eval Loss: 3.809598228931427\n",
            "Running evaluation at iteration 387300...\n",
            "Iteration 387300, Train Loss: 4.038436412811279 Eval Loss: 3.79012220621109\n",
            "Running evaluation at iteration 387400...\n",
            "Iteration 387400, Train Loss: 3.854971408843994 Eval Loss: 3.899973974227905\n",
            "Running evaluation at iteration 387500...\n",
            "Iteration 387500, Train Loss: 4.098294734954834 Eval Loss: 3.9051462483406065\n",
            "Running evaluation at iteration 387600...\n",
            "Iteration 387600, Train Loss: 3.883435010910034 Eval Loss: 3.759649648666382\n",
            "Running evaluation at iteration 387700...\n",
            "Iteration 387700, Train Loss: 2.9352974891662598 Eval Loss: 3.7775577235221864\n",
            "Running evaluation at iteration 387800...\n",
            "Iteration 387800, Train Loss: 4.455453872680664 Eval Loss: 3.815750329494476\n",
            "Running evaluation at iteration 387900...\n",
            "Iteration 387900, Train Loss: 4.13525390625 Eval Loss: 3.8273157596588137\n",
            "Running evaluation at iteration 388000...\n",
            "Iteration 388000, Train Loss: 4.2359113693237305 Eval Loss: 3.78296599149704\n",
            "Running evaluation at iteration 388100...\n",
            "Iteration 388100, Train Loss: 3.570580244064331 Eval Loss: 3.7604854464530946\n",
            "Running evaluation at iteration 388200...\n",
            "Iteration 388200, Train Loss: 4.499967098236084 Eval Loss: 3.8501717543601988\n",
            "Running evaluation at iteration 388300...\n",
            "Iteration 388300, Train Loss: 4.0572590827941895 Eval Loss: 3.8591631484031677\n",
            "Running evaluation at iteration 388400...\n",
            "Iteration 388400, Train Loss: 3.61915922164917 Eval Loss: 3.8359283781051636\n",
            "Running evaluation at iteration 388500...\n",
            "Iteration 388500, Train Loss: 4.066318988800049 Eval Loss: 3.829667954444885\n",
            "Running evaluation at iteration 388600...\n",
            "Iteration 388600, Train Loss: 4.971446514129639 Eval Loss: 3.823686008453369\n",
            "Running evaluation at iteration 388700...\n",
            "Iteration 388700, Train Loss: 3.2233030796051025 Eval Loss: 3.812106966972351\n",
            "Running evaluation at iteration 388800...\n",
            "Iteration 388800, Train Loss: 3.922203540802002 Eval Loss: 3.87051429271698\n",
            "Running evaluation at iteration 388900...\n",
            "Iteration 388900, Train Loss: 3.607593536376953 Eval Loss: 3.806893165111542\n",
            "Running evaluation at iteration 389000...\n",
            "Iteration 389000, Train Loss: 3.8640222549438477 Eval Loss: 3.730105276107788\n",
            "Running evaluation at iteration 389100...\n",
            "Iteration 389100, Train Loss: 3.39607834815979 Eval Loss: 3.7896586585044862\n",
            "Running evaluation at iteration 389200...\n",
            "Iteration 389200, Train Loss: 4.047097682952881 Eval Loss: 3.814907352924347\n",
            "Running evaluation at iteration 389300...\n",
            "Iteration 389300, Train Loss: 4.203038215637207 Eval Loss: 3.7960831475257875\n",
            "Running evaluation at iteration 389400...\n",
            "Iteration 389400, Train Loss: 4.114567279815674 Eval Loss: 3.7786698269844057\n",
            "Running evaluation at iteration 389500...\n",
            "Iteration 389500, Train Loss: 4.3717498779296875 Eval Loss: 3.861605610847473\n",
            "Running evaluation at iteration 389600...\n",
            "Iteration 389600, Train Loss: 3.348886489868164 Eval Loss: 3.869046049118042\n",
            "Running evaluation at iteration 389700...\n",
            "Iteration 389700, Train Loss: 4.184969902038574 Eval Loss: 3.84202246427536\n",
            "Running evaluation at iteration 389800...\n",
            "Iteration 389800, Train Loss: 3.7876036167144775 Eval Loss: 3.816965494155884\n",
            "Running evaluation at iteration 389900...\n",
            "Iteration 389900, Train Loss: 3.7725069522857666 Eval Loss: 3.8006472992897034\n",
            "Running evaluation at iteration 390000...\n",
            "Iteration 390000, Train Loss: 4.22481107711792 Eval Loss: 3.817345118522644\n",
            "Running evaluation at iteration 390100...\n",
            "Iteration 390100, Train Loss: 3.872040271759033 Eval Loss: 3.76088698387146\n",
            "Running evaluation at iteration 390200...\n",
            "Iteration 390200, Train Loss: 4.160909175872803 Eval Loss: 3.8226520800590515\n",
            "Running evaluation at iteration 390300...\n",
            "Iteration 390300, Train Loss: 3.473867177963257 Eval Loss: 3.768913426399231\n",
            "Running evaluation at iteration 390400...\n",
            "Iteration 390400, Train Loss: 4.089404106140137 Eval Loss: 3.893900091648102\n",
            "Running evaluation at iteration 390500...\n",
            "Iteration 390500, Train Loss: 3.0630860328674316 Eval Loss: 3.850771942138672\n",
            "Running evaluation at iteration 390600...\n",
            "Iteration 390600, Train Loss: 3.75346302986145 Eval Loss: 3.8204737067222596\n",
            "Running evaluation at iteration 390700...\n",
            "Iteration 390700, Train Loss: 4.224462985992432 Eval Loss: 3.8136143255233765\n",
            "Running evaluation at iteration 390800...\n",
            "Iteration 390800, Train Loss: 3.9763362407684326 Eval Loss: 3.7566157531738282\n",
            "Running evaluation at iteration 390900...\n",
            "Iteration 390900, Train Loss: 3.95741605758667 Eval Loss: 3.8239633202552796\n",
            "Running evaluation at iteration 391000...\n",
            "Iteration 391000, Train Loss: 4.001374244689941 Eval Loss: 3.8177330327033996\n",
            "Running evaluation at iteration 391100...\n",
            "Iteration 391100, Train Loss: 3.8112049102783203 Eval Loss: 3.857529392242432\n",
            "Running evaluation at iteration 391200...\n",
            "Iteration 391200, Train Loss: 4.273652076721191 Eval Loss: 3.715581703186035\n",
            "Running evaluation at iteration 391300...\n",
            "Iteration 391300, Train Loss: 3.6323132514953613 Eval Loss: 3.8557246351242065\n",
            "Running evaluation at iteration 391400...\n",
            "Iteration 391400, Train Loss: 3.8457751274108887 Eval Loss: 3.8750799679756165\n",
            "Running evaluation at iteration 391500...\n",
            "Iteration 391500, Train Loss: 3.9749398231506348 Eval Loss: 3.807157850265503\n",
            "Running evaluation at iteration 391600...\n",
            "Iteration 391600, Train Loss: 3.2546844482421875 Eval Loss: 3.8869805455207826\n",
            "Running evaluation at iteration 391700...\n",
            "Iteration 391700, Train Loss: 3.932582378387451 Eval Loss: 3.7574387407302856\n",
            "Running evaluation at iteration 391800...\n",
            "Iteration 391800, Train Loss: 3.6199169158935547 Eval Loss: 3.8444288301467897\n",
            "Running evaluation at iteration 391900...\n",
            "Iteration 391900, Train Loss: 3.800816059112549 Eval Loss: 3.7661416721343994\n",
            "Running evaluation at iteration 392000...\n",
            "Iteration 392000, Train Loss: 3.192554235458374 Eval Loss: 3.776315269470215\n",
            "Running evaluation at iteration 392100...\n",
            "Iteration 392100, Train Loss: 3.6236884593963623 Eval Loss: 3.7441296768188477\n",
            "Running evaluation at iteration 392200...\n",
            "Iteration 392200, Train Loss: 3.420931816101074 Eval Loss: 3.8392783331871034\n",
            "Running evaluation at iteration 392300...\n",
            "Iteration 392300, Train Loss: 3.8103394508361816 Eval Loss: 3.827712152004242\n",
            "Running evaluation at iteration 392400...\n",
            "Iteration 392400, Train Loss: 3.2207837104797363 Eval Loss: 3.840988426208496\n",
            "Running evaluation at iteration 392500...\n",
            "Iteration 392500, Train Loss: 4.119568347930908 Eval Loss: 3.7459572315216065\n",
            "Running evaluation at iteration 392600...\n",
            "Iteration 392600, Train Loss: 2.934112071990967 Eval Loss: 3.7316208195686342\n",
            "Running evaluation at iteration 392700...\n",
            "Iteration 392700, Train Loss: 3.9778974056243896 Eval Loss: 3.781510455608368\n",
            "Running evaluation at iteration 392800...\n",
            "Iteration 392800, Train Loss: 3.1438746452331543 Eval Loss: 3.7995325899124146\n",
            "Running evaluation at iteration 392900...\n",
            "Iteration 392900, Train Loss: 4.7525105476379395 Eval Loss: 3.863294713497162\n",
            "Running evaluation at iteration 393000...\n",
            "Iteration 393000, Train Loss: 4.020382404327393 Eval Loss: 3.8684614181518553\n",
            "Running evaluation at iteration 393100...\n",
            "Iteration 393100, Train Loss: 4.308918476104736 Eval Loss: 3.821894314289093\n",
            "Running evaluation at iteration 393200...\n",
            "Iteration 393200, Train Loss: 4.1724700927734375 Eval Loss: 3.7676302552223206\n",
            "Running evaluation at iteration 393300...\n",
            "Iteration 393300, Train Loss: 4.064080238342285 Eval Loss: 3.806333601474762\n",
            "Running evaluation at iteration 393400...\n",
            "Iteration 393400, Train Loss: 4.499837398529053 Eval Loss: 3.845869793891907\n",
            "Running evaluation at iteration 393500...\n",
            "Iteration 393500, Train Loss: 3.6129894256591797 Eval Loss: 3.8233120608329774\n",
            "Running evaluation at iteration 393600...\n",
            "Iteration 393600, Train Loss: 4.471162796020508 Eval Loss: 3.876831636428833\n",
            "Running evaluation at iteration 393700...\n",
            "Iteration 393700, Train Loss: 3.9165899753570557 Eval Loss: 3.8084539580345154\n",
            "Running evaluation at iteration 393800...\n",
            "Iteration 393800, Train Loss: 3.350151777267456 Eval Loss: 3.7809295582771303\n",
            "Running evaluation at iteration 393900...\n",
            "Iteration 393900, Train Loss: 3.6322898864746094 Eval Loss: 3.8013710594177246\n",
            "Running evaluation at iteration 394000...\n",
            "Iteration 394000, Train Loss: 3.2215728759765625 Eval Loss: 3.88936559677124\n",
            "Running evaluation at iteration 394100...\n",
            "Iteration 394100, Train Loss: 4.2980875968933105 Eval Loss: 3.8739619755744936\n",
            "Running evaluation at iteration 394200...\n",
            "Iteration 394200, Train Loss: 3.195260524749756 Eval Loss: 3.9164252853393555\n",
            "Running evaluation at iteration 394300...\n",
            "Iteration 394300, Train Loss: 3.587157964706421 Eval Loss: 3.714611234664917\n",
            "Running evaluation at iteration 394400...\n",
            "Iteration 394400, Train Loss: 3.6622421741485596 Eval Loss: 3.8360676908493043\n",
            "Running evaluation at iteration 394500...\n",
            "Iteration 394500, Train Loss: 4.24099063873291 Eval Loss: 3.839623110294342\n",
            "Running evaluation at iteration 394600...\n",
            "Iteration 394600, Train Loss: 4.078658580780029 Eval Loss: 3.763812096118927\n",
            "Running evaluation at iteration 394700...\n",
            "Iteration 394700, Train Loss: 4.542017936706543 Eval Loss: 3.790009253025055\n",
            "Running evaluation at iteration 394800...\n",
            "Iteration 394800, Train Loss: 4.623931884765625 Eval Loss: 3.847724871635437\n",
            "Running evaluation at iteration 394900...\n",
            "Iteration 394900, Train Loss: 3.62117338180542 Eval Loss: 3.8085973143577574\n",
            "Running evaluation at iteration 395000...\n",
            "Iteration 395000, Train Loss: 3.3300516605377197 Eval Loss: 3.8491088271141054\n",
            "Running evaluation at iteration 395100...\n",
            "Iteration 395100, Train Loss: 3.497405529022217 Eval Loss: 3.776841540336609\n",
            "Running evaluation at iteration 395200...\n",
            "Iteration 395200, Train Loss: 4.590137004852295 Eval Loss: 3.670488076210022\n",
            "Running evaluation at iteration 395300...\n",
            "Iteration 395300, Train Loss: 4.767273426055908 Eval Loss: 3.7220122385025025\n",
            "Running evaluation at iteration 395400...\n",
            "Iteration 395400, Train Loss: 4.326590061187744 Eval Loss: 3.8882516527175905\n",
            "Running evaluation at iteration 395500...\n",
            "Iteration 395500, Train Loss: 3.531533718109131 Eval Loss: 3.7590233397483828\n",
            "Running evaluation at iteration 395600...\n",
            "Iteration 395600, Train Loss: 3.643568992614746 Eval Loss: 3.7638602471351623\n",
            "Running evaluation at iteration 395700...\n",
            "Iteration 395700, Train Loss: 3.2560017108917236 Eval Loss: 3.7644425988197328\n",
            "Running evaluation at iteration 395800...\n",
            "Iteration 395800, Train Loss: 3.882781982421875 Eval Loss: 3.8034103775024413\n",
            "Running evaluation at iteration 395900...\n",
            "Iteration 395900, Train Loss: 2.897918701171875 Eval Loss: 3.7676496052742006\n",
            "Running evaluation at iteration 396000...\n",
            "Iteration 396000, Train Loss: 3.627516031265259 Eval Loss: 3.7624718618392943\n",
            "Running evaluation at iteration 396100...\n",
            "Iteration 396100, Train Loss: 4.025879859924316 Eval Loss: 3.952564227581024\n",
            "Running evaluation at iteration 396200...\n",
            "Iteration 396200, Train Loss: 3.281627655029297 Eval Loss: 3.867638907432556\n",
            "Running evaluation at iteration 396300...\n",
            "Iteration 396300, Train Loss: 4.22706413269043 Eval Loss: 3.7289132142066954\n",
            "Running evaluation at iteration 396400...\n",
            "Iteration 396400, Train Loss: 4.252376556396484 Eval Loss: 3.82366685628891\n",
            "Running evaluation at iteration 396500...\n",
            "Iteration 396500, Train Loss: 4.1739959716796875 Eval Loss: 3.7467810463905336\n",
            "Running evaluation at iteration 396600...\n",
            "Iteration 396600, Train Loss: 3.8580896854400635 Eval Loss: 3.8035170578956605\n",
            "Running evaluation at iteration 396700...\n",
            "Iteration 396700, Train Loss: 3.61470890045166 Eval Loss: 3.833664982318878\n",
            "Running evaluation at iteration 396800...\n",
            "Iteration 396800, Train Loss: 3.98124361038208 Eval Loss: 3.8237904500961304\n",
            "Running evaluation at iteration 396900...\n",
            "Iteration 396900, Train Loss: 4.3784990310668945 Eval Loss: 3.7099259376525877\n",
            "Running evaluation at iteration 397000...\n",
            "Iteration 397000, Train Loss: 3.712768793106079 Eval Loss: 3.865465531349182\n",
            "Running evaluation at iteration 397100...\n",
            "Iteration 397100, Train Loss: 3.4928138256073 Eval Loss: 3.8599933314323427\n",
            "Running evaluation at iteration 397200...\n",
            "Iteration 397200, Train Loss: 4.071279048919678 Eval Loss: 3.886830244064331\n",
            "Running evaluation at iteration 397300...\n",
            "Iteration 397300, Train Loss: 4.52145528793335 Eval Loss: 3.776874513626099\n",
            "Running evaluation at iteration 397400...\n",
            "Iteration 397400, Train Loss: 3.395467758178711 Eval Loss: 3.808215718269348\n",
            "Running evaluation at iteration 397500...\n",
            "Iteration 397500, Train Loss: 4.219087600708008 Eval Loss: 3.8516527819633484\n",
            "Running evaluation at iteration 397600...\n",
            "Iteration 397600, Train Loss: 5.877110481262207 Eval Loss: 3.817362673282623\n",
            "Running evaluation at iteration 397700...\n",
            "Iteration 397700, Train Loss: 3.6056249141693115 Eval Loss: 3.779385859966278\n",
            "Running evaluation at iteration 397800...\n",
            "Iteration 397800, Train Loss: 3.6399664878845215 Eval Loss: 3.8385720229148865\n",
            "Running evaluation at iteration 397900...\n",
            "Iteration 397900, Train Loss: 3.959685802459717 Eval Loss: 3.8045452451705932\n",
            "Running evaluation at iteration 398000...\n",
            "Iteration 398000, Train Loss: 3.7782740592956543 Eval Loss: 3.8016417241096496\n",
            "Running evaluation at iteration 398100...\n",
            "Iteration 398100, Train Loss: 4.272866249084473 Eval Loss: 3.7742006039619445\n",
            "Running evaluation at iteration 398200...\n",
            "Iteration 398200, Train Loss: 3.261711359024048 Eval Loss: 3.9264175581932066\n",
            "Running evaluation at iteration 398300...\n",
            "Iteration 398300, Train Loss: 3.582984209060669 Eval Loss: 3.8506665205955506\n",
            "Running evaluation at iteration 398400...\n",
            "Iteration 398400, Train Loss: 2.806532382965088 Eval Loss: 3.7740262031555174\n",
            "Running evaluation at iteration 398500...\n",
            "Iteration 398500, Train Loss: 4.594011306762695 Eval Loss: 3.827950186729431\n",
            "Running evaluation at iteration 398600...\n",
            "Iteration 398600, Train Loss: 4.015670299530029 Eval Loss: 3.8076309418678282\n",
            "Running evaluation at iteration 398700...\n",
            "Iteration 398700, Train Loss: 3.531364917755127 Eval Loss: 3.765795135498047\n",
            "Running evaluation at iteration 398800...\n",
            "Iteration 398800, Train Loss: 3.9320874214172363 Eval Loss: 3.9084330534935\n",
            "Running evaluation at iteration 398900...\n",
            "Iteration 398900, Train Loss: 4.381768703460693 Eval Loss: 3.6783922028541567\n",
            "Running evaluation at iteration 399000...\n",
            "Iteration 399000, Train Loss: 3.916539192199707 Eval Loss: 3.881784243583679\n",
            "Running evaluation at iteration 399100...\n",
            "Iteration 399100, Train Loss: 4.044500350952148 Eval Loss: 3.7708561563491823\n",
            "Running evaluation at iteration 399200...\n",
            "Iteration 399200, Train Loss: 5.06601095199585 Eval Loss: 3.7752502512931825\n",
            "Running evaluation at iteration 399300...\n",
            "Iteration 399300, Train Loss: 4.270742416381836 Eval Loss: 3.786553673744202\n",
            "Running evaluation at iteration 399400...\n",
            "Iteration 399400, Train Loss: 3.2300028800964355 Eval Loss: 3.837769374847412\n",
            "Running evaluation at iteration 399500...\n",
            "Iteration 399500, Train Loss: 4.126062870025635 Eval Loss: 3.8150437450408936\n",
            "Running evaluation at iteration 399600...\n",
            "Iteration 399600, Train Loss: 3.9964685440063477 Eval Loss: 3.858078193664551\n",
            "Running evaluation at iteration 399700...\n",
            "Iteration 399700, Train Loss: 3.0536961555480957 Eval Loss: 3.8429463124275207\n",
            "Running evaluation at iteration 399800...\n",
            "Iteration 399800, Train Loss: 4.349760055541992 Eval Loss: 3.861974928379059\n",
            "Running evaluation at iteration 399900...\n",
            "Iteration 399900, Train Loss: 3.7613513469696045 Eval Loss: 3.8425490927696226\n",
            "Running evaluation at iteration 400000...\n",
            "Iteration 400000, Train Loss: 3.658233404159546 Eval Loss: 3.8243607592582705\n",
            "Running evaluation at iteration 400100...\n",
            "Iteration 400100, Train Loss: 3.577873945236206 Eval Loss: 3.852733392715454\n",
            "Running evaluation at iteration 400200...\n",
            "Iteration 400200, Train Loss: 4.948303699493408 Eval Loss: 3.8129233598709105\n",
            "Running evaluation at iteration 400300...\n",
            "Iteration 400300, Train Loss: 4.038840293884277 Eval Loss: 3.7860446548461915\n",
            "Running evaluation at iteration 400400...\n",
            "Iteration 400400, Train Loss: 5.054722785949707 Eval Loss: 3.70328373670578\n",
            "Running evaluation at iteration 400500...\n",
            "Iteration 400500, Train Loss: 3.5285871028900146 Eval Loss: 3.8569843554496765\n",
            "Running evaluation at iteration 400600...\n",
            "Iteration 400600, Train Loss: 3.7881593704223633 Eval Loss: 3.895380117893219\n",
            "Running evaluation at iteration 400700...\n",
            "Iteration 400700, Train Loss: 3.783738613128662 Eval Loss: 3.8692587018013\n",
            "Running evaluation at iteration 400800...\n",
            "Iteration 400800, Train Loss: 3.318079948425293 Eval Loss: 3.8141044855117796\n",
            "Running evaluation at iteration 400900...\n",
            "Iteration 400900, Train Loss: 3.078207015991211 Eval Loss: 3.7814631724357604\n",
            "Running evaluation at iteration 401000...\n",
            "Iteration 401000, Train Loss: 3.7697641849517822 Eval Loss: 3.823667812347412\n",
            "Running evaluation at iteration 401100...\n",
            "Iteration 401100, Train Loss: 4.234532356262207 Eval Loss: 3.8226646256446837\n",
            "Running evaluation at iteration 401200...\n",
            "Iteration 401200, Train Loss: 2.7754762172698975 Eval Loss: 3.7914545559883117\n",
            "Running evaluation at iteration 401300...\n",
            "Iteration 401300, Train Loss: 4.127403259277344 Eval Loss: 3.9406655979156495\n",
            "Running evaluation at iteration 401400...\n",
            "Iteration 401400, Train Loss: 3.7719688415527344 Eval Loss: 3.788457663059235\n",
            "Running evaluation at iteration 401500...\n",
            "Iteration 401500, Train Loss: 4.536842346191406 Eval Loss: 3.8063774418830874\n",
            "Running evaluation at iteration 401600...\n",
            "Iteration 401600, Train Loss: 3.3909387588500977 Eval Loss: 3.8267626547813416\n",
            "Running evaluation at iteration 401700...\n",
            "Iteration 401700, Train Loss: 3.3693456649780273 Eval Loss: 3.8821538257598878\n",
            "Running evaluation at iteration 401800...\n",
            "Iteration 401800, Train Loss: 4.487363815307617 Eval Loss: 3.82375408411026\n",
            "Running evaluation at iteration 401900...\n",
            "Iteration 401900, Train Loss: 3.4101994037628174 Eval Loss: 3.7974829721450805\n",
            "Running evaluation at iteration 402000...\n",
            "Iteration 402000, Train Loss: 3.090830087661743 Eval Loss: 3.801495089530945\n",
            "Running evaluation at iteration 402100...\n",
            "Iteration 402100, Train Loss: 3.607905864715576 Eval Loss: 3.7189895629882814\n",
            "Running evaluation at iteration 402200...\n",
            "Iteration 402200, Train Loss: 4.414065361022949 Eval Loss: 3.731299993991852\n",
            "Running evaluation at iteration 402300...\n",
            "Iteration 402300, Train Loss: 4.244978904724121 Eval Loss: 3.831049294471741\n",
            "Running evaluation at iteration 402400...\n",
            "Iteration 402400, Train Loss: 3.4321377277374268 Eval Loss: 3.8466170024871826\n",
            "Running evaluation at iteration 402500...\n",
            "Iteration 402500, Train Loss: 3.9345312118530273 Eval Loss: 3.780478265285492\n",
            "Running evaluation at iteration 402600...\n",
            "Iteration 402600, Train Loss: 4.045313835144043 Eval Loss: 3.7904448962211608\n",
            "Running evaluation at iteration 402700...\n",
            "Iteration 402700, Train Loss: 3.8817574977874756 Eval Loss: 3.855398700237274\n",
            "Running evaluation at iteration 402800...\n",
            "Iteration 402800, Train Loss: 3.249281406402588 Eval Loss: 3.8028583407402037\n",
            "Running evaluation at iteration 402900...\n",
            "Iteration 402900, Train Loss: 4.580803394317627 Eval Loss: 3.7579412198066713\n",
            "Running evaluation at iteration 403000...\n",
            "Iteration 403000, Train Loss: 3.667015790939331 Eval Loss: 3.7846975994110106\n",
            "Running evaluation at iteration 403100...\n",
            "Iteration 403100, Train Loss: 3.4626834392547607 Eval Loss: 3.8260330820083617\n",
            "Running evaluation at iteration 403200...\n",
            "Iteration 403200, Train Loss: 3.862492322921753 Eval Loss: 3.8769200158119204\n",
            "Running evaluation at iteration 403300...\n",
            "Iteration 403300, Train Loss: 3.4511749744415283 Eval Loss: 3.871239936351776\n",
            "Running evaluation at iteration 403400...\n",
            "Iteration 403400, Train Loss: 4.485236644744873 Eval Loss: 3.808858368396759\n",
            "Running evaluation at iteration 403500...\n",
            "Iteration 403500, Train Loss: 3.8782734870910645 Eval Loss: 3.8348019909858704\n",
            "Running evaluation at iteration 403600...\n",
            "Iteration 403600, Train Loss: 3.9839212894439697 Eval Loss: 3.7769904589653014\n",
            "Running evaluation at iteration 403700...\n",
            "Iteration 403700, Train Loss: 3.7682018280029297 Eval Loss: 3.8530100560188294\n",
            "Running evaluation at iteration 403800...\n",
            "Iteration 403800, Train Loss: 3.9755501747131348 Eval Loss: 3.7772563433647157\n",
            "Running evaluation at iteration 403900...\n",
            "Iteration 403900, Train Loss: 3.888723134994507 Eval Loss: 3.7121239566802977\n",
            "Running evaluation at iteration 404000...\n",
            "Iteration 404000, Train Loss: 2.8253891468048096 Eval Loss: 3.8612019395828248\n",
            "Running evaluation at iteration 404100...\n",
            "Iteration 404100, Train Loss: 3.1671512126922607 Eval Loss: 3.7796696066856383\n",
            "Running evaluation at iteration 404200...\n",
            "Iteration 404200, Train Loss: 3.5933072566986084 Eval Loss: 3.7312436366081236\n",
            "Running evaluation at iteration 404300...\n",
            "Iteration 404300, Train Loss: 3.6900978088378906 Eval Loss: 3.730024313926697\n",
            "Running evaluation at iteration 404400...\n",
            "Iteration 404400, Train Loss: 3.580548048019409 Eval Loss: 3.7663268995285035\n",
            "Running evaluation at iteration 404500...\n",
            "Iteration 404500, Train Loss: 4.0742363929748535 Eval Loss: 3.760359642505646\n",
            "Running evaluation at iteration 404600...\n",
            "Iteration 404600, Train Loss: 3.6526973247528076 Eval Loss: 3.799955654144287\n",
            "Running evaluation at iteration 404700...\n",
            "Iteration 404700, Train Loss: 4.177986145019531 Eval Loss: 3.779699971675873\n",
            "Running evaluation at iteration 404800...\n",
            "Iteration 404800, Train Loss: 3.982649564743042 Eval Loss: 3.7848146891593935\n",
            "Running evaluation at iteration 404900...\n",
            "Iteration 404900, Train Loss: 3.4065544605255127 Eval Loss: 3.844668598175049\n",
            "Running evaluation at iteration 405000...\n",
            "Iteration 405000, Train Loss: 4.2329912185668945 Eval Loss: 3.8683730792999267\n",
            "Running evaluation at iteration 405100...\n",
            "Iteration 405100, Train Loss: 3.1937592029571533 Eval Loss: 3.80490035533905\n",
            "Running evaluation at iteration 405200...\n",
            "Iteration 405200, Train Loss: 3.4168834686279297 Eval Loss: 3.828742582798004\n",
            "Running evaluation at iteration 405300...\n",
            "Iteration 405300, Train Loss: 3.716829538345337 Eval Loss: 3.947430183887482\n",
            "Running evaluation at iteration 405400...\n",
            "Iteration 405400, Train Loss: 3.8734123706817627 Eval Loss: 3.792978534698486\n",
            "Running evaluation at iteration 405500...\n",
            "Iteration 405500, Train Loss: 3.0410609245300293 Eval Loss: 3.8348760056495665\n",
            "Running evaluation at iteration 405600...\n",
            "Iteration 405600, Train Loss: 3.8059794902801514 Eval Loss: 3.791211955547333\n",
            "Running evaluation at iteration 405700...\n",
            "Iteration 405700, Train Loss: 4.074671268463135 Eval Loss: 3.828604905605316\n",
            "Running evaluation at iteration 405800...\n",
            "Iteration 405800, Train Loss: 3.781445264816284 Eval Loss: 3.8711505365371703\n",
            "Running evaluation at iteration 405900...\n",
            "Iteration 405900, Train Loss: 4.048223495483398 Eval Loss: 3.8022003531455995\n",
            "Running evaluation at iteration 406000...\n",
            "Iteration 406000, Train Loss: 2.807746648788452 Eval Loss: 3.8855968713760376\n",
            "Running evaluation at iteration 406100...\n",
            "Iteration 406100, Train Loss: 4.130195617675781 Eval Loss: 3.8643388175964355\n",
            "Running evaluation at iteration 406200...\n",
            "Iteration 406200, Train Loss: 4.314692497253418 Eval Loss: 3.7652474212646485\n",
            "Running evaluation at iteration 406300...\n",
            "Iteration 406300, Train Loss: 3.930440902709961 Eval Loss: 3.73189567565918\n",
            "Running evaluation at iteration 406400...\n",
            "Iteration 406400, Train Loss: 4.234602928161621 Eval Loss: 3.8091730666160584\n",
            "Running evaluation at iteration 406500...\n",
            "Iteration 406500, Train Loss: 3.2575876712799072 Eval Loss: 3.8047230219841004\n",
            "Running evaluation at iteration 406600...\n",
            "Iteration 406600, Train Loss: 3.8555757999420166 Eval Loss: 3.8431757402420046\n",
            "Running evaluation at iteration 406700...\n",
            "Iteration 406700, Train Loss: 3.6687841415405273 Eval Loss: 3.7272330474853517\n",
            "Running evaluation at iteration 406800...\n",
            "Iteration 406800, Train Loss: 3.977613687515259 Eval Loss: 3.680931010246277\n",
            "Running evaluation at iteration 406900...\n",
            "Iteration 406900, Train Loss: 3.6073129177093506 Eval Loss: 3.849098331928253\n",
            "Running evaluation at iteration 407000...\n",
            "Iteration 407000, Train Loss: 5.341134071350098 Eval Loss: 3.8048206973075867\n",
            "Running evaluation at iteration 407100...\n",
            "Iteration 407100, Train Loss: 3.983572006225586 Eval Loss: 3.807826330661774\n",
            "Running evaluation at iteration 407200...\n",
            "Iteration 407200, Train Loss: 3.9610300064086914 Eval Loss: 3.893295073509216\n",
            "Running evaluation at iteration 407300...\n",
            "Iteration 407300, Train Loss: 2.934746742248535 Eval Loss: 3.7897307538986205\n",
            "Running evaluation at iteration 407400...\n",
            "Iteration 407400, Train Loss: 4.451007843017578 Eval Loss: 3.803822636604309\n",
            "Running evaluation at iteration 407500...\n",
            "Iteration 407500, Train Loss: 3.393009662628174 Eval Loss: 3.8655107498168944\n",
            "Running evaluation at iteration 407600...\n",
            "Iteration 407600, Train Loss: 4.482237339019775 Eval Loss: 3.9064457035064697\n",
            "Running evaluation at iteration 407700...\n",
            "Iteration 407700, Train Loss: 4.200473308563232 Eval Loss: 3.7750522971153258\n",
            "Running evaluation at iteration 407800...\n",
            "Iteration 407800, Train Loss: 3.7123045921325684 Eval Loss: 3.788912150859833\n",
            "Running evaluation at iteration 407900...\n",
            "Iteration 407900, Train Loss: 3.720146894454956 Eval Loss: 3.8548337507247923\n",
            "Running evaluation at iteration 408000...\n",
            "Iteration 408000, Train Loss: 4.285057067871094 Eval Loss: 3.8348683500289917\n",
            "Running evaluation at iteration 408100...\n",
            "Iteration 408100, Train Loss: 4.651975631713867 Eval Loss: 3.912335810661316\n",
            "Running evaluation at iteration 408200...\n",
            "Iteration 408200, Train Loss: 4.342621803283691 Eval Loss: 3.7706888246536256\n",
            "Running evaluation at iteration 408300...\n",
            "Iteration 408300, Train Loss: 3.250002145767212 Eval Loss: 3.795426969528198\n",
            "Running evaluation at iteration 408400...\n",
            "Iteration 408400, Train Loss: 4.307767391204834 Eval Loss: 3.7727845525741577\n",
            "Running evaluation at iteration 408500...\n",
            "Iteration 408500, Train Loss: 3.7120754718780518 Eval Loss: 3.837926933765411\n",
            "Running evaluation at iteration 408600...\n",
            "Iteration 408600, Train Loss: 3.389751434326172 Eval Loss: 3.8143902730941774\n",
            "Running evaluation at iteration 408700...\n",
            "Iteration 408700, Train Loss: 3.6742374897003174 Eval Loss: 3.665397250652313\n",
            "Running evaluation at iteration 408800...\n",
            "Iteration 408800, Train Loss: 3.2397632598876953 Eval Loss: 3.7919936299324037\n",
            "Running evaluation at iteration 408900...\n",
            "Iteration 408900, Train Loss: 4.496824264526367 Eval Loss: 3.8867751789093017\n",
            "Running evaluation at iteration 409000...\n",
            "Iteration 409000, Train Loss: 3.5734481811523438 Eval Loss: 3.851495957374573\n",
            "Running evaluation at iteration 409100...\n",
            "Iteration 409100, Train Loss: 4.346445560455322 Eval Loss: 3.7439975690841676\n",
            "Running evaluation at iteration 409200...\n",
            "Iteration 409200, Train Loss: 4.6851091384887695 Eval Loss: 3.8725273418426513\n",
            "Running evaluation at iteration 409300...\n",
            "Iteration 409300, Train Loss: 2.870897054672241 Eval Loss: 3.7387582993507387\n",
            "Running evaluation at iteration 409400...\n",
            "Iteration 409400, Train Loss: 4.724396705627441 Eval Loss: 3.7723723578453066\n",
            "Running evaluation at iteration 409500...\n",
            "Iteration 409500, Train Loss: 3.8114657402038574 Eval Loss: 3.8280775260925295\n",
            "Running evaluation at iteration 409600...\n",
            "Iteration 409600, Train Loss: 4.6577653884887695 Eval Loss: 3.8660427570343017\n",
            "Running evaluation at iteration 409700...\n",
            "Iteration 409700, Train Loss: 3.755324125289917 Eval Loss: 3.7377950024604796\n",
            "Running evaluation at iteration 409800...\n",
            "Iteration 409800, Train Loss: 3.8992905616760254 Eval Loss: 3.852430658340454\n",
            "Running evaluation at iteration 409900...\n",
            "Iteration 409900, Train Loss: 3.952260732650757 Eval Loss: 3.733900065422058\n",
            "Running evaluation at iteration 410000...\n",
            "Iteration 410000, Train Loss: 3.5230815410614014 Eval Loss: 3.7714219641685487\n",
            "Running evaluation at iteration 410100...\n",
            "Iteration 410100, Train Loss: 3.9382271766662598 Eval Loss: 3.8083064341545105\n",
            "Running evaluation at iteration 410200...\n",
            "Iteration 410200, Train Loss: 2.8459651470184326 Eval Loss: 3.7765088415145875\n",
            "Running evaluation at iteration 410300...\n",
            "Iteration 410300, Train Loss: 4.174361228942871 Eval Loss: 3.771094274520874\n",
            "Running evaluation at iteration 410400...\n",
            "Iteration 410400, Train Loss: 3.7831099033355713 Eval Loss: 3.838508505821228\n",
            "Running evaluation at iteration 410500...\n",
            "Iteration 410500, Train Loss: 3.364302158355713 Eval Loss: 3.838269214630127\n",
            "Running evaluation at iteration 410600...\n",
            "Iteration 410600, Train Loss: 4.646881103515625 Eval Loss: 3.8255279994010927\n",
            "Running evaluation at iteration 410700...\n",
            "Iteration 410700, Train Loss: 4.165491104125977 Eval Loss: 3.9224280190467833\n",
            "Running evaluation at iteration 410800...\n",
            "Iteration 410800, Train Loss: 3.2367167472839355 Eval Loss: 3.8500828194618224\n",
            "Running evaluation at iteration 410900...\n",
            "Iteration 410900, Train Loss: 3.4309542179107666 Eval Loss: 3.8728934001922606\n",
            "Running evaluation at iteration 411000...\n",
            "Iteration 411000, Train Loss: 3.830836057662964 Eval Loss: 3.82045770406723\n",
            "Running evaluation at iteration 411100...\n",
            "Iteration 411100, Train Loss: 4.263099670410156 Eval Loss: 3.8613676762580873\n",
            "Running evaluation at iteration 411200...\n",
            "Iteration 411200, Train Loss: 2.9602580070495605 Eval Loss: 3.7987640380859373\n",
            "Running evaluation at iteration 411300...\n",
            "Iteration 411300, Train Loss: 3.8423988819122314 Eval Loss: 3.712821192741394\n",
            "Running evaluation at iteration 411400...\n",
            "Iteration 411400, Train Loss: 3.6194260120391846 Eval Loss: 3.8433818769454957\n",
            "Running evaluation at iteration 411500...\n",
            "Iteration 411500, Train Loss: 2.6936452388763428 Eval Loss: 3.793119444847107\n",
            "Running evaluation at iteration 411600...\n",
            "Iteration 411600, Train Loss: 2.8013617992401123 Eval Loss: 3.797805144786835\n",
            "Running evaluation at iteration 411700...\n",
            "Iteration 411700, Train Loss: 2.7227675914764404 Eval Loss: 3.8559046387672424\n",
            "Running evaluation at iteration 411800...\n",
            "Iteration 411800, Train Loss: 4.446568489074707 Eval Loss: 3.9057010412216187\n",
            "Running evaluation at iteration 411900...\n",
            "Iteration 411900, Train Loss: 3.6216237545013428 Eval Loss: 3.7713409447669983\n",
            "Running evaluation at iteration 412000...\n",
            "Iteration 412000, Train Loss: 3.6183156967163086 Eval Loss: 3.6528112840652467\n",
            "Running evaluation at iteration 412100...\n",
            "Iteration 412100, Train Loss: 3.5166213512420654 Eval Loss: 3.8195390582084654\n",
            "Running evaluation at iteration 412200...\n",
            "Iteration 412200, Train Loss: 4.208088397979736 Eval Loss: 3.792029232978821\n",
            "Running evaluation at iteration 412300...\n",
            "Iteration 412300, Train Loss: 3.4040403366088867 Eval Loss: 3.83092205286026\n",
            "Running evaluation at iteration 412400...\n",
            "Iteration 412400, Train Loss: 3.211402416229248 Eval Loss: 3.8621577286720274\n",
            "Running evaluation at iteration 412500...\n",
            "Iteration 412500, Train Loss: 3.0702860355377197 Eval Loss: 3.7977634358406065\n",
            "Running evaluation at iteration 412600...\n",
            "Iteration 412600, Train Loss: 3.4290621280670166 Eval Loss: 3.7266307258605957\n",
            "Running evaluation at iteration 412700...\n",
            "Iteration 412700, Train Loss: 3.449948787689209 Eval Loss: 3.8331700468063357\n",
            "Running evaluation at iteration 412800...\n",
            "Iteration 412800, Train Loss: 3.452864408493042 Eval Loss: 3.7840794348716735\n",
            "Running evaluation at iteration 412900...\n",
            "Iteration 412900, Train Loss: 3.4819066524505615 Eval Loss: 3.8225703835487366\n",
            "Running evaluation at iteration 413000...\n",
            "Iteration 413000, Train Loss: 4.174346923828125 Eval Loss: 3.716028926372528\n",
            "Running evaluation at iteration 413100...\n",
            "Iteration 413100, Train Loss: 3.675975799560547 Eval Loss: 3.8457772755622863\n",
            "Running evaluation at iteration 413200...\n",
            "Iteration 413200, Train Loss: 3.3747613430023193 Eval Loss: 3.781996982097626\n",
            "Running evaluation at iteration 413300...\n",
            "Iteration 413300, Train Loss: 3.603864908218384 Eval Loss: 3.7424875855445863\n",
            "Running evaluation at iteration 413400...\n",
            "Iteration 413400, Train Loss: 3.909949541091919 Eval Loss: 3.7033917474746705\n",
            "Running evaluation at iteration 413500...\n",
            "Iteration 413500, Train Loss: 3.7943685054779053 Eval Loss: 3.765878162384033\n",
            "Running evaluation at iteration 413600...\n",
            "Iteration 413600, Train Loss: 3.9128732681274414 Eval Loss: 3.8428504991531374\n",
            "Running evaluation at iteration 413700...\n",
            "Iteration 413700, Train Loss: 3.964553117752075 Eval Loss: 3.8275298500061035\n",
            "Running evaluation at iteration 413800...\n",
            "Iteration 413800, Train Loss: 3.1961352825164795 Eval Loss: 3.903551654815674\n",
            "Running evaluation at iteration 413900...\n",
            "Iteration 413900, Train Loss: 3.6942548751831055 Eval Loss: 3.718194355964661\n",
            "Running evaluation at iteration 414000...\n",
            "Iteration 414000, Train Loss: 3.7846853733062744 Eval Loss: 3.7486625289916993\n",
            "Running evaluation at iteration 414100...\n",
            "Iteration 414100, Train Loss: 3.2003061771392822 Eval Loss: 3.7681809282302856\n",
            "Running evaluation at iteration 414200...\n",
            "Iteration 414200, Train Loss: 4.239482402801514 Eval Loss: 3.732632431983948\n",
            "Running evaluation at iteration 414300...\n",
            "Iteration 414300, Train Loss: 4.073869705200195 Eval Loss: 3.855876543521881\n",
            "Running evaluation at iteration 414400...\n",
            "Iteration 414400, Train Loss: 3.825634479522705 Eval Loss: 3.6789491534233094\n",
            "Running evaluation at iteration 414500...\n",
            "Iteration 414500, Train Loss: 3.837573766708374 Eval Loss: 3.9038552260398864\n",
            "Running evaluation at iteration 414600...\n",
            "Iteration 414600, Train Loss: 3.5589632987976074 Eval Loss: 3.790425190925598\n",
            "Running evaluation at iteration 414700...\n",
            "Iteration 414700, Train Loss: 4.356125354766846 Eval Loss: 3.8510327982902526\n",
            "Running evaluation at iteration 414800...\n",
            "Iteration 414800, Train Loss: 3.2245633602142334 Eval Loss: 3.74779123544693\n",
            "Running evaluation at iteration 414900...\n",
            "Iteration 414900, Train Loss: 4.696000099182129 Eval Loss: 3.7939615392684938\n",
            "Running evaluation at iteration 415000...\n",
            "Iteration 415000, Train Loss: 4.248888969421387 Eval Loss: 3.739240415096283\n",
            "Running evaluation at iteration 415100...\n",
            "Iteration 415100, Train Loss: 3.1496975421905518 Eval Loss: 3.834511091709137\n",
            "Running evaluation at iteration 415200...\n",
            "Iteration 415200, Train Loss: 3.7316009998321533 Eval Loss: 3.7975809526443483\n",
            "Running evaluation at iteration 415300...\n",
            "Iteration 415300, Train Loss: 3.8806369304656982 Eval Loss: 3.877471749782562\n",
            "Running evaluation at iteration 415400...\n",
            "Iteration 415400, Train Loss: 2.9839112758636475 Eval Loss: 3.791732747554779\n",
            "Running evaluation at iteration 415500...\n",
            "Iteration 415500, Train Loss: 4.3895182609558105 Eval Loss: 3.854738109111786\n",
            "Running evaluation at iteration 415600...\n",
            "Iteration 415600, Train Loss: 4.884110450744629 Eval Loss: 3.839323036670685\n",
            "Running evaluation at iteration 415700...\n",
            "Iteration 415700, Train Loss: 4.323644161224365 Eval Loss: 3.799691174030304\n",
            "Running evaluation at iteration 415800...\n",
            "Iteration 415800, Train Loss: 3.6240153312683105 Eval Loss: 3.837221245765686\n",
            "Running evaluation at iteration 415900...\n",
            "Iteration 415900, Train Loss: 4.841404438018799 Eval Loss: 3.798710422515869\n",
            "Running evaluation at iteration 416000...\n",
            "Iteration 416000, Train Loss: 3.0405709743499756 Eval Loss: 3.854049937725067\n",
            "Running evaluation at iteration 416100...\n",
            "Iteration 416100, Train Loss: 3.829887628555298 Eval Loss: 3.750079460144043\n",
            "Running evaluation at iteration 416200...\n",
            "Iteration 416200, Train Loss: 4.86559534072876 Eval Loss: 3.75335853099823\n",
            "Running evaluation at iteration 416300...\n",
            "Iteration 416300, Train Loss: 3.7554664611816406 Eval Loss: 3.7275656056404114\n",
            "Running evaluation at iteration 416400...\n",
            "Iteration 416400, Train Loss: 3.406238555908203 Eval Loss: 3.789143214225769\n",
            "Running evaluation at iteration 416500...\n",
            "Iteration 416500, Train Loss: 4.083310127258301 Eval Loss: 3.8255223536491396\n",
            "Running evaluation at iteration 416600...\n",
            "Iteration 416600, Train Loss: 4.31761360168457 Eval Loss: 3.850845522880554\n",
            "Running evaluation at iteration 416700...\n",
            "Iteration 416700, Train Loss: 4.18193244934082 Eval Loss: 3.8678291821479798\n",
            "Running evaluation at iteration 416800...\n",
            "Iteration 416800, Train Loss: 2.987720251083374 Eval Loss: 3.8416017866134644\n",
            "Running evaluation at iteration 416900...\n",
            "Iteration 416900, Train Loss: 4.342520713806152 Eval Loss: 3.8034489822387694\n",
            "Running evaluation at iteration 417000...\n",
            "Iteration 417000, Train Loss: 3.586841344833374 Eval Loss: 3.838821566104889\n",
            "Running evaluation at iteration 417100...\n",
            "Iteration 417100, Train Loss: 3.869657039642334 Eval Loss: 3.730355181694031\n",
            "Running evaluation at iteration 417200...\n",
            "Iteration 417200, Train Loss: 4.878880023956299 Eval Loss: 3.823849322795868\n",
            "Running evaluation at iteration 417300...\n",
            "Iteration 417300, Train Loss: 3.052527904510498 Eval Loss: 3.801673991680145\n",
            "Running evaluation at iteration 417400...\n",
            "Iteration 417400, Train Loss: 3.257373809814453 Eval Loss: 3.806011981964111\n",
            "Running evaluation at iteration 417500...\n",
            "Iteration 417500, Train Loss: 4.6163811683654785 Eval Loss: 3.7610216975212096\n",
            "Running evaluation at iteration 417600...\n",
            "Iteration 417600, Train Loss: 3.4872915744781494 Eval Loss: 3.822689435482025\n",
            "Running evaluation at iteration 417700...\n",
            "Iteration 417700, Train Loss: 3.910966634750366 Eval Loss: 3.782006597518921\n",
            "Running evaluation at iteration 417800...\n",
            "Iteration 417800, Train Loss: 4.746744632720947 Eval Loss: 3.7495127868652345\n",
            "Running evaluation at iteration 417900...\n",
            "Iteration 417900, Train Loss: 4.329356670379639 Eval Loss: 3.7792714500427245\n",
            "Running evaluation at iteration 418000...\n",
            "Iteration 418000, Train Loss: 3.7018964290618896 Eval Loss: 3.7895202016830445\n",
            "Running evaluation at iteration 418100...\n",
            "Iteration 418100, Train Loss: 4.3149895668029785 Eval Loss: 3.768034234046936\n",
            "Running evaluation at iteration 418200...\n",
            "Iteration 418200, Train Loss: 3.861405372619629 Eval Loss: 3.888078279495239\n",
            "Running evaluation at iteration 418300...\n",
            "Iteration 418300, Train Loss: 3.258023738861084 Eval Loss: 3.8180075311660766\n",
            "Running evaluation at iteration 418400...\n",
            "Iteration 418400, Train Loss: 3.6326286792755127 Eval Loss: 3.7706404519081116\n",
            "Running evaluation at iteration 418500...\n",
            "Iteration 418500, Train Loss: 4.677250385284424 Eval Loss: 3.8011939620971678\n",
            "Running evaluation at iteration 418600...\n",
            "Iteration 418600, Train Loss: 4.471948146820068 Eval Loss: 3.8328041791915894\n",
            "Running evaluation at iteration 418700...\n",
            "Iteration 418700, Train Loss: 4.1603264808654785 Eval Loss: 3.7809578514099123\n",
            "Running evaluation at iteration 418800...\n",
            "Iteration 418800, Train Loss: 3.1863226890563965 Eval Loss: 3.805562391281128\n",
            "Running evaluation at iteration 418900...\n",
            "Iteration 418900, Train Loss: 4.325596332550049 Eval Loss: 3.874455306529999\n",
            "Running evaluation at iteration 419000...\n",
            "Iteration 419000, Train Loss: 3.766937494277954 Eval Loss: 3.6904029273986816\n",
            "Running evaluation at iteration 419100...\n",
            "Iteration 419100, Train Loss: 4.416135787963867 Eval Loss: 3.8021458315849306\n",
            "Running evaluation at iteration 419200...\n",
            "Iteration 419200, Train Loss: 2.8944265842437744 Eval Loss: 3.816146311759949\n",
            "Running evaluation at iteration 419300...\n",
            "Iteration 419300, Train Loss: 3.9893598556518555 Eval Loss: 3.8224313759803774\n",
            "Running evaluation at iteration 419400...\n",
            "Iteration 419400, Train Loss: 3.948838233947754 Eval Loss: 3.745724880695343\n",
            "Running evaluation at iteration 419500...\n",
            "Iteration 419500, Train Loss: 3.6618130207061768 Eval Loss: 3.8922717928886414\n",
            "Running evaluation at iteration 419600...\n",
            "Iteration 419600, Train Loss: 4.466723442077637 Eval Loss: 3.749036340713501\n",
            "Running evaluation at iteration 419700...\n",
            "Iteration 419700, Train Loss: 4.275047302246094 Eval Loss: 3.80365088224411\n",
            "Running evaluation at iteration 419800...\n",
            "Iteration 419800, Train Loss: 4.616490840911865 Eval Loss: 3.775502290725708\n",
            "Running evaluation at iteration 419900...\n",
            "Iteration 419900, Train Loss: 3.33561635017395 Eval Loss: 3.8986770343780517\n",
            "Running evaluation at iteration 420000...\n",
            "Iteration 420000, Train Loss: 3.819658041000366 Eval Loss: 3.7918310260772703\n",
            "Running evaluation at iteration 420100...\n",
            "Iteration 420100, Train Loss: 3.437375068664551 Eval Loss: 3.7374798703193663\n",
            "Running evaluation at iteration 420200...\n",
            "Iteration 420200, Train Loss: 3.2324106693267822 Eval Loss: 3.8482024264335633\n",
            "Running evaluation at iteration 420300...\n",
            "Iteration 420300, Train Loss: 5.116767406463623 Eval Loss: 3.872705826759338\n",
            "Running evaluation at iteration 420400...\n",
            "Iteration 420400, Train Loss: 4.628076076507568 Eval Loss: 3.752700593471527\n",
            "Running evaluation at iteration 420500...\n",
            "Iteration 420500, Train Loss: 5.419093608856201 Eval Loss: 3.8091661977767943\n",
            "Running evaluation at iteration 420600...\n",
            "Iteration 420600, Train Loss: 3.484180450439453 Eval Loss: 3.80417489528656\n",
            "Running evaluation at iteration 420700...\n",
            "Iteration 420700, Train Loss: 4.4146881103515625 Eval Loss: 3.767457613945007\n",
            "Running evaluation at iteration 420800...\n",
            "Iteration 420800, Train Loss: 3.40978741645813 Eval Loss: 3.8509685039520263\n",
            "Running evaluation at iteration 420900...\n",
            "Iteration 420900, Train Loss: 4.251938819885254 Eval Loss: 3.7554597783088686\n",
            "Running evaluation at iteration 421000...\n",
            "Iteration 421000, Train Loss: 3.8170981407165527 Eval Loss: 3.8412995505332947\n",
            "Running evaluation at iteration 421100...\n",
            "Iteration 421100, Train Loss: 3.6963140964508057 Eval Loss: 3.8583341312408446\n",
            "Running evaluation at iteration 421200...\n",
            "Iteration 421200, Train Loss: 2.9508655071258545 Eval Loss: 3.7926101303100586\n",
            "Running evaluation at iteration 421300...\n",
            "Iteration 421300, Train Loss: 3.4661455154418945 Eval Loss: 3.82517772436142\n",
            "Running evaluation at iteration 421400...\n",
            "Iteration 421400, Train Loss: 3.8043630123138428 Eval Loss: 3.8555186653137206\n",
            "Running evaluation at iteration 421500...\n",
            "Iteration 421500, Train Loss: 3.855531692504883 Eval Loss: 3.785895507335663\n",
            "Running evaluation at iteration 421600...\n",
            "Iteration 421600, Train Loss: 3.496009111404419 Eval Loss: 3.8158860301971433\n",
            "Running evaluation at iteration 421700...\n",
            "Iteration 421700, Train Loss: 4.807866096496582 Eval Loss: 3.752171502113342\n",
            "Running evaluation at iteration 421800...\n",
            "Iteration 421800, Train Loss: 3.8898439407348633 Eval Loss: 3.8405376052856446\n",
            "Running evaluation at iteration 421900...\n",
            "Iteration 421900, Train Loss: 2.9623053073883057 Eval Loss: 3.8038997983932497\n",
            "Running evaluation at iteration 422000...\n",
            "Iteration 422000, Train Loss: 4.277229309082031 Eval Loss: 3.725445873737335\n",
            "Running evaluation at iteration 422100...\n",
            "Iteration 422100, Train Loss: 3.339725971221924 Eval Loss: 3.8885096192359923\n",
            "Running evaluation at iteration 422200...\n",
            "Iteration 422200, Train Loss: 4.094432353973389 Eval Loss: 3.7932098126411438\n",
            "Running evaluation at iteration 422300...\n",
            "Iteration 422300, Train Loss: 4.236777305603027 Eval Loss: 3.8698180103302002\n",
            "Running evaluation at iteration 422400...\n",
            "Iteration 422400, Train Loss: 3.741173267364502 Eval Loss: 3.757000563144684\n",
            "Running evaluation at iteration 422500...\n",
            "Iteration 422500, Train Loss: 4.250328063964844 Eval Loss: 3.8369014072418213\n",
            "Running evaluation at iteration 422600...\n",
            "Iteration 422600, Train Loss: 3.9780988693237305 Eval Loss: 3.793624849319458\n",
            "Running evaluation at iteration 422700...\n",
            "Iteration 422700, Train Loss: 4.105949878692627 Eval Loss: 3.8251973962783814\n",
            "Running evaluation at iteration 422800...\n",
            "Iteration 422800, Train Loss: 3.7653403282165527 Eval Loss: 3.763581576347351\n",
            "Running evaluation at iteration 422900...\n",
            "Iteration 422900, Train Loss: 4.781031608581543 Eval Loss: 3.7564596056938173\n",
            "Running evaluation at iteration 423000...\n",
            "Iteration 423000, Train Loss: 3.5560922622680664 Eval Loss: 3.7506688022613526\n",
            "Running evaluation at iteration 423100...\n",
            "Iteration 423100, Train Loss: 4.223606586456299 Eval Loss: 3.7499050235748292\n",
            "Running evaluation at iteration 423200...\n",
            "Iteration 423200, Train Loss: 3.482576847076416 Eval Loss: 3.7880649757385254\n",
            "Running evaluation at iteration 423300...\n",
            "Iteration 423300, Train Loss: 3.991636276245117 Eval Loss: 3.791936821937561\n",
            "Running evaluation at iteration 423400...\n",
            "Iteration 423400, Train Loss: 3.302222490310669 Eval Loss: 3.8277027153968812\n",
            "Running evaluation at iteration 423500...\n",
            "Iteration 423500, Train Loss: 3.6196932792663574 Eval Loss: 3.920558063983917\n",
            "Running evaluation at iteration 423600...\n",
            "Iteration 423600, Train Loss: 3.847606658935547 Eval Loss: 3.8830349946022036\n",
            "Running evaluation at iteration 423700...\n",
            "Iteration 423700, Train Loss: 4.240187168121338 Eval Loss: 3.844770257472992\n",
            "Running evaluation at iteration 423800...\n",
            "Iteration 423800, Train Loss: 5.366674900054932 Eval Loss: 3.7872723388671874\n",
            "Running evaluation at iteration 423900...\n",
            "Iteration 423900, Train Loss: 3.921534776687622 Eval Loss: 3.8245209789276124\n",
            "Running evaluation at iteration 424000...\n",
            "Iteration 424000, Train Loss: 4.567699432373047 Eval Loss: 3.8484827971458433\n",
            "Running evaluation at iteration 424100...\n",
            "Iteration 424100, Train Loss: 4.117066383361816 Eval Loss: 3.833441572189331\n",
            "Running evaluation at iteration 424200...\n",
            "Iteration 424200, Train Loss: 3.098900318145752 Eval Loss: 3.726423237323761\n",
            "Running evaluation at iteration 424300...\n",
            "Iteration 424300, Train Loss: 3.758772373199463 Eval Loss: 3.8170196986198426\n",
            "Running evaluation at iteration 424400...\n",
            "Iteration 424400, Train Loss: 4.140585899353027 Eval Loss: 3.813027727603912\n",
            "Running evaluation at iteration 424500...\n",
            "Iteration 424500, Train Loss: 4.076432228088379 Eval Loss: 3.7985930013656617\n",
            "Running evaluation at iteration 424600...\n",
            "Iteration 424600, Train Loss: 3.6886227130889893 Eval Loss: 3.8113248467445375\n",
            "Running evaluation at iteration 424700...\n",
            "Iteration 424700, Train Loss: 3.4863600730895996 Eval Loss: 3.761047372817993\n",
            "Running evaluation at iteration 424800...\n",
            "Iteration 424800, Train Loss: 3.29590106010437 Eval Loss: 3.828092432022095\n",
            "Running evaluation at iteration 424900...\n",
            "Iteration 424900, Train Loss: 4.320619106292725 Eval Loss: 3.80021143913269\n",
            "Running evaluation at iteration 425000...\n",
            "Iteration 425000, Train Loss: 3.3238134384155273 Eval Loss: 3.7946394443511964\n",
            "Running evaluation at iteration 425100...\n",
            "Iteration 425100, Train Loss: 3.4303746223449707 Eval Loss: 3.8251797461509707\n",
            "Running evaluation at iteration 425200...\n",
            "Iteration 425200, Train Loss: 4.671108722686768 Eval Loss: 3.790728073120117\n",
            "Running evaluation at iteration 425300...\n",
            "Iteration 425300, Train Loss: 4.7531585693359375 Eval Loss: 3.8066444182395935\n",
            "Running evaluation at iteration 425400...\n",
            "Iteration 425400, Train Loss: 3.4449989795684814 Eval Loss: 3.753477830886841\n",
            "Running evaluation at iteration 425500...\n",
            "Iteration 425500, Train Loss: 4.251543998718262 Eval Loss: 3.839025933742523\n",
            "Running evaluation at iteration 425600...\n",
            "Iteration 425600, Train Loss: 3.32167911529541 Eval Loss: 3.751087727546692\n",
            "Running evaluation at iteration 425700...\n",
            "Iteration 425700, Train Loss: 3.3770644664764404 Eval Loss: 3.845600700378418\n",
            "Running evaluation at iteration 425800...\n",
            "Iteration 425800, Train Loss: 4.374063491821289 Eval Loss: 3.7842582058906555\n",
            "Running evaluation at iteration 425900...\n",
            "Iteration 425900, Train Loss: 3.595263957977295 Eval Loss: 3.8395659208297728\n",
            "Running evaluation at iteration 426000...\n",
            "Iteration 426000, Train Loss: 4.491399765014648 Eval Loss: 3.826042354106903\n",
            "Running evaluation at iteration 426100...\n",
            "Iteration 426100, Train Loss: 3.648860216140747 Eval Loss: 3.8574564552307127\n",
            "Running evaluation at iteration 426200...\n",
            "Iteration 426200, Train Loss: 4.197808265686035 Eval Loss: 3.840504722595215\n",
            "Running evaluation at iteration 426300...\n",
            "Iteration 426300, Train Loss: 3.5485591888427734 Eval Loss: 3.708516118526459\n",
            "Running evaluation at iteration 426400...\n",
            "Iteration 426400, Train Loss: 3.389699935913086 Eval Loss: 3.8796987414360045\n",
            "Running evaluation at iteration 426500...\n",
            "Iteration 426500, Train Loss: 3.688091516494751 Eval Loss: 3.8585434007644652\n",
            "Running evaluation at iteration 426600...\n",
            "Iteration 426600, Train Loss: 3.9352903366088867 Eval Loss: 3.836477468013763\n",
            "Running evaluation at iteration 426700...\n",
            "Iteration 426700, Train Loss: 3.8451359272003174 Eval Loss: 3.7847944259643556\n",
            "Running evaluation at iteration 426800...\n",
            "Iteration 426800, Train Loss: 3.855454444885254 Eval Loss: 3.697935245037079\n",
            "Running evaluation at iteration 426900...\n",
            "Iteration 426900, Train Loss: 3.7066164016723633 Eval Loss: 3.7538093376159667\n",
            "Running evaluation at iteration 427000...\n",
            "Iteration 427000, Train Loss: 4.379074573516846 Eval Loss: 3.8018361139297485\n",
            "Running evaluation at iteration 427100...\n",
            "Iteration 427100, Train Loss: 4.09588623046875 Eval Loss: 3.7065772461891173\n",
            "Running evaluation at iteration 427200...\n",
            "Iteration 427200, Train Loss: 3.3016772270202637 Eval Loss: 3.8019959139823913\n",
            "Running evaluation at iteration 427300...\n",
            "Iteration 427300, Train Loss: 3.422645092010498 Eval Loss: 3.889479696750641\n",
            "Running evaluation at iteration 427400...\n",
            "Iteration 427400, Train Loss: 2.9163644313812256 Eval Loss: 3.878535747528076\n",
            "Running evaluation at iteration 427500...\n",
            "Iteration 427500, Train Loss: 2.3364481925964355 Eval Loss: 3.732312777042389\n",
            "Running evaluation at iteration 427600...\n",
            "Iteration 427600, Train Loss: 3.0633490085601807 Eval Loss: 3.7894512701034544\n",
            "Running evaluation at iteration 427700...\n",
            "Iteration 427700, Train Loss: 3.928426504135132 Eval Loss: 3.8458446979522707\n",
            "Running evaluation at iteration 427800...\n",
            "Iteration 427800, Train Loss: 3.43876576423645 Eval Loss: 3.831594066619873\n",
            "Running evaluation at iteration 427900...\n",
            "Iteration 427900, Train Loss: 3.7344555854797363 Eval Loss: 3.6758989214897158\n",
            "Running evaluation at iteration 428000...\n",
            "Iteration 428000, Train Loss: 3.8821566104888916 Eval Loss: 3.9442665219306945\n",
            "Running evaluation at iteration 428100...\n",
            "Iteration 428100, Train Loss: 3.8447296619415283 Eval Loss: 3.8279413104057314\n",
            "Running evaluation at iteration 428200...\n",
            "Iteration 428200, Train Loss: 4.161101341247559 Eval Loss: 3.7508925223350524\n",
            "Running evaluation at iteration 428300...\n",
            "Iteration 428300, Train Loss: 3.931149482727051 Eval Loss: 3.801869194507599\n",
            "Running evaluation at iteration 428400...\n",
            "Iteration 428400, Train Loss: 3.7498183250427246 Eval Loss: 3.7629641342163085\n",
            "Running evaluation at iteration 428500...\n",
            "Iteration 428500, Train Loss: 4.068315029144287 Eval Loss: 3.7524283027648924\n",
            "Running evaluation at iteration 428600...\n",
            "Iteration 428600, Train Loss: 3.159811496734619 Eval Loss: 3.8641721701622007\n",
            "Running evaluation at iteration 428700...\n",
            "Iteration 428700, Train Loss: 3.697810173034668 Eval Loss: 3.8094022488594055\n",
            "Running evaluation at iteration 428800...\n",
            "Iteration 428800, Train Loss: 3.384209632873535 Eval Loss: 3.8233700251579283\n",
            "Running evaluation at iteration 428900...\n",
            "Iteration 428900, Train Loss: 4.171170711517334 Eval Loss: 3.7588549947738645\n",
            "Running evaluation at iteration 429000...\n",
            "Iteration 429000, Train Loss: 4.223233699798584 Eval Loss: 3.771868894100189\n",
            "Running evaluation at iteration 429100...\n",
            "Iteration 429100, Train Loss: 3.8923585414886475 Eval Loss: 3.8068538165092467\n",
            "Running evaluation at iteration 429200...\n",
            "Iteration 429200, Train Loss: 3.999037027359009 Eval Loss: 3.8309752416610716\n",
            "Running evaluation at iteration 429300...\n",
            "Iteration 429300, Train Loss: 4.717965602874756 Eval Loss: 3.729402940273285\n",
            "Running evaluation at iteration 429400...\n",
            "Iteration 429400, Train Loss: 5.224363327026367 Eval Loss: 3.8034368896484376\n",
            "Running evaluation at iteration 429500...\n",
            "Iteration 429500, Train Loss: 3.8852593898773193 Eval Loss: 3.748541498184204\n",
            "Running evaluation at iteration 429600...\n",
            "Iteration 429600, Train Loss: 3.8789093494415283 Eval Loss: 3.849119505882263\n",
            "Running evaluation at iteration 429700...\n",
            "Iteration 429700, Train Loss: 4.1072611808776855 Eval Loss: 3.8124755907058714\n",
            "Running evaluation at iteration 429800...\n",
            "Iteration 429800, Train Loss: 4.162282466888428 Eval Loss: 3.819203324317932\n",
            "Running evaluation at iteration 429900...\n",
            "Iteration 429900, Train Loss: 3.9244608879089355 Eval Loss: 3.816778955459595\n",
            "Running evaluation at iteration 430000...\n",
            "Iteration 430000, Train Loss: 4.078683853149414 Eval Loss: 3.8204002499580385\n",
            "Running evaluation at iteration 430100...\n",
            "Iteration 430100, Train Loss: 3.718355655670166 Eval Loss: 3.895298659801483\n",
            "Running evaluation at iteration 430200...\n",
            "Iteration 430200, Train Loss: 4.608537197113037 Eval Loss: 3.811562855243683\n",
            "Running evaluation at iteration 430300...\n",
            "Iteration 430300, Train Loss: 4.3251752853393555 Eval Loss: 3.774433126449585\n",
            "Running evaluation at iteration 430400...\n",
            "Iteration 430400, Train Loss: 3.8445262908935547 Eval Loss: 3.7733092641830446\n",
            "Running evaluation at iteration 430500...\n",
            "Iteration 430500, Train Loss: 3.320446491241455 Eval Loss: 3.866781973838806\n",
            "Running evaluation at iteration 430600...\n",
            "Iteration 430600, Train Loss: 3.6608357429504395 Eval Loss: 3.792779071331024\n",
            "Running evaluation at iteration 430700...\n",
            "Iteration 430700, Train Loss: 4.025417327880859 Eval Loss: 3.796155421733856\n",
            "Running evaluation at iteration 430800...\n",
            "Iteration 430800, Train Loss: 4.778051853179932 Eval Loss: 3.8887527346611024\n",
            "Running evaluation at iteration 430900...\n",
            "Iteration 430900, Train Loss: 4.411300182342529 Eval Loss: 3.7815299367904665\n",
            "Running evaluation at iteration 431000...\n",
            "Iteration 431000, Train Loss: 3.2668020725250244 Eval Loss: 3.8518874716758726\n",
            "Running evaluation at iteration 431100...\n",
            "Iteration 431100, Train Loss: 4.871184825897217 Eval Loss: 3.847511010169983\n",
            "Running evaluation at iteration 431200...\n",
            "Iteration 431200, Train Loss: 3.8373208045959473 Eval Loss: 3.7868849301338194\n",
            "Running evaluation at iteration 431300...\n",
            "Iteration 431300, Train Loss: 3.7011945247650146 Eval Loss: 3.785165300369263\n",
            "Running evaluation at iteration 431400...\n",
            "Iteration 431400, Train Loss: 3.353242874145508 Eval Loss: 3.7550283789634706\n",
            "Running evaluation at iteration 431500...\n",
            "Iteration 431500, Train Loss: 3.5077967643737793 Eval Loss: 3.8857605862617492\n",
            "Running evaluation at iteration 431600...\n",
            "Iteration 431600, Train Loss: 4.4415669441223145 Eval Loss: 3.8475293016433714\n",
            "Running evaluation at iteration 431700...\n",
            "Iteration 431700, Train Loss: 3.9573469161987305 Eval Loss: 3.7140988874435426\n",
            "Running evaluation at iteration 431800...\n",
            "Iteration 431800, Train Loss: 3.748896360397339 Eval Loss: 3.773250253200531\n",
            "Running evaluation at iteration 431900...\n",
            "Iteration 431900, Train Loss: 5.015906810760498 Eval Loss: 3.8014300656318665\n",
            "Running evaluation at iteration 432000...\n",
            "Iteration 432000, Train Loss: 3.046610116958618 Eval Loss: 3.84684937953949\n",
            "Running evaluation at iteration 432100...\n",
            "Iteration 432100, Train Loss: 3.881023645401001 Eval Loss: 3.7882721042633056\n",
            "Running evaluation at iteration 432200...\n",
            "Iteration 432200, Train Loss: 2.7628514766693115 Eval Loss: 3.869671938419342\n",
            "Running evaluation at iteration 432300...\n",
            "Iteration 432300, Train Loss: 4.731081485748291 Eval Loss: 3.7333701777458193\n",
            "Running evaluation at iteration 432400...\n",
            "Iteration 432400, Train Loss: 3.9412620067596436 Eval Loss: 3.843893690109253\n",
            "Running evaluation at iteration 432500...\n",
            "Iteration 432500, Train Loss: 2.7965102195739746 Eval Loss: 3.770228292942047\n",
            "Running evaluation at iteration 432600...\n",
            "Iteration 432600, Train Loss: 3.313138723373413 Eval Loss: 3.7173511385917664\n",
            "Running evaluation at iteration 432700...\n",
            "Iteration 432700, Train Loss: 4.60999059677124 Eval Loss: 3.7277252578735354\n",
            "Running evaluation at iteration 432800...\n",
            "Iteration 432800, Train Loss: 4.459369659423828 Eval Loss: 3.7934195971488953\n",
            "Running evaluation at iteration 432900...\n",
            "Iteration 432900, Train Loss: 3.410709857940674 Eval Loss: 3.7516184663772583\n",
            "Running evaluation at iteration 433000...\n",
            "Iteration 433000, Train Loss: 3.5177297592163086 Eval Loss: 3.749064507484436\n",
            "Running evaluation at iteration 433100...\n",
            "Iteration 433100, Train Loss: 3.405759811401367 Eval Loss: 3.760169095993042\n",
            "Running evaluation at iteration 433200...\n",
            "Iteration 433200, Train Loss: 3.576148748397827 Eval Loss: 3.7624485063552857\n",
            "Running evaluation at iteration 433300...\n",
            "Iteration 433300, Train Loss: 3.266371965408325 Eval Loss: 3.820481767654419\n",
            "Running evaluation at iteration 433400...\n",
            "Iteration 433400, Train Loss: 3.0966498851776123 Eval Loss: 3.806809084415436\n",
            "Running evaluation at iteration 433500...\n",
            "Iteration 433500, Train Loss: 3.2601571083068848 Eval Loss: 3.759216148853302\n",
            "Running evaluation at iteration 433600...\n",
            "Iteration 433600, Train Loss: 3.951364517211914 Eval Loss: 3.8082785773277283\n",
            "Running evaluation at iteration 433700...\n",
            "Iteration 433700, Train Loss: 3.3846700191497803 Eval Loss: 3.80691623210907\n",
            "Running evaluation at iteration 433800...\n",
            "Iteration 433800, Train Loss: 3.867276668548584 Eval Loss: 3.795584354400635\n",
            "Running evaluation at iteration 433900...\n",
            "Iteration 433900, Train Loss: 5.045992851257324 Eval Loss: 3.8362151646614073\n",
            "Running evaluation at iteration 434000...\n",
            "Iteration 434000, Train Loss: 3.3765063285827637 Eval Loss: 3.8128628444671633\n",
            "Running evaluation at iteration 434100...\n",
            "Iteration 434100, Train Loss: 2.7641146183013916 Eval Loss: 3.783115494251251\n",
            "Running evaluation at iteration 434200...\n",
            "Iteration 434200, Train Loss: 2.935448169708252 Eval Loss: 3.8205356550216676\n",
            "Running evaluation at iteration 434300...\n",
            "Iteration 434300, Train Loss: 3.058584213256836 Eval Loss: 3.804235119819641\n",
            "Running evaluation at iteration 434400...\n",
            "Iteration 434400, Train Loss: 4.371706962585449 Eval Loss: 3.751142630577087\n",
            "Running evaluation at iteration 434500...\n",
            "Iteration 434500, Train Loss: 3.4960153102874756 Eval Loss: 3.8900334453582763\n",
            "Running evaluation at iteration 434600...\n",
            "Iteration 434600, Train Loss: 4.008327484130859 Eval Loss: 3.820013587474823\n",
            "Running evaluation at iteration 434700...\n",
            "Iteration 434700, Train Loss: 3.5264546871185303 Eval Loss: 3.7035382270812987\n",
            "Running evaluation at iteration 434800...\n",
            "Iteration 434800, Train Loss: 4.882753849029541 Eval Loss: 3.7736267495155333\n",
            "Running evaluation at iteration 434900...\n",
            "Iteration 434900, Train Loss: 4.5217509269714355 Eval Loss: 3.803293890953064\n",
            "Running evaluation at iteration 435000...\n",
            "Iteration 435000, Train Loss: 4.3279128074646 Eval Loss: 3.750409541130066\n",
            "Running evaluation at iteration 435100...\n",
            "Iteration 435100, Train Loss: 3.675471305847168 Eval Loss: 3.7818915820121766\n",
            "Running evaluation at iteration 435200...\n",
            "Iteration 435200, Train Loss: 3.28006911277771 Eval Loss: 3.7848637175559996\n",
            "Running evaluation at iteration 435300...\n",
            "Iteration 435300, Train Loss: 3.797703504562378 Eval Loss: 3.8426724243164063\n",
            "Running evaluation at iteration 435400...\n",
            "Iteration 435400, Train Loss: 3.4432592391967773 Eval Loss: 3.7743648958206175\n",
            "Running evaluation at iteration 435500...\n",
            "Iteration 435500, Train Loss: 3.2401974201202393 Eval Loss: 3.8329172992706297\n",
            "Running evaluation at iteration 435600...\n",
            "Iteration 435600, Train Loss: 3.0301573276519775 Eval Loss: 3.8385532021522524\n",
            "Running evaluation at iteration 435700...\n",
            "Iteration 435700, Train Loss: 3.4576165676116943 Eval Loss: 3.7983505606651304\n",
            "Running evaluation at iteration 435800...\n",
            "Iteration 435800, Train Loss: 4.130871772766113 Eval Loss: 3.734927055835724\n",
            "Running evaluation at iteration 435900...\n",
            "Iteration 435900, Train Loss: 3.3882012367248535 Eval Loss: 3.745036222934723\n",
            "Running evaluation at iteration 436000...\n",
            "Iteration 436000, Train Loss: 4.730029106140137 Eval Loss: 3.7315892696380617\n",
            "Running evaluation at iteration 436100...\n",
            "Iteration 436100, Train Loss: 3.91670560836792 Eval Loss: 3.788593475818634\n",
            "Running evaluation at iteration 436200...\n",
            "Iteration 436200, Train Loss: 4.591859340667725 Eval Loss: 3.8272251176834104\n",
            "Running evaluation at iteration 436300...\n",
            "Iteration 436300, Train Loss: 4.367669582366943 Eval Loss: 3.719453558921814\n",
            "Running evaluation at iteration 436400...\n",
            "Iteration 436400, Train Loss: 4.884129524230957 Eval Loss: 3.731111714839935\n",
            "Running evaluation at iteration 436500...\n",
            "Iteration 436500, Train Loss: 3.175328016281128 Eval Loss: 3.7558951687812807\n",
            "Running evaluation at iteration 436600...\n",
            "Iteration 436600, Train Loss: 3.9698121547698975 Eval Loss: 3.830794129371643\n",
            "Running evaluation at iteration 436700...\n",
            "Iteration 436700, Train Loss: 2.8621246814727783 Eval Loss: 3.8393964099884035\n",
            "Running evaluation at iteration 436800...\n",
            "Iteration 436800, Train Loss: 3.4395406246185303 Eval Loss: 3.7325227522850035\n",
            "Running evaluation at iteration 436900...\n",
            "Iteration 436900, Train Loss: 4.259328365325928 Eval Loss: 3.7833073496818543\n",
            "Running evaluation at iteration 437000...\n",
            "Iteration 437000, Train Loss: 3.5019521713256836 Eval Loss: 3.7686132860183714\n",
            "Running evaluation at iteration 437100...\n",
            "Iteration 437100, Train Loss: 3.5470826625823975 Eval Loss: 3.784140501022339\n",
            "Running evaluation at iteration 437200...\n",
            "Iteration 437200, Train Loss: 4.166084289550781 Eval Loss: 3.72509877204895\n",
            "Running evaluation at iteration 437300...\n",
            "Iteration 437300, Train Loss: 3.407869338989258 Eval Loss: 3.779975950717926\n",
            "Running evaluation at iteration 437400...\n",
            "Iteration 437400, Train Loss: 3.639477491378784 Eval Loss: 3.791014487743378\n",
            "Running evaluation at iteration 437500...\n",
            "Iteration 437500, Train Loss: 3.667023181915283 Eval Loss: 3.8371864271163942\n",
            "Running evaluation at iteration 437600...\n",
            "Iteration 437600, Train Loss: 4.71390962600708 Eval Loss: 3.9189995098114014\n",
            "Running evaluation at iteration 437700...\n",
            "Iteration 437700, Train Loss: 3.604468584060669 Eval Loss: 3.824167230129242\n",
            "Running evaluation at iteration 437800...\n",
            "Iteration 437800, Train Loss: 3.810598373413086 Eval Loss: 3.8397814631462097\n",
            "Running evaluation at iteration 437900...\n",
            "Iteration 437900, Train Loss: 3.737917184829712 Eval Loss: 3.8956497621536257\n",
            "Running evaluation at iteration 438000...\n",
            "Iteration 438000, Train Loss: 3.490678310394287 Eval Loss: 3.7779994940757753\n",
            "Running evaluation at iteration 438100...\n",
            "Iteration 438100, Train Loss: 3.5770347118377686 Eval Loss: 3.800307343006134\n",
            "Running evaluation at iteration 438200...\n",
            "Iteration 438200, Train Loss: 3.8877429962158203 Eval Loss: 3.856338005065918\n",
            "Running evaluation at iteration 438300...\n",
            "Iteration 438300, Train Loss: 3.742497205734253 Eval Loss: 3.8355765676498415\n",
            "Running evaluation at iteration 438400...\n",
            "Iteration 438400, Train Loss: 4.764131546020508 Eval Loss: 3.806067957878113\n",
            "Running evaluation at iteration 438500...\n",
            "Iteration 438500, Train Loss: 4.120181560516357 Eval Loss: 3.718023979663849\n",
            "Running evaluation at iteration 438600...\n",
            "Iteration 438600, Train Loss: 2.9586756229400635 Eval Loss: 3.8745967102050782\n",
            "Running evaluation at iteration 438700...\n",
            "Iteration 438700, Train Loss: 4.298202991485596 Eval Loss: 3.7686524748802186\n",
            "Running evaluation at iteration 438800...\n",
            "Iteration 438800, Train Loss: 4.248050689697266 Eval Loss: 3.8370684385299683\n",
            "Running evaluation at iteration 438900...\n",
            "Iteration 438900, Train Loss: 3.9861860275268555 Eval Loss: 3.7354900193214418\n",
            "Running evaluation at iteration 439000...\n",
            "Iteration 439000, Train Loss: 3.1585500240325928 Eval Loss: 3.688781695365906\n",
            "Running evaluation at iteration 439100...\n",
            "Iteration 439100, Train Loss: 2.8196685314178467 Eval Loss: 3.780782163143158\n",
            "Running evaluation at iteration 439200...\n",
            "Iteration 439200, Train Loss: 3.703141689300537 Eval Loss: 3.6858838868141173\n",
            "Running evaluation at iteration 439300...\n",
            "Iteration 439300, Train Loss: 4.139440536499023 Eval Loss: 3.846683659553528\n",
            "Running evaluation at iteration 439400...\n",
            "Iteration 439400, Train Loss: 3.506192684173584 Eval Loss: 3.80670179605484\n",
            "Running evaluation at iteration 439500...\n",
            "Iteration 439500, Train Loss: 3.279956817626953 Eval Loss: 3.704605658054352\n",
            "Running evaluation at iteration 439600...\n",
            "Iteration 439600, Train Loss: 3.103184700012207 Eval Loss: 3.8385867953300474\n",
            "Running evaluation at iteration 439700...\n",
            "Iteration 439700, Train Loss: 2.9664878845214844 Eval Loss: 3.859730257987976\n",
            "Running evaluation at iteration 439800...\n",
            "Iteration 439800, Train Loss: 3.238320827484131 Eval Loss: 3.8660733103752136\n",
            "Running evaluation at iteration 439900...\n",
            "Iteration 439900, Train Loss: 4.200575351715088 Eval Loss: 3.704305155277252\n",
            "Running evaluation at iteration 440000...\n",
            "Iteration 440000, Train Loss: 4.049347400665283 Eval Loss: 3.8273990154266357\n",
            "Running evaluation at iteration 440100...\n",
            "Iteration 440100, Train Loss: 3.849099636077881 Eval Loss: 3.7338385653495787\n",
            "Running evaluation at iteration 440200...\n",
            "Iteration 440200, Train Loss: 4.591865062713623 Eval Loss: 3.7553816390037538\n",
            "Running evaluation at iteration 440300...\n",
            "Iteration 440300, Train Loss: 3.513746976852417 Eval Loss: 3.798767423629761\n",
            "Running evaluation at iteration 440400...\n",
            "Iteration 440400, Train Loss: 3.784224033355713 Eval Loss: 3.8565602803230288\n",
            "Running evaluation at iteration 440500...\n",
            "Iteration 440500, Train Loss: 4.345210075378418 Eval Loss: 3.764388542175293\n",
            "Running evaluation at iteration 440600...\n",
            "Iteration 440600, Train Loss: 3.571652889251709 Eval Loss: 3.799667725563049\n",
            "Running evaluation at iteration 440700...\n",
            "Iteration 440700, Train Loss: 4.553635597229004 Eval Loss: 3.875805277824402\n",
            "Running evaluation at iteration 440800...\n",
            "Iteration 440800, Train Loss: 3.13201642036438 Eval Loss: 3.736462666988373\n",
            "Running evaluation at iteration 440900...\n",
            "Iteration 440900, Train Loss: 4.985708236694336 Eval Loss: 3.728399589061737\n",
            "Running evaluation at iteration 441000...\n",
            "Iteration 441000, Train Loss: 3.380861282348633 Eval Loss: 3.873287253379822\n",
            "Running evaluation at iteration 441100...\n",
            "Iteration 441100, Train Loss: 4.537968635559082 Eval Loss: 3.7480035161972047\n",
            "Running evaluation at iteration 441200...\n",
            "Iteration 441200, Train Loss: 3.5672848224639893 Eval Loss: 3.7700483298301695\n",
            "Running evaluation at iteration 441300...\n",
            "Iteration 441300, Train Loss: 3.660752296447754 Eval Loss: 3.7576916337013246\n",
            "Running evaluation at iteration 441400...\n",
            "Iteration 441400, Train Loss: 3.5808937549591064 Eval Loss: 3.779655010700226\n",
            "Running evaluation at iteration 441500...\n",
            "Iteration 441500, Train Loss: 3.6234917640686035 Eval Loss: 3.7774425292015077\n",
            "Running evaluation at iteration 441600...\n",
            "Iteration 441600, Train Loss: 3.8350210189819336 Eval Loss: 3.6819538831710816\n",
            "Running evaluation at iteration 441700...\n",
            "Iteration 441700, Train Loss: 3.2278616428375244 Eval Loss: 3.8854310083389283\n",
            "Running evaluation at iteration 441800...\n",
            "Iteration 441800, Train Loss: 5.074851036071777 Eval Loss: 3.787592091560364\n",
            "Running evaluation at iteration 441900...\n",
            "Iteration 441900, Train Loss: 3.8645336627960205 Eval Loss: 3.8122962975502013\n",
            "Running evaluation at iteration 442000...\n",
            "Iteration 442000, Train Loss: 4.335476875305176 Eval Loss: 3.69496369600296\n",
            "Running evaluation at iteration 442100...\n",
            "Iteration 442100, Train Loss: 3.3229174613952637 Eval Loss: 3.833873131275177\n",
            "Running evaluation at iteration 442200...\n",
            "Iteration 442200, Train Loss: 3.499716281890869 Eval Loss: 3.794348976612091\n",
            "Running evaluation at iteration 442300...\n",
            "Iteration 442300, Train Loss: 2.7362029552459717 Eval Loss: 3.9090763187408446\n",
            "Running evaluation at iteration 442400...\n",
            "Iteration 442400, Train Loss: 4.054612636566162 Eval Loss: 3.7931902170181275\n",
            "Running evaluation at iteration 442500...\n",
            "Iteration 442500, Train Loss: 4.460395812988281 Eval Loss: 3.8067026686668397\n",
            "Running evaluation at iteration 442600...\n",
            "Iteration 442600, Train Loss: 3.870252847671509 Eval Loss: 3.7817704582214358\n",
            "Running evaluation at iteration 442700...\n",
            "Iteration 442700, Train Loss: 3.9537971019744873 Eval Loss: 3.857865056991577\n",
            "Running evaluation at iteration 442800...\n",
            "Iteration 442800, Train Loss: 4.289251327514648 Eval Loss: 3.80457154750824\n",
            "Running evaluation at iteration 442900...\n",
            "Iteration 442900, Train Loss: 4.363584995269775 Eval Loss: 3.7612043499946592\n",
            "Running evaluation at iteration 443000...\n",
            "Iteration 443000, Train Loss: 3.1805226802825928 Eval Loss: 3.820069589614868\n",
            "Running evaluation at iteration 443100...\n",
            "Iteration 443100, Train Loss: 3.0188405513763428 Eval Loss: 3.901040620803833\n",
            "Running evaluation at iteration 443200...\n",
            "Iteration 443200, Train Loss: 3.6751933097839355 Eval Loss: 3.743795700073242\n",
            "Running evaluation at iteration 443300...\n",
            "Iteration 443300, Train Loss: 3.3820087909698486 Eval Loss: 3.842915437221527\n",
            "Running evaluation at iteration 443400...\n",
            "Iteration 443400, Train Loss: 3.130847692489624 Eval Loss: 3.738691246509552\n",
            "Running evaluation at iteration 443500...\n",
            "Iteration 443500, Train Loss: 3.4175126552581787 Eval Loss: 3.7674194741249085\n",
            "Running evaluation at iteration 443600...\n",
            "Iteration 443600, Train Loss: 3.7831196784973145 Eval Loss: 3.7557588028907776\n",
            "Running evaluation at iteration 443700...\n",
            "Iteration 443700, Train Loss: 3.6876838207244873 Eval Loss: 3.84476087808609\n",
            "Running evaluation at iteration 443800...\n",
            "Iteration 443800, Train Loss: 2.8638110160827637 Eval Loss: 3.912627980709076\n",
            "Running evaluation at iteration 443900...\n",
            "Iteration 443900, Train Loss: 3.685530424118042 Eval Loss: 3.814329197406769\n",
            "Running evaluation at iteration 444000...\n",
            "Iteration 444000, Train Loss: 2.9388043880462646 Eval Loss: 3.8254411125183108\n",
            "Running evaluation at iteration 444100...\n",
            "Iteration 444100, Train Loss: 2.9858157634735107 Eval Loss: 3.781516933441162\n",
            "Running evaluation at iteration 444200...\n",
            "Iteration 444200, Train Loss: 4.467996597290039 Eval Loss: 3.8405922865867614\n",
            "Running evaluation at iteration 444300...\n",
            "Iteration 444300, Train Loss: 5.304296493530273 Eval Loss: 3.750390045642853\n",
            "Running evaluation at iteration 444400...\n",
            "Iteration 444400, Train Loss: 4.282840251922607 Eval Loss: 3.8169701671600342\n",
            "Running evaluation at iteration 444500...\n",
            "Iteration 444500, Train Loss: 3.304641008377075 Eval Loss: 3.841443729400635\n",
            "Running evaluation at iteration 444600...\n",
            "Iteration 444600, Train Loss: 4.095561981201172 Eval Loss: 3.8161369657516477\n",
            "Running evaluation at iteration 444700...\n",
            "Iteration 444700, Train Loss: 3.1514642238616943 Eval Loss: 3.749560787677765\n",
            "Running evaluation at iteration 444800...\n",
            "Iteration 444800, Train Loss: 4.09035587310791 Eval Loss: 3.7820027756690977\n",
            "Running evaluation at iteration 444900...\n",
            "Iteration 444900, Train Loss: 3.5967302322387695 Eval Loss: 3.797518653869629\n",
            "Running evaluation at iteration 445000...\n",
            "Iteration 445000, Train Loss: 3.6879677772521973 Eval Loss: 3.8278899574279786\n",
            "Running evaluation at iteration 445100...\n",
            "Iteration 445100, Train Loss: 3.5411531925201416 Eval Loss: 3.8104585790634156\n",
            "Running evaluation at iteration 445200...\n",
            "Iteration 445200, Train Loss: 3.6792356967926025 Eval Loss: 3.77586487531662\n",
            "Running evaluation at iteration 445300...\n",
            "Iteration 445300, Train Loss: 4.4838643074035645 Eval Loss: 3.8405162477493286\n",
            "Running evaluation at iteration 445400...\n",
            "Iteration 445400, Train Loss: 4.618452072143555 Eval Loss: 3.8205162715911865\n",
            "Running evaluation at iteration 445500...\n",
            "Iteration 445500, Train Loss: 3.662440776824951 Eval Loss: 3.709260323047638\n",
            "Running evaluation at iteration 445600...\n",
            "Iteration 445600, Train Loss: 4.259459972381592 Eval Loss: 3.7393424201011656\n",
            "Running evaluation at iteration 445700...\n",
            "Iteration 445700, Train Loss: 3.5518577098846436 Eval Loss: 3.816450572013855\n",
            "Running evaluation at iteration 445800...\n",
            "Iteration 445800, Train Loss: 3.176809549331665 Eval Loss: 3.8725989198684694\n",
            "Running evaluation at iteration 445900...\n",
            "Iteration 445900, Train Loss: 3.387000560760498 Eval Loss: 3.815725483894348\n",
            "Running evaluation at iteration 446000...\n",
            "Iteration 446000, Train Loss: 2.7670888900756836 Eval Loss: 3.8004492354393005\n",
            "Running evaluation at iteration 446100...\n",
            "Iteration 446100, Train Loss: 3.718315362930298 Eval Loss: 3.7803152084350584\n",
            "Running evaluation at iteration 446200...\n",
            "Iteration 446200, Train Loss: 4.116594314575195 Eval Loss: 3.8318579626083373\n",
            "Running evaluation at iteration 446300...\n",
            "Iteration 446300, Train Loss: 4.530482292175293 Eval Loss: 3.8123076248168943\n",
            "Running evaluation at iteration 446400...\n",
            "Iteration 446400, Train Loss: 3.0992543697357178 Eval Loss: 3.709940197467804\n",
            "Running evaluation at iteration 446500...\n",
            "Iteration 446500, Train Loss: 3.6963717937469482 Eval Loss: 3.861677014827728\n",
            "Running evaluation at iteration 446600...\n",
            "Iteration 446600, Train Loss: 4.319558620452881 Eval Loss: 3.842865433692932\n",
            "Running evaluation at iteration 446700...\n",
            "Iteration 446700, Train Loss: 3.7344441413879395 Eval Loss: 3.776056020259857\n",
            "Running evaluation at iteration 446800...\n",
            "Iteration 446800, Train Loss: 4.8444647789001465 Eval Loss: 3.8441260743141172\n",
            "Running evaluation at iteration 446900...\n",
            "Iteration 446900, Train Loss: 3.577580213546753 Eval Loss: 3.679020974636078\n",
            "Running evaluation at iteration 447000...\n",
            "Iteration 447000, Train Loss: 3.9693422317504883 Eval Loss: 3.8839337539672854\n",
            "Running evaluation at iteration 447100...\n",
            "Iteration 447100, Train Loss: 2.9973208904266357 Eval Loss: 3.8316061425209047\n",
            "Running evaluation at iteration 447200...\n",
            "Iteration 447200, Train Loss: 3.1445751190185547 Eval Loss: 3.837957167625427\n",
            "Running evaluation at iteration 447300...\n",
            "Iteration 447300, Train Loss: 3.9598050117492676 Eval Loss: 3.7945162892341613\n",
            "Running evaluation at iteration 447400...\n",
            "Iteration 447400, Train Loss: 3.8662655353546143 Eval Loss: 3.7683561301231383\n",
            "Running evaluation at iteration 447500...\n",
            "Iteration 447500, Train Loss: 4.103829383850098 Eval Loss: 3.7693689584732057\n",
            "Running evaluation at iteration 447600...\n",
            "Iteration 447600, Train Loss: 4.197396278381348 Eval Loss: 3.819218897819519\n",
            "Running evaluation at iteration 447700...\n",
            "Iteration 447700, Train Loss: 3.176500082015991 Eval Loss: 3.7864446568489076\n",
            "Running evaluation at iteration 447800...\n",
            "Iteration 447800, Train Loss: 3.6139354705810547 Eval Loss: 3.746423552036285\n",
            "Running evaluation at iteration 447900...\n",
            "Iteration 447900, Train Loss: 3.7652499675750732 Eval Loss: 3.7524566435813904\n",
            "Running evaluation at iteration 448000...\n",
            "Iteration 448000, Train Loss: 3.687776803970337 Eval Loss: 3.7924003314971926\n",
            "Running evaluation at iteration 448100...\n",
            "Iteration 448100, Train Loss: 3.868542432785034 Eval Loss: 3.812357017993927\n",
            "Running evaluation at iteration 448200...\n",
            "Iteration 448200, Train Loss: 3.720784902572632 Eval Loss: 3.801422839164734\n",
            "Running evaluation at iteration 448300...\n",
            "Iteration 448300, Train Loss: 3.2762672901153564 Eval Loss: 3.71643128156662\n",
            "Running evaluation at iteration 448400...\n",
            "Iteration 448400, Train Loss: 4.205849647521973 Eval Loss: 3.685997142791748\n",
            "Running evaluation at iteration 448500...\n",
            "Iteration 448500, Train Loss: 4.369670867919922 Eval Loss: 3.7811903381347656\n",
            "Running evaluation at iteration 448600...\n",
            "Iteration 448600, Train Loss: 4.817046165466309 Eval Loss: 3.8970788383483885\n",
            "Running evaluation at iteration 448700...\n",
            "Iteration 448700, Train Loss: 4.053386211395264 Eval Loss: 3.78221657037735\n",
            "Running evaluation at iteration 448800...\n",
            "Iteration 448800, Train Loss: 4.128690242767334 Eval Loss: 3.818890118598938\n",
            "Running evaluation at iteration 448900...\n",
            "Iteration 448900, Train Loss: 4.229712009429932 Eval Loss: 3.778281874656677\n",
            "Running evaluation at iteration 449000...\n",
            "Iteration 449000, Train Loss: 3.3893702030181885 Eval Loss: 3.849114315509796\n",
            "Running evaluation at iteration 449100...\n",
            "Iteration 449100, Train Loss: 3.771353006362915 Eval Loss: 3.827654106616974\n",
            "Running evaluation at iteration 449200...\n",
            "Iteration 449200, Train Loss: 4.296229839324951 Eval Loss: 3.745260224342346\n",
            "Running evaluation at iteration 449300...\n",
            "Iteration 449300, Train Loss: 4.08610725402832 Eval Loss: 3.743282067775726\n",
            "Running evaluation at iteration 449400...\n",
            "Iteration 449400, Train Loss: 2.9365992546081543 Eval Loss: 3.842025318145752\n",
            "Running evaluation at iteration 449500...\n",
            "Iteration 449500, Train Loss: 3.596982955932617 Eval Loss: 3.790140218734741\n",
            "Running evaluation at iteration 449600...\n",
            "Iteration 449600, Train Loss: 5.029680252075195 Eval Loss: 3.7441335773468016\n",
            "Running evaluation at iteration 449700...\n",
            "Iteration 449700, Train Loss: 4.001136302947998 Eval Loss: 3.7729448199272158\n",
            "Running evaluation at iteration 449800...\n",
            "Iteration 449800, Train Loss: 3.641772508621216 Eval Loss: 3.817578186988831\n",
            "Running evaluation at iteration 449900...\n",
            "Iteration 449900, Train Loss: 3.877425193786621 Eval Loss: 3.8709209752082825\n",
            "Running evaluation at iteration 450000...\n",
            "Iteration 450000, Train Loss: 5.006099700927734 Eval Loss: 3.755810697078705\n",
            "Running evaluation at iteration 450100...\n",
            "Iteration 450100, Train Loss: 4.121379852294922 Eval Loss: 3.8878299260139464\n",
            "Running evaluation at iteration 450200...\n",
            "Iteration 450200, Train Loss: 4.555333614349365 Eval Loss: 3.7606909251213074\n",
            "Running evaluation at iteration 450300...\n",
            "Iteration 450300, Train Loss: 3.9070632457733154 Eval Loss: 3.8120232963562013\n",
            "Running evaluation at iteration 450400...\n",
            "Iteration 450400, Train Loss: 3.4810283184051514 Eval Loss: 3.6703603959083555\n",
            "Running evaluation at iteration 450500...\n",
            "Iteration 450500, Train Loss: 4.149484157562256 Eval Loss: 3.8439325642585755\n",
            "Running evaluation at iteration 450600...\n",
            "Iteration 450600, Train Loss: 3.537707567214966 Eval Loss: 3.754907569885254\n",
            "Running evaluation at iteration 450700...\n",
            "Iteration 450700, Train Loss: 4.583300590515137 Eval Loss: 3.8387493562698363\n",
            "Running evaluation at iteration 450800...\n",
            "Iteration 450800, Train Loss: 3.0925204753875732 Eval Loss: 3.781720817089081\n",
            "Running evaluation at iteration 450900...\n",
            "Iteration 450900, Train Loss: 3.7340245246887207 Eval Loss: 3.8497183871269227\n",
            "Running evaluation at iteration 451000...\n",
            "Iteration 451000, Train Loss: 4.007052421569824 Eval Loss: 3.776807565689087\n",
            "Running evaluation at iteration 451100...\n",
            "Iteration 451100, Train Loss: 3.4433858394622803 Eval Loss: 3.86272164106369\n",
            "Running evaluation at iteration 451200...\n",
            "Iteration 451200, Train Loss: 3.302140712738037 Eval Loss: 3.8140973258018493\n",
            "Running evaluation at iteration 451300...\n",
            "Iteration 451300, Train Loss: 3.6689486503601074 Eval Loss: 3.79720840215683\n",
            "Running evaluation at iteration 451400...\n",
            "Iteration 451400, Train Loss: 3.3770174980163574 Eval Loss: 3.9651763892173766\n",
            "Running evaluation at iteration 451500...\n",
            "Iteration 451500, Train Loss: 3.9827706813812256 Eval Loss: 3.802954626083374\n",
            "Running evaluation at iteration 451600...\n",
            "Iteration 451600, Train Loss: 3.807913303375244 Eval Loss: 3.815542821884155\n",
            "Running evaluation at iteration 451700...\n",
            "Iteration 451700, Train Loss: 3.2691636085510254 Eval Loss: 3.77100084066391\n",
            "Running evaluation at iteration 451800...\n",
            "Iteration 451800, Train Loss: 4.244833469390869 Eval Loss: 3.8084347581863405\n",
            "Running evaluation at iteration 451900...\n",
            "Iteration 451900, Train Loss: 3.6483795642852783 Eval Loss: 3.734632387161255\n",
            "Running evaluation at iteration 452000...\n",
            "Iteration 452000, Train Loss: 3.683980703353882 Eval Loss: 3.8349114537239073\n",
            "Running evaluation at iteration 452100...\n",
            "Iteration 452100, Train Loss: 3.0978128910064697 Eval Loss: 3.928543791770935\n",
            "Running evaluation at iteration 452200...\n",
            "Iteration 452200, Train Loss: 3.627976894378662 Eval Loss: 3.859284346103668\n",
            "Running evaluation at iteration 452300...\n",
            "Iteration 452300, Train Loss: 2.96614933013916 Eval Loss: 3.8069360136985777\n",
            "Running evaluation at iteration 452400...\n",
            "Iteration 452400, Train Loss: 4.705982208251953 Eval Loss: 3.7466511487960816\n",
            "Running evaluation at iteration 452500...\n",
            "Iteration 452500, Train Loss: 3.223553419113159 Eval Loss: 3.835619308948517\n",
            "Running evaluation at iteration 452600...\n",
            "Iteration 452600, Train Loss: 3.663132667541504 Eval Loss: 3.7139588141441346\n",
            "Running evaluation at iteration 452700...\n",
            "Iteration 452700, Train Loss: 3.806685209274292 Eval Loss: 3.7683987283706664\n",
            "Running evaluation at iteration 452800...\n",
            "Iteration 452800, Train Loss: 4.106644630432129 Eval Loss: 3.7468950581550597\n",
            "Running evaluation at iteration 452900...\n",
            "Iteration 452900, Train Loss: 4.1945719718933105 Eval Loss: 3.8722054886817934\n",
            "Running evaluation at iteration 453000...\n",
            "Iteration 453000, Train Loss: 4.20238733291626 Eval Loss: 3.7267808938026428\n",
            "Running evaluation at iteration 453100...\n",
            "Iteration 453100, Train Loss: 3.2582521438598633 Eval Loss: 3.7200496435165404\n",
            "Running evaluation at iteration 453200...\n",
            "Iteration 453200, Train Loss: 3.2272753715515137 Eval Loss: 3.7215587973594664\n",
            "Running evaluation at iteration 453300...\n",
            "Iteration 453300, Train Loss: 4.221253395080566 Eval Loss: 3.857288210391998\n",
            "Running evaluation at iteration 453400...\n",
            "Iteration 453400, Train Loss: 3.218777894973755 Eval Loss: 3.7748388600349427\n",
            "Running evaluation at iteration 453500...\n",
            "Iteration 453500, Train Loss: 3.359733819961548 Eval Loss: 3.7634837651252746\n",
            "Running evaluation at iteration 453600...\n",
            "Iteration 453600, Train Loss: 3.438899040222168 Eval Loss: 3.840098023414612\n",
            "Running evaluation at iteration 453700...\n",
            "Iteration 453700, Train Loss: 3.839466094970703 Eval Loss: 3.8411499452590943\n",
            "Running evaluation at iteration 453800...\n",
            "Iteration 453800, Train Loss: 3.3273561000823975 Eval Loss: 3.768864305019379\n",
            "Running evaluation at iteration 453900...\n",
            "Iteration 453900, Train Loss: 4.074822902679443 Eval Loss: 3.798840973377228\n",
            "Running evaluation at iteration 454000...\n",
            "Iteration 454000, Train Loss: 4.566939830780029 Eval Loss: 3.847691898345947\n",
            "Running evaluation at iteration 454100...\n",
            "Iteration 454100, Train Loss: 4.144570350646973 Eval Loss: 3.6999983048439025\n",
            "Running evaluation at iteration 454200...\n",
            "Iteration 454200, Train Loss: 3.3840317726135254 Eval Loss: 3.754374861717224\n",
            "Running evaluation at iteration 454300...\n",
            "Iteration 454300, Train Loss: 3.2503151893615723 Eval Loss: 3.834079878330231\n",
            "Running evaluation at iteration 454400...\n",
            "Iteration 454400, Train Loss: 3.659172773361206 Eval Loss: 3.7575186610221865\n",
            "Running evaluation at iteration 454500...\n",
            "Iteration 454500, Train Loss: 3.838791608810425 Eval Loss: 3.7460976529121397\n",
            "Running evaluation at iteration 454600...\n",
            "Iteration 454600, Train Loss: 5.358231067657471 Eval Loss: 3.768167176246643\n",
            "Running evaluation at iteration 454700...\n",
            "Iteration 454700, Train Loss: 4.329992771148682 Eval Loss: 3.683423964977264\n",
            "Running evaluation at iteration 454800...\n",
            "Iteration 454800, Train Loss: 3.6072304248809814 Eval Loss: 3.862187922000885\n",
            "Running evaluation at iteration 454900...\n",
            "Iteration 454900, Train Loss: 4.340121269226074 Eval Loss: 3.7518696331977845\n",
            "Running evaluation at iteration 455000...\n",
            "Iteration 455000, Train Loss: 4.049752235412598 Eval Loss: 3.806917805671692\n",
            "Running evaluation at iteration 455100...\n",
            "Iteration 455100, Train Loss: 4.349127769470215 Eval Loss: 3.6927175402641295\n",
            "Running evaluation at iteration 455200...\n",
            "Iteration 455200, Train Loss: 4.089595794677734 Eval Loss: 3.8372337794303895\n",
            "Running evaluation at iteration 455300...\n",
            "Iteration 455300, Train Loss: 3.7599589824676514 Eval Loss: 3.8177430725097654\n",
            "Running evaluation at iteration 455400...\n",
            "Iteration 455400, Train Loss: 4.214700222015381 Eval Loss: 3.7791247940063477\n",
            "Running evaluation at iteration 455500...\n",
            "Iteration 455500, Train Loss: 3.7079598903656006 Eval Loss: 3.8479229974746705\n",
            "Running evaluation at iteration 455600...\n",
            "Iteration 455600, Train Loss: 3.3148789405822754 Eval Loss: 3.9049646139144896\n",
            "Running evaluation at iteration 455700...\n",
            "Iteration 455700, Train Loss: 3.07279109954834 Eval Loss: 3.838738100528717\n",
            "Running evaluation at iteration 455800...\n",
            "Iteration 455800, Train Loss: 4.0977678298950195 Eval Loss: 3.7847519516944885\n",
            "Running evaluation at iteration 455900...\n",
            "Iteration 455900, Train Loss: 3.4033408164978027 Eval Loss: 3.6742732095718384\n",
            "Running evaluation at iteration 456000...\n",
            "Iteration 456000, Train Loss: 3.7106356620788574 Eval Loss: 3.8688983488082886\n",
            "Running evaluation at iteration 456100...\n",
            "Iteration 456100, Train Loss: 3.4831392765045166 Eval Loss: 3.761224706172943\n",
            "Running evaluation at iteration 456200...\n",
            "Iteration 456200, Train Loss: 3.6206881999969482 Eval Loss: 3.672578511238098\n",
            "Running evaluation at iteration 456300...\n",
            "Iteration 456300, Train Loss: 5.069087028503418 Eval Loss: 3.761561408042908\n",
            "Running evaluation at iteration 456400...\n",
            "Iteration 456400, Train Loss: 3.8871471881866455 Eval Loss: 3.8002391958236696\n",
            "Running evaluation at iteration 456500...\n",
            "Iteration 456500, Train Loss: 3.89351749420166 Eval Loss: 3.8670070147514344\n",
            "Running evaluation at iteration 456600...\n",
            "Iteration 456600, Train Loss: 3.4021780490875244 Eval Loss: 3.855385458469391\n",
            "Running evaluation at iteration 456700...\n",
            "Iteration 456700, Train Loss: 3.764547824859619 Eval Loss: 3.84895956993103\n",
            "Running evaluation at iteration 456800...\n",
            "Iteration 456800, Train Loss: 4.013848304748535 Eval Loss: 3.854144024848938\n",
            "Running evaluation at iteration 456900...\n",
            "Iteration 456900, Train Loss: 3.9960978031158447 Eval Loss: 3.8809412145614623\n",
            "Running evaluation at iteration 457000...\n",
            "Iteration 457000, Train Loss: 2.984379529953003 Eval Loss: 3.7685893845558165\n",
            "Running evaluation at iteration 457100...\n",
            "Iteration 457100, Train Loss: 3.687286615371704 Eval Loss: 3.8079294085502626\n",
            "Running evaluation at iteration 457200...\n",
            "Iteration 457200, Train Loss: 4.611809253692627 Eval Loss: 3.7660330271720888\n",
            "Running evaluation at iteration 457300...\n",
            "Iteration 457300, Train Loss: 3.295292377471924 Eval Loss: 3.7855937552452086\n",
            "Running evaluation at iteration 457400...\n",
            "Iteration 457400, Train Loss: 2.694809913635254 Eval Loss: 3.7874506545066833\n",
            "Running evaluation at iteration 457500...\n",
            "Iteration 457500, Train Loss: 3.8138225078582764 Eval Loss: 3.7657203793525698\n",
            "Running evaluation at iteration 457600...\n",
            "Iteration 457600, Train Loss: 3.785097599029541 Eval Loss: 3.7729758834838867\n",
            "Running evaluation at iteration 457700...\n",
            "Iteration 457700, Train Loss: 4.433220863342285 Eval Loss: 3.8161544609069824\n",
            "Running evaluation at iteration 457800...\n",
            "Iteration 457800, Train Loss: 4.196568489074707 Eval Loss: 3.8036936116218567\n",
            "Running evaluation at iteration 457900...\n",
            "Iteration 457900, Train Loss: 4.254382133483887 Eval Loss: 3.8053035712242127\n",
            "Running evaluation at iteration 458000...\n",
            "Iteration 458000, Train Loss: 4.098909378051758 Eval Loss: 3.756014747619629\n",
            "Running evaluation at iteration 458100...\n",
            "Iteration 458100, Train Loss: 4.188525676727295 Eval Loss: 3.7824017882347105\n",
            "Running evaluation at iteration 458200...\n",
            "Iteration 458200, Train Loss: 3.88261079788208 Eval Loss: 3.8205417251586913\n",
            "Running evaluation at iteration 458300...\n",
            "Iteration 458300, Train Loss: 3.617650270462036 Eval Loss: 3.783032476902008\n",
            "Running evaluation at iteration 458400...\n",
            "Iteration 458400, Train Loss: 3.488004684448242 Eval Loss: 3.9236777353286745\n",
            "Running evaluation at iteration 458500...\n",
            "Iteration 458500, Train Loss: 3.2254247665405273 Eval Loss: 3.7915472865104674\n",
            "Running evaluation at iteration 458600...\n",
            "Iteration 458600, Train Loss: 4.1245808601379395 Eval Loss: 3.785727288722992\n",
            "Running evaluation at iteration 458700...\n",
            "Iteration 458700, Train Loss: 3.934361457824707 Eval Loss: 3.829095368385315\n",
            "Running evaluation at iteration 458800...\n",
            "Iteration 458800, Train Loss: 2.645998954772949 Eval Loss: 3.7379306316375733\n",
            "Running evaluation at iteration 458900...\n",
            "Iteration 458900, Train Loss: 3.6998450756073 Eval Loss: 3.764193549156189\n",
            "Running evaluation at iteration 459000...\n",
            "Iteration 459000, Train Loss: 3.598139762878418 Eval Loss: 3.8685796594619752\n",
            "Running evaluation at iteration 459100...\n",
            "Iteration 459100, Train Loss: 3.203878879547119 Eval Loss: 3.781541492938995\n",
            "Running evaluation at iteration 459200...\n",
            "Iteration 459200, Train Loss: 4.137789249420166 Eval Loss: 3.818390545845032\n",
            "Running evaluation at iteration 459300...\n",
            "Iteration 459300, Train Loss: 4.304485321044922 Eval Loss: 3.728172481060028\n",
            "Running evaluation at iteration 459400...\n",
            "Iteration 459400, Train Loss: 3.7761733531951904 Eval Loss: 3.8176121401786802\n",
            "Running evaluation at iteration 459500...\n",
            "Iteration 459500, Train Loss: 3.7172441482543945 Eval Loss: 3.6854799008369445\n",
            "Running evaluation at iteration 459600...\n",
            "Iteration 459600, Train Loss: 4.400396823883057 Eval Loss: 3.7743794417381284\n",
            "Running evaluation at iteration 459700...\n",
            "Iteration 459700, Train Loss: 3.8334898948669434 Eval Loss: 3.8457307195663453\n",
            "Running evaluation at iteration 459800...\n",
            "Iteration 459800, Train Loss: 2.5095314979553223 Eval Loss: 3.7982838320732117\n",
            "Running evaluation at iteration 459900...\n",
            "Iteration 459900, Train Loss: 3.47625994682312 Eval Loss: 3.8403825092315675\n",
            "Running evaluation at iteration 460000...\n",
            "Iteration 460000, Train Loss: 4.523284912109375 Eval Loss: 3.785293617248535\n",
            "Running evaluation at iteration 460100...\n",
            "Iteration 460100, Train Loss: 3.907986879348755 Eval Loss: 3.8517123794555665\n",
            "Running evaluation at iteration 460200...\n",
            "Iteration 460200, Train Loss: 4.182643413543701 Eval Loss: 3.7181189346313475\n",
            "Running evaluation at iteration 460300...\n",
            "Iteration 460300, Train Loss: 3.8332738876342773 Eval Loss: 3.769746582508087\n",
            "Running evaluation at iteration 460400...\n",
            "Iteration 460400, Train Loss: 3.708948850631714 Eval Loss: 3.8083045411109926\n",
            "Running evaluation at iteration 460500...\n",
            "Iteration 460500, Train Loss: 3.957930326461792 Eval Loss: 3.7524534463882446\n",
            "Running evaluation at iteration 460600...\n",
            "Iteration 460600, Train Loss: 4.4820733070373535 Eval Loss: 3.819817669391632\n",
            "Running evaluation at iteration 460700...\n",
            "Iteration 460700, Train Loss: 4.109329700469971 Eval Loss: 3.8193443846702575\n",
            "Running evaluation at iteration 460800...\n",
            "Iteration 460800, Train Loss: 3.8538765907287598 Eval Loss: 3.8580659794807435\n",
            "Running evaluation at iteration 460900...\n",
            "Iteration 460900, Train Loss: 3.904597520828247 Eval Loss: 3.7550468492507934\n",
            "Running evaluation at iteration 461000...\n",
            "Iteration 461000, Train Loss: 3.902324914932251 Eval Loss: 3.7146472573280334\n",
            "Running evaluation at iteration 461100...\n",
            "Iteration 461100, Train Loss: 4.016244411468506 Eval Loss: 3.714506890773773\n",
            "Running evaluation at iteration 461200...\n",
            "Iteration 461200, Train Loss: 4.4381232261657715 Eval Loss: 3.771951575279236\n",
            "Running evaluation at iteration 461300...\n",
            "Iteration 461300, Train Loss: 3.4308128356933594 Eval Loss: 3.7760813856124877\n",
            "Running evaluation at iteration 461400...\n",
            "Iteration 461400, Train Loss: 3.6162402629852295 Eval Loss: 3.745919363498688\n",
            "Running evaluation at iteration 461500...\n",
            "Iteration 461500, Train Loss: 4.140323638916016 Eval Loss: 3.745476484298706\n",
            "Running evaluation at iteration 461600...\n",
            "Iteration 461600, Train Loss: 4.273846626281738 Eval Loss: 3.8377952432632445\n",
            "Running evaluation at iteration 461700...\n",
            "Iteration 461700, Train Loss: 3.930873394012451 Eval Loss: 3.8336398720741274\n",
            "Running evaluation at iteration 461800...\n",
            "Iteration 461800, Train Loss: 3.4072811603546143 Eval Loss: 3.8142595744132994\n",
            "Running evaluation at iteration 461900...\n",
            "Iteration 461900, Train Loss: 3.8107032775878906 Eval Loss: 3.708334729671478\n",
            "Running evaluation at iteration 462000...\n",
            "Iteration 462000, Train Loss: 3.676976442337036 Eval Loss: 3.8673270535469055\n",
            "Running evaluation at iteration 462100...\n",
            "Iteration 462100, Train Loss: 3.216341733932495 Eval Loss: 3.7741319799423216\n",
            "Running evaluation at iteration 462200...\n",
            "Iteration 462200, Train Loss: 3.6665685176849365 Eval Loss: 3.6837983345985412\n",
            "Running evaluation at iteration 462300...\n",
            "Iteration 462300, Train Loss: 4.691309452056885 Eval Loss: 3.851819622516632\n",
            "Running evaluation at iteration 462400...\n",
            "Iteration 462400, Train Loss: 2.9571826457977295 Eval Loss: 3.763700053691864\n",
            "Running evaluation at iteration 462500...\n",
            "Iteration 462500, Train Loss: 3.524965524673462 Eval Loss: 3.7739207696914674\n",
            "Running evaluation at iteration 462600...\n",
            "Iteration 462600, Train Loss: 3.861360549926758 Eval Loss: 3.7933765959739687\n",
            "Running evaluation at iteration 462700...\n",
            "Iteration 462700, Train Loss: 3.8838024139404297 Eval Loss: 3.8327704191207888\n",
            "Running evaluation at iteration 462800...\n",
            "Iteration 462800, Train Loss: 4.05250358581543 Eval Loss: 3.7491556644439696\n",
            "Running evaluation at iteration 462900...\n",
            "Iteration 462900, Train Loss: 3.542022705078125 Eval Loss: 3.7773118901252745\n",
            "Running evaluation at iteration 463000...\n",
            "Iteration 463000, Train Loss: 4.302592754364014 Eval Loss: 3.8397842764854433\n",
            "Running evaluation at iteration 463100...\n",
            "Iteration 463100, Train Loss: 3.927961587905884 Eval Loss: 3.7259439754486086\n",
            "Running evaluation at iteration 463200...\n",
            "Iteration 463200, Train Loss: 2.8307976722717285 Eval Loss: 3.7681234788894655\n",
            "Running evaluation at iteration 463300...\n",
            "Iteration 463300, Train Loss: 2.9666826725006104 Eval Loss: 3.779172646999359\n",
            "Running evaluation at iteration 463400...\n",
            "Iteration 463400, Train Loss: 3.6718311309814453 Eval Loss: 3.7622573375701904\n",
            "Running evaluation at iteration 463500...\n",
            "Iteration 463500, Train Loss: 4.444551467895508 Eval Loss: 3.748359706401825\n",
            "Running evaluation at iteration 463600...\n",
            "Iteration 463600, Train Loss: 3.6530895233154297 Eval Loss: 3.8289542388916016\n",
            "Running evaluation at iteration 463700...\n",
            "Iteration 463700, Train Loss: 3.6534736156463623 Eval Loss: 3.9052900648117066\n",
            "Running evaluation at iteration 463800...\n",
            "Iteration 463800, Train Loss: 3.9740359783172607 Eval Loss: 3.8366804933547973\n",
            "Running evaluation at iteration 463900...\n",
            "Iteration 463900, Train Loss: 4.383554458618164 Eval Loss: 3.722086033821106\n",
            "Running evaluation at iteration 464000...\n",
            "Iteration 464000, Train Loss: 3.7777507305145264 Eval Loss: 3.821227180957794\n",
            "Running evaluation at iteration 464100...\n",
            "Iteration 464100, Train Loss: 3.940687894821167 Eval Loss: 3.779435167312622\n",
            "Running evaluation at iteration 464200...\n",
            "Iteration 464200, Train Loss: 3.996457576751709 Eval Loss: 3.8372528624534605\n",
            "Running evaluation at iteration 464300...\n",
            "Iteration 464300, Train Loss: 4.496184349060059 Eval Loss: 3.797600212097168\n",
            "Running evaluation at iteration 464400...\n",
            "Iteration 464400, Train Loss: 4.026376724243164 Eval Loss: 3.8297810220718382\n",
            "Running evaluation at iteration 464500...\n",
            "Iteration 464500, Train Loss: 4.119660377502441 Eval Loss: 3.6814730381965637\n",
            "Running evaluation at iteration 464600...\n",
            "Iteration 464600, Train Loss: 3.5858113765716553 Eval Loss: 3.8169070219993593\n",
            "Running evaluation at iteration 464700...\n",
            "Iteration 464700, Train Loss: 3.4949421882629395 Eval Loss: 3.6851480388641358\n",
            "Running evaluation at iteration 464800...\n",
            "Iteration 464800, Train Loss: 3.087183713912964 Eval Loss: 3.778608212471008\n",
            "Running evaluation at iteration 464900...\n",
            "Iteration 464900, Train Loss: 4.7952399253845215 Eval Loss: 3.7657406902313233\n",
            "Running evaluation at iteration 465000...\n",
            "Iteration 465000, Train Loss: 4.5331711769104 Eval Loss: 3.749860818386078\n",
            "Running evaluation at iteration 465100...\n",
            "Iteration 465100, Train Loss: 3.254683017730713 Eval Loss: 3.7979564881324768\n",
            "Running evaluation at iteration 465200...\n",
            "Iteration 465200, Train Loss: 3.7342658042907715 Eval Loss: 3.7166184186935425\n",
            "Running evaluation at iteration 465300...\n",
            "Iteration 465300, Train Loss: 3.537858009338379 Eval Loss: 3.788387858867645\n",
            "Running evaluation at iteration 465400...\n",
            "Iteration 465400, Train Loss: 3.9129645824432373 Eval Loss: 3.7477365589141844\n",
            "Running evaluation at iteration 465500...\n",
            "Iteration 465500, Train Loss: 4.060853481292725 Eval Loss: 3.7739758491516113\n",
            "Running evaluation at iteration 465600...\n",
            "Iteration 465600, Train Loss: 5.034669876098633 Eval Loss: 3.8133520102500915\n",
            "Running evaluation at iteration 465700...\n",
            "Iteration 465700, Train Loss: 4.017974853515625 Eval Loss: 3.8313701820373534\n",
            "Running evaluation at iteration 465800...\n",
            "Iteration 465800, Train Loss: 3.925981044769287 Eval Loss: 3.906819710731506\n",
            "Running evaluation at iteration 465900...\n",
            "Iteration 465900, Train Loss: 3.6797242164611816 Eval Loss: 3.7534783625602723\n",
            "Running evaluation at iteration 466000...\n",
            "Iteration 466000, Train Loss: 3.1860220432281494 Eval Loss: 3.9035662317276003\n",
            "Running evaluation at iteration 466100...\n",
            "Iteration 466100, Train Loss: 4.898499011993408 Eval Loss: 3.8126916909217834\n",
            "Running evaluation at iteration 466200...\n",
            "Iteration 466200, Train Loss: 3.5352401733398438 Eval Loss: 3.9394466876983643\n",
            "Running evaluation at iteration 466300...\n",
            "Iteration 466300, Train Loss: 4.721819877624512 Eval Loss: 3.855838134288788\n",
            "Running evaluation at iteration 466400...\n",
            "Iteration 466400, Train Loss: 4.052917957305908 Eval Loss: 3.8072888088226318\n",
            "Running evaluation at iteration 466500...\n",
            "Iteration 466500, Train Loss: 3.9477734565734863 Eval Loss: 3.8009288120269775\n",
            "Running evaluation at iteration 466600...\n",
            "Iteration 466600, Train Loss: 3.9296135902404785 Eval Loss: 3.7172358083724975\n",
            "Running evaluation at iteration 466700...\n",
            "Iteration 466700, Train Loss: 3.2223002910614014 Eval Loss: 3.7493371629714964\n",
            "Running evaluation at iteration 466800...\n",
            "Iteration 466800, Train Loss: 3.753396511077881 Eval Loss: 3.8538979363441466\n",
            "Running evaluation at iteration 466900...\n",
            "Iteration 466900, Train Loss: 4.091233253479004 Eval Loss: 3.7138676977157594\n",
            "Running evaluation at iteration 467000...\n",
            "Iteration 467000, Train Loss: 3.0267834663391113 Eval Loss: 3.808686833381653\n",
            "Running evaluation at iteration 467100...\n",
            "Iteration 467100, Train Loss: 3.6927716732025146 Eval Loss: 3.8043133020401\n",
            "Running evaluation at iteration 467200...\n",
            "Iteration 467200, Train Loss: 4.233077049255371 Eval Loss: 3.8001101922988894\n",
            "Running evaluation at iteration 467300...\n",
            "Iteration 467300, Train Loss: 3.710089683532715 Eval Loss: 3.753513021469116\n",
            "Running evaluation at iteration 467400...\n",
            "Iteration 467400, Train Loss: 3.782461404800415 Eval Loss: 3.7757178711891175\n",
            "Running evaluation at iteration 467500...\n",
            "Iteration 467500, Train Loss: 3.5529701709747314 Eval Loss: 3.838227753639221\n",
            "Running evaluation at iteration 467600...\n",
            "Iteration 467600, Train Loss: 4.2888264656066895 Eval Loss: 3.8333469367027284\n",
            "Running evaluation at iteration 467700...\n",
            "Iteration 467700, Train Loss: 3.4608325958251953 Eval Loss: 3.8332292532920835\n",
            "Running evaluation at iteration 467800...\n",
            "Iteration 467800, Train Loss: 4.296123027801514 Eval Loss: 3.8427735900878908\n",
            "Running evaluation at iteration 467900...\n",
            "Iteration 467900, Train Loss: 3.398681402206421 Eval Loss: 3.8060554027557374\n",
            "Running evaluation at iteration 468000...\n",
            "Iteration 468000, Train Loss: 3.886658191680908 Eval Loss: 3.734887356758118\n",
            "Running evaluation at iteration 468100...\n",
            "Iteration 468100, Train Loss: 3.644078493118286 Eval Loss: 3.774701807498932\n",
            "Running evaluation at iteration 468200...\n",
            "Iteration 468200, Train Loss: 4.3535308837890625 Eval Loss: 3.8387752079963686\n",
            "Running evaluation at iteration 468300...\n",
            "Iteration 468300, Train Loss: 5.178501129150391 Eval Loss: 3.84351615190506\n",
            "Running evaluation at iteration 468400...\n",
            "Iteration 468400, Train Loss: 3.151461601257324 Eval Loss: 3.7066264533996582\n",
            "Running evaluation at iteration 468500...\n",
            "Iteration 468500, Train Loss: 2.746732473373413 Eval Loss: 3.7314102911949156\n",
            "Running evaluation at iteration 468600...\n",
            "Iteration 468600, Train Loss: 3.927356004714966 Eval Loss: 3.8330077266693117\n",
            "Running evaluation at iteration 468700...\n",
            "Iteration 468700, Train Loss: 4.1765289306640625 Eval Loss: 3.8308686947822572\n",
            "Running evaluation at iteration 468800...\n",
            "Iteration 468800, Train Loss: 3.9721267223358154 Eval Loss: 3.7992621731758116\n",
            "Running evaluation at iteration 468900...\n",
            "Iteration 468900, Train Loss: 3.356365203857422 Eval Loss: 3.7125119590759277\n",
            "Running evaluation at iteration 469000...\n",
            "Iteration 469000, Train Loss: 4.688770294189453 Eval Loss: 3.7748931646347046\n",
            "Running evaluation at iteration 469100...\n",
            "Iteration 469100, Train Loss: 4.4282307624816895 Eval Loss: 3.792635970115662\n",
            "Running evaluation at iteration 469200...\n",
            "Iteration 469200, Train Loss: 4.351900577545166 Eval Loss: 3.8650624418258666\n",
            "Running evaluation at iteration 469300...\n",
            "Iteration 469300, Train Loss: 3.62375807762146 Eval Loss: 3.808668038845062\n",
            "Running evaluation at iteration 469400...\n",
            "Iteration 469400, Train Loss: 3.719475269317627 Eval Loss: 3.7870273566246033\n",
            "Running evaluation at iteration 469500...\n",
            "Iteration 469500, Train Loss: 4.674108505249023 Eval Loss: 3.795454754829407\n",
            "Running evaluation at iteration 469600...\n",
            "Iteration 469600, Train Loss: 4.137646198272705 Eval Loss: 3.7109097743034365\n",
            "Running evaluation at iteration 469700...\n",
            "Iteration 469700, Train Loss: 3.4818458557128906 Eval Loss: 3.8018611812591554\n",
            "Running evaluation at iteration 469800...\n",
            "Iteration 469800, Train Loss: 3.405691623687744 Eval Loss: 3.758063082695007\n",
            "Running evaluation at iteration 469900...\n",
            "Iteration 469900, Train Loss: 3.5564520359039307 Eval Loss: 3.8147085738182067\n",
            "Running evaluation at iteration 470000...\n",
            "Iteration 470000, Train Loss: 3.720705986022949 Eval Loss: 3.792770128250122\n",
            "Running evaluation at iteration 470100...\n",
            "Iteration 470100, Train Loss: 4.0612592697143555 Eval Loss: 3.82648654460907\n",
            "Running evaluation at iteration 470200...\n",
            "Iteration 470200, Train Loss: 3.9740686416625977 Eval Loss: 3.7753717207908633\n",
            "Running evaluation at iteration 470300...\n",
            "Iteration 470300, Train Loss: 3.2884278297424316 Eval Loss: 3.8369824576377867\n",
            "Running evaluation at iteration 470400...\n",
            "Iteration 470400, Train Loss: 4.5538740158081055 Eval Loss: 3.7574462890625\n",
            "Running evaluation at iteration 470500...\n",
            "Iteration 470500, Train Loss: 3.1356112957000732 Eval Loss: 3.746509301662445\n",
            "Running evaluation at iteration 470600...\n",
            "Iteration 470600, Train Loss: 4.437526226043701 Eval Loss: 3.766141531467438\n",
            "Running evaluation at iteration 470700...\n",
            "Iteration 470700, Train Loss: 2.741809606552124 Eval Loss: 3.8270048880577088\n",
            "Running evaluation at iteration 470800...\n",
            "Iteration 470800, Train Loss: 4.00670862197876 Eval Loss: 3.836478624343872\n",
            "Running evaluation at iteration 470900...\n",
            "Iteration 470900, Train Loss: 3.29632830619812 Eval Loss: 3.864035954475403\n",
            "Running evaluation at iteration 471000...\n",
            "Iteration 471000, Train Loss: 3.7613115310668945 Eval Loss: 3.7524379229545595\n",
            "Running evaluation at iteration 471100...\n",
            "Iteration 471100, Train Loss: 4.513742923736572 Eval Loss: 3.8339422845840456\n",
            "Running evaluation at iteration 471200...\n",
            "Iteration 471200, Train Loss: 4.039575576782227 Eval Loss: 3.7570155835151673\n",
            "Running evaluation at iteration 471300...\n",
            "Iteration 471300, Train Loss: 3.0772743225097656 Eval Loss: 3.793709433078766\n",
            "Running evaluation at iteration 471400...\n",
            "Iteration 471400, Train Loss: 3.916785717010498 Eval Loss: 3.843650133609772\n",
            "Running evaluation at iteration 471500...\n",
            "Iteration 471500, Train Loss: 4.4504008293151855 Eval Loss: 3.7943691754341127\n",
            "Running evaluation at iteration 471600...\n",
            "Iteration 471600, Train Loss: 4.087617874145508 Eval Loss: 3.9114089441299438\n",
            "Running evaluation at iteration 471700...\n",
            "Iteration 471700, Train Loss: 3.923116445541382 Eval Loss: 3.790897719860077\n",
            "Running evaluation at iteration 471800...\n",
            "Iteration 471800, Train Loss: 4.563230037689209 Eval Loss: 3.7279742527008057\n",
            "Running evaluation at iteration 471900...\n",
            "Iteration 471900, Train Loss: 4.537540912628174 Eval Loss: 3.784955062866211\n",
            "Running evaluation at iteration 472000...\n",
            "Iteration 472000, Train Loss: 3.4713757038116455 Eval Loss: 3.8230945372581484\n",
            "Running evaluation at iteration 472100...\n",
            "Iteration 472100, Train Loss: 4.967000484466553 Eval Loss: 3.778816967010498\n",
            "Running evaluation at iteration 472200...\n",
            "Iteration 472200, Train Loss: 4.071874618530273 Eval Loss: 3.7587426328659057\n",
            "Running evaluation at iteration 472300...\n",
            "Iteration 472300, Train Loss: 4.494083881378174 Eval Loss: 3.750586414337158\n",
            "Running evaluation at iteration 472400...\n",
            "Iteration 472400, Train Loss: 3.7766013145446777 Eval Loss: 3.8001507568359374\n",
            "Running evaluation at iteration 472500...\n",
            "Iteration 472500, Train Loss: 3.480325698852539 Eval Loss: 3.8766925525665283\n",
            "Running evaluation at iteration 472600...\n",
            "Iteration 472600, Train Loss: 3.8087422847747803 Eval Loss: 3.842158737182617\n",
            "Running evaluation at iteration 472700...\n",
            "Iteration 472700, Train Loss: 3.8898162841796875 Eval Loss: 3.8223907446861265\n",
            "Running evaluation at iteration 472800...\n",
            "Iteration 472800, Train Loss: 4.014501094818115 Eval Loss: 3.8214267563819884\n",
            "Running evaluation at iteration 472900...\n",
            "Iteration 472900, Train Loss: 4.380655288696289 Eval Loss: 3.778647541999817\n",
            "Running evaluation at iteration 473000...\n",
            "Iteration 473000, Train Loss: 4.061485767364502 Eval Loss: 3.7979067873954775\n",
            "Running evaluation at iteration 473100...\n",
            "Iteration 473100, Train Loss: 4.712841510772705 Eval Loss: 3.7725934743881226\n",
            "Running evaluation at iteration 473200...\n",
            "Iteration 473200, Train Loss: 3.766880512237549 Eval Loss: 3.881692712306976\n",
            "Running evaluation at iteration 473300...\n",
            "Iteration 473300, Train Loss: 4.373336315155029 Eval Loss: 3.798530035018921\n",
            "Running evaluation at iteration 473400...\n",
            "Iteration 473400, Train Loss: 4.241157531738281 Eval Loss: 3.812745759487152\n",
            "Running evaluation at iteration 473500...\n",
            "Iteration 473500, Train Loss: 3.3698909282684326 Eval Loss: 3.7591136264801026\n",
            "Running evaluation at iteration 473600...\n",
            "Iteration 473600, Train Loss: 4.16154670715332 Eval Loss: 3.80702525138855\n",
            "Running evaluation at iteration 473700...\n",
            "Iteration 473700, Train Loss: 3.827195167541504 Eval Loss: 3.7790910053253173\n",
            "Running evaluation at iteration 473800...\n",
            "Iteration 473800, Train Loss: 4.477703094482422 Eval Loss: 3.8017518854141237\n",
            "Running evaluation at iteration 473900...\n",
            "Iteration 473900, Train Loss: 3.785165786743164 Eval Loss: 3.773933231830597\n",
            "Running evaluation at iteration 474000...\n",
            "Iteration 474000, Train Loss: 4.322485446929932 Eval Loss: 3.8835683131217955\n",
            "Running evaluation at iteration 474100...\n",
            "Iteration 474100, Train Loss: 3.3839213848114014 Eval Loss: 3.8483753061294554\n",
            "Running evaluation at iteration 474200...\n",
            "Iteration 474200, Train Loss: 3.2896728515625 Eval Loss: 3.870848217010498\n",
            "Running evaluation at iteration 474300...\n",
            "Iteration 474300, Train Loss: 3.9731297492980957 Eval Loss: 3.7633377075195313\n",
            "Running evaluation at iteration 474400...\n",
            "Iteration 474400, Train Loss: 3.859694004058838 Eval Loss: 3.7738062834739683\n",
            "Running evaluation at iteration 474500...\n",
            "Iteration 474500, Train Loss: 3.5849947929382324 Eval Loss: 3.7701297402381897\n",
            "Running evaluation at iteration 474600...\n",
            "Iteration 474600, Train Loss: 3.4529342651367188 Eval Loss: 3.745452980995178\n",
            "Running evaluation at iteration 474700...\n",
            "Iteration 474700, Train Loss: 3.822038412094116 Eval Loss: 3.7705160665512083\n",
            "Running evaluation at iteration 474800...\n",
            "Iteration 474800, Train Loss: 4.286079406738281 Eval Loss: 3.841636998653412\n",
            "Running evaluation at iteration 474900...\n",
            "Iteration 474900, Train Loss: 3.600320339202881 Eval Loss: 3.8650930738449096\n",
            "Running evaluation at iteration 475000...\n",
            "Iteration 475000, Train Loss: 3.4480369091033936 Eval Loss: 3.732317147254944\n",
            "Running evaluation at iteration 475100...\n",
            "Iteration 475100, Train Loss: 3.6270031929016113 Eval Loss: 3.8392724919319154\n",
            "Running evaluation at iteration 475200...\n",
            "Iteration 475200, Train Loss: 3.8392374515533447 Eval Loss: 3.768344883918762\n",
            "Running evaluation at iteration 475300...\n",
            "Iteration 475300, Train Loss: 3.969650983810425 Eval Loss: 3.747878141403198\n",
            "Running evaluation at iteration 475400...\n",
            "Iteration 475400, Train Loss: 4.00899076461792 Eval Loss: 3.7817106509208678\n",
            "Running evaluation at iteration 475500...\n",
            "Iteration 475500, Train Loss: 4.282079219818115 Eval Loss: 3.8767002391815186\n",
            "Running evaluation at iteration 475600...\n",
            "Iteration 475600, Train Loss: 4.153829097747803 Eval Loss: 3.853005566596985\n",
            "Running evaluation at iteration 475700...\n",
            "Iteration 475700, Train Loss: 3.696399688720703 Eval Loss: 3.860019726753235\n",
            "Running evaluation at iteration 475800...\n",
            "Iteration 475800, Train Loss: 4.396430015563965 Eval Loss: 3.770281856060028\n",
            "Running evaluation at iteration 475900...\n",
            "Iteration 475900, Train Loss: 3.200335741043091 Eval Loss: 3.842451455593109\n",
            "Running evaluation at iteration 476000...\n",
            "Iteration 476000, Train Loss: 3.4846880435943604 Eval Loss: 3.865326476097107\n",
            "Running evaluation at iteration 476100...\n",
            "Iteration 476100, Train Loss: 3.624424934387207 Eval Loss: 3.7885801005363464\n",
            "Running evaluation at iteration 476200...\n",
            "Iteration 476200, Train Loss: 3.6767454147338867 Eval Loss: 3.798657114505768\n",
            "Running evaluation at iteration 476300...\n",
            "Iteration 476300, Train Loss: 4.0474772453308105 Eval Loss: 3.7748932671546935\n",
            "Running evaluation at iteration 476400...\n",
            "Iteration 476400, Train Loss: 4.194957733154297 Eval Loss: 3.8387515139579773\n",
            "Running evaluation at iteration 476500...\n",
            "Iteration 476500, Train Loss: 4.515239238739014 Eval Loss: 3.8337947392463683\n",
            "Running evaluation at iteration 476600...\n",
            "Iteration 476600, Train Loss: 3.1869614124298096 Eval Loss: 3.876407780647278\n",
            "Running evaluation at iteration 476700...\n",
            "Iteration 476700, Train Loss: 3.666487455368042 Eval Loss: 3.8987920570373533\n",
            "Running evaluation at iteration 476800...\n",
            "Iteration 476800, Train Loss: 4.06320333480835 Eval Loss: 3.7333417081832887\n",
            "Running evaluation at iteration 476900...\n",
            "Iteration 476900, Train Loss: 4.12767219543457 Eval Loss: 3.7525600528717042\n",
            "Running evaluation at iteration 477000...\n",
            "Iteration 477000, Train Loss: 3.5270252227783203 Eval Loss: 3.6906954860687256\n",
            "Running evaluation at iteration 477100...\n",
            "Iteration 477100, Train Loss: 3.953186511993408 Eval Loss: 3.8411076617240907\n",
            "Running evaluation at iteration 477200...\n",
            "Iteration 477200, Train Loss: 3.2814245223999023 Eval Loss: 3.8500587797164916\n",
            "Running evaluation at iteration 477300...\n",
            "Iteration 477300, Train Loss: 3.954740524291992 Eval Loss: 3.7312966346740724\n",
            "Running evaluation at iteration 477400...\n",
            "Iteration 477400, Train Loss: 4.244835376739502 Eval Loss: 3.8124019718170166\n",
            "Running evaluation at iteration 477500...\n",
            "Iteration 477500, Train Loss: 4.560402870178223 Eval Loss: 3.7784307527542116\n",
            "Running evaluation at iteration 477600...\n",
            "Iteration 477600, Train Loss: 3.1953253746032715 Eval Loss: 3.89914165019989\n",
            "Running evaluation at iteration 477700...\n",
            "Iteration 477700, Train Loss: 3.8339431285858154 Eval Loss: 3.8458138608932497\n",
            "Running evaluation at iteration 477800...\n",
            "Iteration 477800, Train Loss: 3.72222900390625 Eval Loss: 3.7450492787361145\n",
            "Running evaluation at iteration 477900...\n",
            "Iteration 477900, Train Loss: 3.826850414276123 Eval Loss: 3.886290829181671\n",
            "Running evaluation at iteration 478000...\n",
            "Iteration 478000, Train Loss: 3.1472718715667725 Eval Loss: 3.710383837223053\n",
            "Running evaluation at iteration 478100...\n",
            "Iteration 478100, Train Loss: 4.0555219650268555 Eval Loss: 3.7864298248291015\n",
            "Running evaluation at iteration 478200...\n",
            "Iteration 478200, Train Loss: 4.194639205932617 Eval Loss: 3.849574010372162\n",
            "Running evaluation at iteration 478300...\n",
            "Iteration 478300, Train Loss: 3.1360576152801514 Eval Loss: 3.7618471431732177\n",
            "Running evaluation at iteration 478400...\n",
            "Iteration 478400, Train Loss: 3.7381134033203125 Eval Loss: 3.8154307794570923\n",
            "Running evaluation at iteration 478500...\n",
            "Iteration 478500, Train Loss: 3.2793939113616943 Eval Loss: 3.834209403991699\n",
            "Running evaluation at iteration 478600...\n",
            "Iteration 478600, Train Loss: 4.519957542419434 Eval Loss: 3.733306231498718\n",
            "Running evaluation at iteration 478700...\n",
            "Iteration 478700, Train Loss: 3.6407384872436523 Eval Loss: 3.7619691920280456\n",
            "Running evaluation at iteration 478800...\n",
            "Iteration 478800, Train Loss: 3.5485734939575195 Eval Loss: 3.765182957649231\n",
            "Running evaluation at iteration 478900...\n",
            "Iteration 478900, Train Loss: 4.336084365844727 Eval Loss: 3.7129630422592164\n",
            "Running evaluation at iteration 479000...\n",
            "Iteration 479000, Train Loss: 4.316304683685303 Eval Loss: 3.807691605091095\n",
            "Running evaluation at iteration 479100...\n",
            "Iteration 479100, Train Loss: 4.503801345825195 Eval Loss: 3.8492848467826843\n",
            "Running evaluation at iteration 479200...\n",
            "Iteration 479200, Train Loss: 3.467398166656494 Eval Loss: 3.8122737908363344\n",
            "Running evaluation at iteration 479300...\n",
            "Iteration 479300, Train Loss: 3.4544944763183594 Eval Loss: 3.7721953344345094\n",
            "Running evaluation at iteration 479400...\n",
            "Iteration 479400, Train Loss: 3.8309640884399414 Eval Loss: 3.815264084339142\n",
            "Running evaluation at iteration 479500...\n",
            "Iteration 479500, Train Loss: 3.574016571044922 Eval Loss: 3.8574488162994385\n",
            "Running evaluation at iteration 479600...\n",
            "Iteration 479600, Train Loss: 4.25718879699707 Eval Loss: 3.8050794649124144\n",
            "Running evaluation at iteration 479700...\n",
            "Iteration 479700, Train Loss: 4.006814002990723 Eval Loss: 3.76898264169693\n",
            "Running evaluation at iteration 479800...\n",
            "Iteration 479800, Train Loss: 3.3582003116607666 Eval Loss: 3.8145363974571227\n",
            "Running evaluation at iteration 479900...\n",
            "Iteration 479900, Train Loss: 4.2407426834106445 Eval Loss: 3.8229306292533876\n",
            "Running evaluation at iteration 480000...\n",
            "Iteration 480000, Train Loss: 4.318090915679932 Eval Loss: 3.758434548377991\n",
            "Running evaluation at iteration 480100...\n",
            "Iteration 480100, Train Loss: 3.232828140258789 Eval Loss: 3.8479433250427246\n",
            "Running evaluation at iteration 480200...\n",
            "Iteration 480200, Train Loss: 3.6815810203552246 Eval Loss: 3.819177718162537\n",
            "Running evaluation at iteration 480300...\n",
            "Iteration 480300, Train Loss: 3.446657657623291 Eval Loss: 3.808942656517029\n",
            "Running evaluation at iteration 480400...\n",
            "Iteration 480400, Train Loss: 3.473963737487793 Eval Loss: 3.74009072303772\n",
            "Running evaluation at iteration 480500...\n",
            "Iteration 480500, Train Loss: 3.825129508972168 Eval Loss: 3.8147980117797853\n",
            "Running evaluation at iteration 480600...\n",
            "Iteration 480600, Train Loss: 4.011651515960693 Eval Loss: 3.875601465702057\n",
            "Running evaluation at iteration 480700...\n",
            "Iteration 480700, Train Loss: 3.480219602584839 Eval Loss: 3.791587450504303\n",
            "Running evaluation at iteration 480800...\n",
            "Iteration 480800, Train Loss: 4.4697136878967285 Eval Loss: 3.794704911708832\n",
            "Running evaluation at iteration 480900...\n",
            "Iteration 480900, Train Loss: 3.387760639190674 Eval Loss: 3.777919075489044\n",
            "Running evaluation at iteration 481000...\n",
            "Iteration 481000, Train Loss: 3.4109323024749756 Eval Loss: 3.8249702715873717\n",
            "Running evaluation at iteration 481100...\n",
            "Iteration 481100, Train Loss: 3.5789191722869873 Eval Loss: 3.853950388431549\n",
            "Running evaluation at iteration 481200...\n",
            "Iteration 481200, Train Loss: 4.743325233459473 Eval Loss: 3.7715948176383973\n",
            "Running evaluation at iteration 481300...\n",
            "Iteration 481300, Train Loss: 4.119165420532227 Eval Loss: 3.780801918506622\n",
            "Running evaluation at iteration 481400...\n",
            "Iteration 481400, Train Loss: 4.1759233474731445 Eval Loss: 3.7696638393402098\n",
            "Running evaluation at iteration 481500...\n",
            "Iteration 481500, Train Loss: 4.1761064529418945 Eval Loss: 3.79039843082428\n",
            "Running evaluation at iteration 481600...\n",
            "Iteration 481600, Train Loss: 4.30316686630249 Eval Loss: 3.741135175228119\n",
            "Running evaluation at iteration 481700...\n",
            "Iteration 481700, Train Loss: 3.0194284915924072 Eval Loss: 3.7279215097427367\n",
            "Running evaluation at iteration 481800...\n",
            "Iteration 481800, Train Loss: 3.8013851642608643 Eval Loss: 3.862291388511658\n",
            "Running evaluation at iteration 481900...\n",
            "Iteration 481900, Train Loss: 3.893540620803833 Eval Loss: 3.8510651350021363\n",
            "Running evaluation at iteration 482000...\n",
            "Iteration 482000, Train Loss: 3.366103172302246 Eval Loss: 3.9204828667640688\n",
            "Running evaluation at iteration 482100...\n",
            "Iteration 482100, Train Loss: 3.749502658843994 Eval Loss: 3.7867600226402285\n",
            "Running evaluation at iteration 482200...\n",
            "Iteration 482200, Train Loss: 4.136815071105957 Eval Loss: 3.858983654975891\n",
            "Running evaluation at iteration 482300...\n",
            "Iteration 482300, Train Loss: 4.3493804931640625 Eval Loss: 3.7645507431030274\n",
            "Running evaluation at iteration 482400...\n",
            "Iteration 482400, Train Loss: 2.998518705368042 Eval Loss: 3.7218381690979006\n",
            "Running evaluation at iteration 482500...\n",
            "Iteration 482500, Train Loss: 3.404601573944092 Eval Loss: 3.772542791366577\n",
            "Running evaluation at iteration 482600...\n",
            "Iteration 482600, Train Loss: 4.035866737365723 Eval Loss: 3.7560108637809755\n",
            "Running evaluation at iteration 482700...\n",
            "Iteration 482700, Train Loss: 3.8043065071105957 Eval Loss: 3.8622723960876466\n",
            "Running evaluation at iteration 482800...\n",
            "Iteration 482800, Train Loss: 3.703361749649048 Eval Loss: 3.8027501678466797\n",
            "Running evaluation at iteration 482900...\n",
            "Iteration 482900, Train Loss: 3.801790475845337 Eval Loss: 3.747174255847931\n",
            "Running evaluation at iteration 483000...\n",
            "Iteration 483000, Train Loss: 3.4583539962768555 Eval Loss: 3.7651809287071227\n",
            "Running evaluation at iteration 483100...\n",
            "Iteration 483100, Train Loss: 4.170256614685059 Eval Loss: 3.756363456249237\n",
            "Running evaluation at iteration 483200...\n",
            "Iteration 483200, Train Loss: 3.479771852493286 Eval Loss: 3.9143366980552674\n",
            "Running evaluation at iteration 483300...\n",
            "Iteration 483300, Train Loss: 3.8353421688079834 Eval Loss: 3.783373908996582\n",
            "Running evaluation at iteration 483400...\n",
            "Iteration 483400, Train Loss: 3.7063517570495605 Eval Loss: 3.8851894474029542\n",
            "Running evaluation at iteration 483500...\n",
            "Iteration 483500, Train Loss: 3.5138862133026123 Eval Loss: 3.7768902444839476\n",
            "Running evaluation at iteration 483600...\n",
            "Iteration 483600, Train Loss: 3.340665340423584 Eval Loss: 3.7948485612869263\n",
            "Running evaluation at iteration 483700...\n",
            "Iteration 483700, Train Loss: 3.4117746353149414 Eval Loss: 3.7429177570343017\n",
            "Running evaluation at iteration 483800...\n",
            "Iteration 483800, Train Loss: 4.0473408699035645 Eval Loss: 3.7559077954292297\n",
            "Running evaluation at iteration 483900...\n",
            "Iteration 483900, Train Loss: 3.4973862171173096 Eval Loss: 3.7512244033813475\n",
            "Running evaluation at iteration 484000...\n",
            "Iteration 484000, Train Loss: 4.384375095367432 Eval Loss: 3.752416923046112\n",
            "Running evaluation at iteration 484100...\n",
            "Iteration 484100, Train Loss: 3.494419574737549 Eval Loss: 3.7687086915969847\n",
            "Running evaluation at iteration 484200...\n",
            "Iteration 484200, Train Loss: 3.9375991821289062 Eval Loss: 3.8121636843681337\n",
            "Running evaluation at iteration 484300...\n",
            "Iteration 484300, Train Loss: 3.680265426635742 Eval Loss: 3.793390545845032\n",
            "Running evaluation at iteration 484400...\n",
            "Iteration 484400, Train Loss: 4.113774299621582 Eval Loss: 3.748115749359131\n",
            "Running evaluation at iteration 484500...\n",
            "Iteration 484500, Train Loss: 4.137650489807129 Eval Loss: 3.7868621516227723\n",
            "Running evaluation at iteration 484600...\n",
            "Iteration 484600, Train Loss: 3.8884332180023193 Eval Loss: 3.8834342765808105\n",
            "Running evaluation at iteration 484700...\n",
            "Iteration 484700, Train Loss: 4.637331962585449 Eval Loss: 3.881401653289795\n",
            "Running evaluation at iteration 484800...\n",
            "Iteration 484800, Train Loss: 5.314676284790039 Eval Loss: 3.747895984649658\n",
            "Running evaluation at iteration 484900...\n",
            "Iteration 484900, Train Loss: 3.4349520206451416 Eval Loss: 3.7250748920440673\n",
            "Running evaluation at iteration 485000...\n",
            "Iteration 485000, Train Loss: 3.234388828277588 Eval Loss: 3.6994671535491945\n",
            "Running evaluation at iteration 485100...\n",
            "Iteration 485100, Train Loss: 3.7471835613250732 Eval Loss: 3.791681547164917\n",
            "Running evaluation at iteration 485200...\n",
            "Iteration 485200, Train Loss: 3.705355644226074 Eval Loss: 3.838365671634674\n",
            "Running evaluation at iteration 485300...\n",
            "Iteration 485300, Train Loss: 3.8941404819488525 Eval Loss: 3.782991898059845\n",
            "Running evaluation at iteration 485400...\n",
            "Iteration 485400, Train Loss: 3.8479275703430176 Eval Loss: 3.8174513220787047\n",
            "Running evaluation at iteration 485500...\n",
            "Iteration 485500, Train Loss: 3.4973413944244385 Eval Loss: 3.8115017223358154\n",
            "Running evaluation at iteration 485600...\n",
            "Iteration 485600, Train Loss: 4.492222785949707 Eval Loss: 3.7881044435501097\n",
            "Running evaluation at iteration 485700...\n",
            "Iteration 485700, Train Loss: 3.6857123374938965 Eval Loss: 3.864048249721527\n",
            "Running evaluation at iteration 485800...\n",
            "Iteration 485800, Train Loss: 3.375579833984375 Eval Loss: 3.857756986618042\n",
            "Running evaluation at iteration 485900...\n",
            "Iteration 485900, Train Loss: 4.090122222900391 Eval Loss: 3.7543862152099607\n",
            "Running evaluation at iteration 486000...\n",
            "Iteration 486000, Train Loss: 4.522099494934082 Eval Loss: 3.8165163660049437\n",
            "Running evaluation at iteration 486100...\n",
            "Iteration 486100, Train Loss: 4.090266227722168 Eval Loss: 3.754856300354004\n",
            "Running evaluation at iteration 486200...\n",
            "Iteration 486200, Train Loss: 4.407106399536133 Eval Loss: 3.8345244550704956\n",
            "Running evaluation at iteration 486300...\n",
            "Iteration 486300, Train Loss: 3.3947668075561523 Eval Loss: 3.916423819065094\n",
            "Running evaluation at iteration 486400...\n",
            "Iteration 486400, Train Loss: 3.5712318420410156 Eval Loss: 3.802234833240509\n",
            "Running evaluation at iteration 486500...\n",
            "Iteration 486500, Train Loss: 3.8801562786102295 Eval Loss: 3.76285404920578\n",
            "Running evaluation at iteration 486600...\n",
            "Iteration 486600, Train Loss: 3.7754149436950684 Eval Loss: 3.8413516592979433\n",
            "Running evaluation at iteration 486700...\n",
            "Iteration 486700, Train Loss: 4.011552810668945 Eval Loss: 3.8148199248313905\n",
            "Running evaluation at iteration 486800...\n",
            "Iteration 486800, Train Loss: 3.55121111869812 Eval Loss: 3.8550344729423522\n",
            "Running evaluation at iteration 486900...\n",
            "Iteration 486900, Train Loss: 4.808581352233887 Eval Loss: 3.7122328519821166\n",
            "Running evaluation at iteration 487000...\n",
            "Iteration 487000, Train Loss: 4.863259315490723 Eval Loss: 3.789894685745239\n",
            "Running evaluation at iteration 487100...\n",
            "Iteration 487100, Train Loss: 3.768517017364502 Eval Loss: 3.824531764984131\n",
            "Running evaluation at iteration 487200...\n",
            "Iteration 487200, Train Loss: 4.0849809646606445 Eval Loss: 3.667400553226471\n",
            "Running evaluation at iteration 487300...\n",
            "Iteration 487300, Train Loss: 3.352539539337158 Eval Loss: 3.7515763306617735\n",
            "Running evaluation at iteration 487400...\n",
            "Iteration 487400, Train Loss: 4.169417858123779 Eval Loss: 3.7839827156066894\n",
            "Running evaluation at iteration 487500...\n",
            "Iteration 487500, Train Loss: 3.5662522315979004 Eval Loss: 3.7917615509033205\n",
            "Running evaluation at iteration 487600...\n",
            "Iteration 487600, Train Loss: 2.9912002086639404 Eval Loss: 3.852955379486084\n",
            "Running evaluation at iteration 487700...\n",
            "Iteration 487700, Train Loss: 3.756737470626831 Eval Loss: 3.764313337802887\n",
            "Running evaluation at iteration 487800...\n",
            "Iteration 487800, Train Loss: 4.821444988250732 Eval Loss: 3.7261885833740234\n",
            "Running evaluation at iteration 487900...\n",
            "Iteration 487900, Train Loss: 3.899066925048828 Eval Loss: 3.735386588573456\n",
            "Running evaluation at iteration 488000...\n",
            "Iteration 488000, Train Loss: 4.389490127563477 Eval Loss: 3.812608983516693\n",
            "Running evaluation at iteration 488100...\n",
            "Iteration 488100, Train Loss: 4.316549301147461 Eval Loss: 3.77303067445755\n",
            "Running evaluation at iteration 488200...\n",
            "Iteration 488200, Train Loss: 3.5562984943389893 Eval Loss: 3.752163133621216\n",
            "Running evaluation at iteration 488300...\n",
            "Iteration 488300, Train Loss: 4.830268383026123 Eval Loss: 3.7794056129455567\n",
            "Running evaluation at iteration 488400...\n",
            "Iteration 488400, Train Loss: 3.7726216316223145 Eval Loss: 3.8001661252975465\n",
            "Running evaluation at iteration 488500...\n",
            "Iteration 488500, Train Loss: 4.585406303405762 Eval Loss: 3.848419477939606\n",
            "Running evaluation at iteration 488600...\n",
            "Iteration 488600, Train Loss: 3.597148895263672 Eval Loss: 3.779401891231537\n",
            "Running evaluation at iteration 488700...\n",
            "Iteration 488700, Train Loss: 4.300110340118408 Eval Loss: 3.761236562728882\n",
            "Running evaluation at iteration 488800...\n",
            "Iteration 488800, Train Loss: 3.8783931732177734 Eval Loss: 3.8613199949264527\n",
            "Running evaluation at iteration 488900...\n",
            "Iteration 488900, Train Loss: 4.250857353210449 Eval Loss: 3.793165690898895\n",
            "Running evaluation at iteration 489000...\n",
            "Iteration 489000, Train Loss: 3.4018936157226562 Eval Loss: 3.7821930837631226\n",
            "Running evaluation at iteration 489100...\n",
            "Iteration 489100, Train Loss: 4.652117729187012 Eval Loss: 3.798132619857788\n",
            "Running evaluation at iteration 489200...\n",
            "Iteration 489200, Train Loss: 4.0871663093566895 Eval Loss: 3.798779773712158\n",
            "Running evaluation at iteration 489300...\n",
            "Iteration 489300, Train Loss: 2.8408849239349365 Eval Loss: 3.850751643180847\n",
            "Running evaluation at iteration 489400...\n",
            "Iteration 489400, Train Loss: 4.595182418823242 Eval Loss: 3.755675392150879\n",
            "Running evaluation at iteration 489500...\n",
            "Iteration 489500, Train Loss: 3.292757987976074 Eval Loss: 3.9452114701271057\n",
            "Running evaluation at iteration 489600...\n",
            "Iteration 489600, Train Loss: 4.315591812133789 Eval Loss: 3.7443717885017396\n",
            "Running evaluation at iteration 489700...\n",
            "Iteration 489700, Train Loss: 3.4985814094543457 Eval Loss: 3.78891241312027\n",
            "Running evaluation at iteration 489800...\n",
            "Iteration 489800, Train Loss: 4.515467643737793 Eval Loss: 3.8689368891716005\n",
            "Running evaluation at iteration 489900...\n",
            "Iteration 489900, Train Loss: 3.2374162673950195 Eval Loss: 3.802493164539337\n",
            "Running evaluation at iteration 490000...\n",
            "Iteration 490000, Train Loss: 4.4770097732543945 Eval Loss: 3.7916161942481996\n",
            "Running evaluation at iteration 490100...\n",
            "Iteration 490100, Train Loss: 3.633329391479492 Eval Loss: 3.887786192893982\n",
            "Running evaluation at iteration 490200...\n",
            "Iteration 490200, Train Loss: 3.7146332263946533 Eval Loss: 3.8616250443458555\n",
            "Running evaluation at iteration 490300...\n",
            "Iteration 490300, Train Loss: 2.8227527141571045 Eval Loss: 3.819293427467346\n",
            "Running evaluation at iteration 490400...\n",
            "Iteration 490400, Train Loss: 4.483936786651611 Eval Loss: 3.8034271740913392\n",
            "Running evaluation at iteration 490500...\n",
            "Iteration 490500, Train Loss: 4.049118995666504 Eval Loss: 3.7881141901016235\n",
            "Running evaluation at iteration 490600...\n",
            "Iteration 490600, Train Loss: 4.045392036437988 Eval Loss: 3.856403443813324\n",
            "Running evaluation at iteration 490700...\n",
            "Iteration 490700, Train Loss: 4.203510284423828 Eval Loss: 3.767954068183899\n",
            "Running evaluation at iteration 490800...\n",
            "Iteration 490800, Train Loss: 3.7306859493255615 Eval Loss: 3.800789887905121\n",
            "Running evaluation at iteration 490900...\n",
            "Iteration 490900, Train Loss: 3.684903144836426 Eval Loss: 3.7894399833679198\n",
            "Running evaluation at iteration 491000...\n",
            "Iteration 491000, Train Loss: 3.332972764968872 Eval Loss: 3.8050258421897887\n",
            "Running evaluation at iteration 491100...\n",
            "Iteration 491100, Train Loss: 4.274869918823242 Eval Loss: 3.8069842290878295\n",
            "Running evaluation at iteration 491200...\n",
            "Iteration 491200, Train Loss: 3.4396018981933594 Eval Loss: 3.8461596536636353\n",
            "Running evaluation at iteration 491300...\n",
            "Iteration 491300, Train Loss: 4.204314231872559 Eval Loss: 3.7839965534210207\n",
            "Running evaluation at iteration 491400...\n",
            "Iteration 491400, Train Loss: 3.625819683074951 Eval Loss: 3.7526166367530824\n",
            "Running evaluation at iteration 491500...\n",
            "Iteration 491500, Train Loss: 4.451358795166016 Eval Loss: 3.724932363033295\n",
            "Running evaluation at iteration 491600...\n",
            "Iteration 491600, Train Loss: 2.7250144481658936 Eval Loss: 3.779342851638794\n",
            "Running evaluation at iteration 491700...\n",
            "Iteration 491700, Train Loss: 3.843754291534424 Eval Loss: 3.7366512513160703\n",
            "Running evaluation at iteration 491800...\n",
            "Iteration 491800, Train Loss: 3.4276657104492188 Eval Loss: 3.734096100330353\n",
            "Running evaluation at iteration 491900...\n",
            "Iteration 491900, Train Loss: 3.3778905868530273 Eval Loss: 3.7664360928535463\n",
            "Running evaluation at iteration 492000...\n",
            "Iteration 492000, Train Loss: 4.08506441116333 Eval Loss: 3.7769913601875307\n",
            "Running evaluation at iteration 492100...\n",
            "Iteration 492100, Train Loss: 3.651559352874756 Eval Loss: 3.7952992343902587\n",
            "Running evaluation at iteration 492200...\n",
            "Iteration 492200, Train Loss: 3.5460567474365234 Eval Loss: 3.8147650861740114\n",
            "Running evaluation at iteration 492300...\n",
            "Iteration 492300, Train Loss: 4.342039585113525 Eval Loss: 3.8225419783592223\n",
            "Running evaluation at iteration 492400...\n",
            "Iteration 492400, Train Loss: 3.7460784912109375 Eval Loss: 3.7906942224502562\n",
            "Running evaluation at iteration 492500...\n",
            "Iteration 492500, Train Loss: 3.1106622219085693 Eval Loss: 3.7173561453819275\n",
            "Running evaluation at iteration 492600...\n",
            "Iteration 492600, Train Loss: 3.907592296600342 Eval Loss: 3.745581967830658\n",
            "Running evaluation at iteration 492700...\n",
            "Iteration 492700, Train Loss: 3.191967487335205 Eval Loss: 3.742306821346283\n",
            "Running evaluation at iteration 492800...\n",
            "Iteration 492800, Train Loss: 3.8247721195220947 Eval Loss: 3.8174040794372557\n",
            "Running evaluation at iteration 492900...\n",
            "Iteration 492900, Train Loss: 5.019245624542236 Eval Loss: 3.8109669518470763\n",
            "Running evaluation at iteration 493000...\n",
            "Iteration 493000, Train Loss: 3.5994668006896973 Eval Loss: 3.831521632671356\n",
            "Running evaluation at iteration 493100...\n",
            "Iteration 493100, Train Loss: 4.345086574554443 Eval Loss: 3.7759034180641176\n",
            "Running evaluation at iteration 493200...\n",
            "Iteration 493200, Train Loss: 4.0089826583862305 Eval Loss: 3.803579807281494\n",
            "Running evaluation at iteration 493300...\n",
            "Iteration 493300, Train Loss: 3.7788238525390625 Eval Loss: 3.8563398790359495\n",
            "Running evaluation at iteration 493400...\n",
            "Iteration 493400, Train Loss: 3.7112863063812256 Eval Loss: 3.801964044570923\n",
            "Running evaluation at iteration 493500...\n",
            "Iteration 493500, Train Loss: 4.456193447113037 Eval Loss: 3.8314510917663576\n",
            "Running evaluation at iteration 493600...\n",
            "Iteration 493600, Train Loss: 4.264543056488037 Eval Loss: 3.716809551715851\n",
            "Running evaluation at iteration 493700...\n",
            "Iteration 493700, Train Loss: 3.813398838043213 Eval Loss: 3.7536197113990784\n",
            "Running evaluation at iteration 493800...\n",
            "Iteration 493800, Train Loss: 3.8690168857574463 Eval Loss: 3.735219759941101\n",
            "Running evaluation at iteration 493900...\n",
            "Iteration 493900, Train Loss: 4.347970008850098 Eval Loss: 3.8374905943870545\n",
            "Running evaluation at iteration 494000...\n",
            "Iteration 494000, Train Loss: 4.157741546630859 Eval Loss: 3.7932861471176147\n",
            "Running evaluation at iteration 494100...\n",
            "Iteration 494100, Train Loss: 3.464258909225464 Eval Loss: 3.7579431629180906\n",
            "Running evaluation at iteration 494200...\n",
            "Iteration 494200, Train Loss: 4.5401482582092285 Eval Loss: 3.744625160694122\n",
            "Running evaluation at iteration 494300...\n",
            "Iteration 494300, Train Loss: 3.810706377029419 Eval Loss: 3.799165108203888\n",
            "Running evaluation at iteration 494400...\n",
            "Iteration 494400, Train Loss: 3.4573848247528076 Eval Loss: 3.8074335169792177\n",
            "Running evaluation at iteration 494500...\n",
            "Iteration 494500, Train Loss: 3.6703529357910156 Eval Loss: 3.7361171197891236\n",
            "Running evaluation at iteration 494600...\n",
            "Iteration 494600, Train Loss: 4.260100364685059 Eval Loss: 3.7854072880744933\n",
            "Running evaluation at iteration 494700...\n",
            "Iteration 494700, Train Loss: 3.6111721992492676 Eval Loss: 3.8210381984710695\n",
            "Running evaluation at iteration 494800...\n",
            "Iteration 494800, Train Loss: 3.025378465652466 Eval Loss: 3.758303873538971\n",
            "Running evaluation at iteration 494900...\n",
            "Iteration 494900, Train Loss: 3.4225761890411377 Eval Loss: 3.884821743965149\n",
            "Running evaluation at iteration 495000...\n",
            "Iteration 495000, Train Loss: 4.325651168823242 Eval Loss: 3.8221790051460265\n",
            "Running evaluation at iteration 495100...\n",
            "Iteration 495100, Train Loss: 4.915923118591309 Eval Loss: 3.8195614981651307\n",
            "Running evaluation at iteration 495200...\n",
            "Iteration 495200, Train Loss: 4.258682727813721 Eval Loss: 3.821360764503479\n",
            "Running evaluation at iteration 495300...\n",
            "Iteration 495300, Train Loss: 3.5604729652404785 Eval Loss: 3.7353822135925294\n",
            "Running evaluation at iteration 495400...\n",
            "Iteration 495400, Train Loss: 4.3199992179870605 Eval Loss: 3.730411446094513\n",
            "Running evaluation at iteration 495500...\n",
            "Iteration 495500, Train Loss: 3.877924680709839 Eval Loss: 3.8478713989257813\n",
            "Running evaluation at iteration 495600...\n",
            "Iteration 495600, Train Loss: 3.8872618675231934 Eval Loss: 3.7971532464027407\n",
            "Running evaluation at iteration 495700...\n",
            "Iteration 495700, Train Loss: 3.601985454559326 Eval Loss: 3.79338219165802\n",
            "Running evaluation at iteration 495800...\n",
            "Iteration 495800, Train Loss: 4.140847682952881 Eval Loss: 3.762843623161316\n",
            "Running evaluation at iteration 495900...\n",
            "Iteration 495900, Train Loss: 3.3853557109832764 Eval Loss: 3.7503159475326537\n",
            "Running evaluation at iteration 496000...\n",
            "Iteration 496000, Train Loss: 2.582430124282837 Eval Loss: 3.7460337018966676\n",
            "Running evaluation at iteration 496100...\n",
            "Iteration 496100, Train Loss: 3.7555272579193115 Eval Loss: 3.722850203514099\n",
            "Running evaluation at iteration 496200...\n",
            "Iteration 496200, Train Loss: 4.581661224365234 Eval Loss: 3.832993950843811\n",
            "Running evaluation at iteration 496300...\n",
            "Iteration 496300, Train Loss: 3.1580498218536377 Eval Loss: 3.838678843975067\n",
            "Running evaluation at iteration 496400...\n",
            "Iteration 496400, Train Loss: 3.5234932899475098 Eval Loss: 3.7815401411056517\n",
            "Running evaluation at iteration 496500...\n",
            "Iteration 496500, Train Loss: 4.770031452178955 Eval Loss: 3.7227732491493226\n",
            "Running evaluation at iteration 496600...\n",
            "Iteration 496600, Train Loss: 3.5567626953125 Eval Loss: 3.830926480293274\n",
            "Running evaluation at iteration 496700...\n",
            "Iteration 496700, Train Loss: 3.9607748985290527 Eval Loss: 3.789461860656738\n",
            "Running evaluation at iteration 496800...\n",
            "Iteration 496800, Train Loss: 3.8542659282684326 Eval Loss: 3.730412549972534\n",
            "Running evaluation at iteration 496900...\n",
            "Iteration 496900, Train Loss: 3.559091091156006 Eval Loss: 3.798339743614197\n",
            "Running evaluation at iteration 497000...\n",
            "Iteration 497000, Train Loss: 3.4732308387756348 Eval Loss: 3.74795951128006\n",
            "Running evaluation at iteration 497100...\n",
            "Iteration 497100, Train Loss: 3.791759490966797 Eval Loss: 3.82318945646286\n",
            "Running evaluation at iteration 497200...\n",
            "Iteration 497200, Train Loss: 3.044912099838257 Eval Loss: 3.8549739956855773\n",
            "Running evaluation at iteration 497300...\n",
            "Iteration 497300, Train Loss: 4.161127090454102 Eval Loss: 3.7385493302345276\n",
            "Running evaluation at iteration 497400...\n",
            "Iteration 497400, Train Loss: 4.049868583679199 Eval Loss: 3.8717183208465578\n",
            "Running evaluation at iteration 497500...\n",
            "Iteration 497500, Train Loss: 3.2487432956695557 Eval Loss: 3.7400593256950376\n",
            "Running evaluation at iteration 497600...\n",
            "Iteration 497600, Train Loss: 3.2674942016601562 Eval Loss: 3.8067103815078736\n",
            "Running evaluation at iteration 497700...\n",
            "Iteration 497700, Train Loss: 4.282341957092285 Eval Loss: 3.791045904159546\n",
            "Running evaluation at iteration 497800...\n",
            "Iteration 497800, Train Loss: 3.3806753158569336 Eval Loss: 3.769808900356293\n",
            "Running evaluation at iteration 497900...\n",
            "Iteration 497900, Train Loss: 3.3137426376342773 Eval Loss: 3.8090808033943175\n",
            "Running evaluation at iteration 498000...\n",
            "Iteration 498000, Train Loss: 4.277585029602051 Eval Loss: 3.8341827726364137\n",
            "Running evaluation at iteration 498100...\n",
            "Iteration 498100, Train Loss: 3.859988212585449 Eval Loss: 3.8497211265563966\n",
            "Running evaluation at iteration 498200...\n",
            "Iteration 498200, Train Loss: 4.206120014190674 Eval Loss: 3.8800561165809633\n",
            "Running evaluation at iteration 498300...\n",
            "Iteration 498300, Train Loss: 3.4601523876190186 Eval Loss: 3.774447913169861\n",
            "Running evaluation at iteration 498400...\n",
            "Iteration 498400, Train Loss: 4.215731620788574 Eval Loss: 3.7704061889648437\n",
            "Running evaluation at iteration 498500...\n",
            "Iteration 498500, Train Loss: 4.7209792137146 Eval Loss: 3.7745204758644104\n",
            "Running evaluation at iteration 498600...\n",
            "Iteration 498600, Train Loss: 4.130168914794922 Eval Loss: 3.7600314664840697\n",
            "Running evaluation at iteration 498700...\n",
            "Iteration 498700, Train Loss: 3.656179189682007 Eval Loss: 3.759608962535858\n",
            "Running evaluation at iteration 498800...\n",
            "Iteration 498800, Train Loss: 4.099437713623047 Eval Loss: 3.9449953842163086\n",
            "Running evaluation at iteration 498900...\n",
            "Iteration 498900, Train Loss: 4.65391731262207 Eval Loss: 3.7410615134239196\n",
            "Running evaluation at iteration 499000...\n",
            "Iteration 499000, Train Loss: 2.876652956008911 Eval Loss: 3.7814640021324157\n",
            "Running evaluation at iteration 499100...\n",
            "Iteration 499100, Train Loss: 4.331911087036133 Eval Loss: 3.811871633529663\n",
            "Running evaluation at iteration 499200...\n",
            "Iteration 499200, Train Loss: 3.349799156188965 Eval Loss: 3.831345252990723\n",
            "Running evaluation at iteration 499300...\n",
            "Iteration 499300, Train Loss: 3.9934656620025635 Eval Loss: 3.738645224571228\n",
            "Running evaluation at iteration 499400...\n",
            "Iteration 499400, Train Loss: 4.075997829437256 Eval Loss: 3.7747025752067564\n",
            "Running evaluation at iteration 499500...\n",
            "Iteration 499500, Train Loss: 3.790778875350952 Eval Loss: 3.8040423440933226\n",
            "Running evaluation at iteration 499600...\n",
            "Iteration 499600, Train Loss: 4.439549922943115 Eval Loss: 3.7650689768791197\n",
            "Running evaluation at iteration 499700...\n",
            "Iteration 499700, Train Loss: 4.178234100341797 Eval Loss: 3.7230581736564634\n",
            "Running evaluation at iteration 499800...\n",
            "Iteration 499800, Train Loss: 3.6406869888305664 Eval Loss: 3.775996253490448\n",
            "Running evaluation at iteration 499900...\n",
            "Iteration 499900, Train Loss: 3.8130879402160645 Eval Loss: 3.78347305059433\n",
            "Running evaluation at iteration 500000...\n",
            "Iteration 500000, Train Loss: 3.3939990997314453 Eval Loss: 3.76672447681427\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "solver(model_name=\"bigram\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gikf2jvU8iQx"
      },
      "source": [
        "### Train and Valid Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "img1 = mpimg.imread('/content/drive/MyDrive/ECE_239AS.2/Project3_skeleton/Graphs/Training_Loss.png')\n",
        "plt.figure()\n",
        "plt.imshow(img1)\n",
        "plt.axis('off')     # hide axes\n",
        "plt.title('Training Loss')\n",
        "plt.show()\n",
        "\n",
        "img2 = mpimg.imread('/content/drive/MyDrive/ECE_239AS.2/Project3_skeleton/Graphs/Eval_Loss.png')\n",
        "plt.figure()\n",
        "plt.imshow(img2)\n",
        "plt.axis('off')\n",
        "plt.title('Validation Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "nSUDGDMzB9yb",
        "outputId": "70339035-9495-427a-bd35-e9ab6c161897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAEuCAYAAAATAREiAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAos9JREFUeJzsvXe8ZEd55/2rOqHTzXFyljSSRgEFJIQikgAhMBKyMbDYgHdZ2xiv1+F97ddrbGC9tsFer7O9xhgwGctEIQmEhHIcaSRNznnm5tzphKr3jwqnTnffmTvx3jtTXz5iZrpPn1Mn1ZOfIpxzDovFYrFYLOctdLYHYLFYLBaLZXaxyoDFYrFYLOc5VhmwWCwWi+U8xyoDFovFYrGc51hlwGKxWCyW8xyrDFgsFovFcp5jlQGLxWKxWM5zrDJgsVgsFst5jlUGLBaLxWI5z7HKgMUyB/nwhz+MFStWnNRvP/nJT4IQcnoHZLFYzmmsMmCxnACEkBn99/jjj8/2UGeFD3/4w2hqaprtYVgslhOE2LUJLJaZ85WvfCX173/7t3/DI488gi9/+cupz++880709vae9HHCMARjDJlM5oR/G0URoihCNps96eOfLB/+8Idx//33Y2pq6qwf22KxnDzubA/AYplPfPCDH0z9+/nnn8cjjzxS93ktpVIJ+Xx+xsfxPO+kxgcAruvCde2rbbFYZo4NE1gsp5lbb70V69atw8svv4ybb74Z+Xwev//7vw8A+N73voe7774bixYtQiaTwerVq/E//+f/RBzHqX3U5gzs27cPhBD8xV/8Bf75n/8Zq1evRiaTwbXXXouXXnop9dtGOQOEEHz84x/Hd7/7Xaxbtw6ZTAaXXnopHn744brxP/7447jmmmuQzWaxevVq/N//+39Pex7Cv//7v+Pqq69GLpdDV1cXPvjBD+Lw4cOpbfr6+vCRj3wES5YsQSaTwcKFC/Hud78b+/bt09usX78eb3vb29DV1YVcLoeVK1fil37pl07bOC2W8wVrPlgsZ4Dh4WHcddddeN/73ocPfvCDOmTwxS9+EU1NTfit3/otNDU14bHHHsMf/uEfYmJiAn/+539+3P1+7Wtfw+TkJH75l38ZhBB89rOfxXve8x7s2bPnuN6Ep59+Gt/+9rfxsY99DM3Nzfibv/kb3HfffThw4AA6OzsBABs2bMDb3/52LFy4EJ/61KcQxzE+/elPo7u7+9QviuSLX/wiPvKRj+Daa6/Fn/7pn6K/vx9//dd/jWeeeQYbNmxAW1sbAOC+++7D5s2b8eu//utYsWIFBgYG8Mgjj+DAgQP6329961vR3d2N3/u930NbWxv27duHb3/726dtrBbLeQO3WCwnza/92q/x2tfolltu4QD4P/3TP9VtXyqV6j775V/+ZZ7P53mlUtGffehDH+LLly/X/967dy8HwDs7O/nIyIj+/Hvf+x4HwH/wgx/oz/7oj/6obkwAuO/7fNeuXfqz1157jQPgf/u3f6s/e9e73sXz+Tw/fPiw/mznzp3cdd26fTbiQx/6EC8UCtN+HwQB7+np4evWrePlcll//sADD3AA/A//8A8555yPjo5yAPzP//zPp93Xd77zHQ6Av/TSS8cdl8ViOTY2TGCxnAEymQw+8pGP1H2ey+X03ycnJzE0NISbbroJpVIJ27ZtO+5+f/7nfx7t7e363zfddBMAYM+ePcf97R133IHVq1frf19++eVoaWnRv43jGD/5yU9wzz33YNGiRXq7NWvW4K677jru/mfC+vXrMTAwgI997GOpBMe7774ba9euxQ9/+EMA4jr5vo/HH38co6OjDfelPAgPPPAAwjA8LeOzWM5XrDJgsZwBFi9eDN/36z7fvHkz7r33XrS2tqKlpQXd3d06+XB8fPy4+122bFnq30oxmE5gHuu36vfqtwMDAyiXy1izZk3ddo0+Oxn2798PALjooovqvlu7dq3+PpPJ4DOf+Qweeugh9Pb24uabb8ZnP/tZ9PX16e1vueUW3HffffjUpz6Frq4uvPvd78YXvvAFVKvV0zJWi+V8wioDFssZwPQAKMbGxnDLLbfgtddew6c//Wn84Ac/wCOPPILPfOYzAADG2HH36zhOw8/5DCqET+W3s8F//+//HTt27MCf/umfIpvN4hOf+AQuvvhibNiwAYBIirz//vvx3HPP4eMf/zgOHz6MX/qlX8LVV19tSxstlhPEKgMWy1ni8ccfx/DwML74xS/iN37jN/DOd74Td9xxR8rtP5v09PQgm81i165ddd81+uxkWL58OQBg+/btdd9t375df69YvXo1fvu3fxs//vGPsWnTJgRBgP/9v/93apvrr78e/+t//S+sX78eX/3qV7F582Z84xvfOC3jtVjOF6wyYLGcJZRlblriQRDgH/7hH2ZrSCkcx8Edd9yB7373uzhy5Ij+fNeuXXjooYdOyzGuueYa9PT04J/+6Z9S7vyHHnoIW7duxd133w1A9GWoVCqp365evRrNzc36d6Ojo3VejSuvvBIAbKjAYjlBbGmhxXKWuOGGG9De3o4PfehD+G//7b+BEIIvf/nLc8pN/8lPfhI//vGP8eY3vxm/+qu/ijiO8Xd/93dYt24dXn311RntIwxD/PEf/3Hd5x0dHfjYxz6Gz3zmM/jIRz6CW265Be9///t1aeGKFSvwm7/5mwCAHTt24Pbbb8d73/teXHLJJXBdF9/5znfQ39+P973vfQCAL33pS/iHf/gH3HvvvVi9ejUmJyfxuc99Di0tLXjHO95x2q6JxXI+YJUBi+Us0dnZiQceeAC//du/jT/4gz9Ae3s7PvjBD+L222/H2972ttkeHgDg6quvxkMPPYTf+Z3fwSc+8QksXboUn/70p7F169YZVTsAwtvxiU98ou7z1atX42Mf+xg+/OEPI5/P48/+7M/wu7/7uygUCrj33nvxmc98RlcILF26FO9///vx6KOP4stf/jJc18XatWvxrW99C/fddx8AkUD44osv4hvf+Ab6+/vR2tqKN77xjfjqV7+KlStXnrZrYrGcD9i1CSwWy3G55557sHnzZuzcuXO2h2KxWM4ANmfAYrGkKJfLqX/v3LkTDz74IG699dbZGZDFYjnjWM+AxWJJsXDhQnz4wx/GqlWrsH//fvzjP/4jqtUqNmzYgAsuuGC2h2exWM4ANmfAYrGkePvb346vf/3r6OvrQyaTwZve9Cb8yZ/8iVUELJZzGOsZsFgsFovlPMfmDFgsFovFcp5jlQGLxWKxWM5zrDJgsVgsFst5jlUGLBaLxWI5z7HKgMVisVgs5zlWGbBYLBaL5TzHKgMWi8VisZznWGXAYrFYLJbzHKsMWCwWi8VynmOVAYvFYrFYznOsMmCxWCwWy3mOVQYsFovFYjnPscqAxWKxWCznOVYZsFgsFovlPMcqAxaLxWKxnOdYZcBisVgslvMcqwxYLBaLxXKeY5UBi8VisVjOc6wyYLGcw3DOUSqVEEXRbA/FYrHMYdzZHoDFYqknDENsePU1VKvV1OeEEFx66SVob2ub0X4GB4fwu7//P/D2t70V7/3Z+0AIOe5vGGN4feMmZLMZXHThhTP6jcVimd9YZcBimYNUKlX8y79+AYNDQ5iYmEC1GqCrqxOu4+IT/+P/m7EykMtlcfnll2HZ0qUzPnYcx/i7f/gnLFq4AJ/6o0+c5BlYLJb5hFUGLJY5SFNTAX/5558B4xx/9/f/iFdfex1/9zf/B7lsFp7n4fWNG7F06VLs2bMHURTjqjdcif37D2DXnj3IZjK46g1Xorm5GdlsFrfdcgsWLOgFAOzZuw+OQ+G5HjZu2oTu7i5cftllcN30VMA5B+e8blyccxw6fBhbtmxFU1MTrrj8MhQKBQDA5OQkNrz6GiqVCi6+eC2WLF4MANixcxd279mD7q4urFt3KXLZrPU2WCxzDKsMWCxzEEIImpqawDmH53mglKK5qQm5XA4jo6P4xCc/jUsuvhhbt23HjTe8CYNDQ/j8v34RbW1tOHL0CK684gp8+o8+galiEX/wR5/Ef3r/+/C+9/4cvvzVr2HHjp1wXRcTkxMYHR3Dpz/5h7jpzTccV0BzzvHMs8/hs//7L9FUKGBqqojly5fhU3/4CeTzOXzqj/8E+/cfQHd3F+7/9nfx2T/9X9i8ZQv+1599FiuWL8P4+Djue8+9eM897z5LV9FiscwUqwxYLPOQKIwwMDCAv/urv0RXVyeKpRLWXXopujo78N3v/wCf/8KXcPjIUbS2tCCOmbbyOWOYnJrEZ//0T9DW2oJf/fhv4KWX1uOmN99w3GOWy2X88798HpesXYs/+P3fw6HDh/Hff/v/xQ9++EO8/a13YtPmzfjgB96Pn33PvZiaKqKtrRUbXn0NhUIef/gHv49cNgff9870pbFYLCeBrSawWOYpN914IxYvXoRMJgNCCB597DH87u//AX744MMIwxDFYrHh77o6u7B61Up0dHSgtbUFpXJpRscbHhnBocNHcP11b0ShUMDq1auxfNlSbNy0GW1t7Xj7W9+Kr339m/iDP/wkdu/ZA8453nLbrXAcB7/xW7+Db91/Pyo1CZEWi2VuYJUBi2We4joOAJH9/zd/+/d46pln8Sv/9b/glz/6n0EJARrE/OshwEw2A+C6LhzHQblcEceNY1SDAJlMBp7n4uMf+xX82Z/8MbK5HD7xyU9j2/YduOTitfj7v/krvPe++/DADx/C3/79P4LNaFwWi+VsYsMEFss8J45j7Nt/AL29PWhqasIjP3nstAjcgcFBPPnU0wAhcB0Hl122DldcdhkefPhhXHrpxdi9ey8OHDiI97/35zAxMYGHf/QIrr/ujXjH29+G5557HuPjY3jiyafgui5ueNP1eOGllzAwMADOGECtHWKxzCWsMmCxzHFaW1vR092tE/wopejt7UGhSWTxe56Hd73zHfjil76M3/sfn8DVb3gDVixfDs/3QR2KBb09OuO/ra0NpXIZgEhS7O7qRFtNmSIhBF1dndixYyf++m//HgCQL+TxF5/5U/zGr/8a/vKv/wa//4k/gud6eP/Pvxe33noLhoaG8OL69fjWf/yHCA+85TasW7cODz70ML7zve8jqAbI5/P4+Md+BY70aFgslrkD4Y3qhywWy5yAc45qtYo4jpHP50EIAWMMpVIZvu/B930AIlQwPDICSinaWltRLleQy4kSvlKpBN/34XkeKtUqOGPI5XIAgFK5DIdSnXegjlkulxHHsR4HIUQfPwxDjIyOwvd9tLW2glIKzjmiKMLI6CgIIehob9dCf3JyElPFIlqam1EoFGxZocUyB7HKgMVisVgs5zk2cGexWCwWy3mOVQYsFovFYjnPscqAxWKxWCznOVYZsFgsFovlPMcqAxaLxWKxnOdYZcBimUViFoOx+PgbzmFiFs37c7BYznesMmCxzCKvbv8pXt/55CnvJ44jVINSw2WHzzQvbf4RNu957pT2wTlHFIcYGDmIMAoAiHM62L8DOw9sQLk6NSvnZrGcL1hlwGKZRaZKYyiWxxHHwrrWqwtK4Wj+O2Yx4jgC51z+x8A5QxxHODy4C4+//O/gUJ8nvzOFaPK7ZP9qH+ljRYjleNR/YnuWOn4Uh5gsjqJUnkiNmzGW+q15XPPv5rieePnf8Q///lsYnegH5xzb9r2EV7b+BNv3vYRv/fgvELMotX3MYlSjqlUSLJbTgG1HbLGcBjjnmKgcRTVqvFKgQz2055eCkvpWvHsOb8K+I1vAOcN9d/x3uI6Hx9d/CwOjB9HZugi3v/ED2H3wVWzY/hjKlSnccs3PYeWidfjJC18F4zEYYxgaO4yDfdvgOj4AjsvW3ITu9iV4afOPsLBrJZYuuEgf7/H130IURzjQtxUrF61DEFVw4Og23Hbt+7Bm6ZXYdfBVvLT5YRBCcevVP4eWpk48+uLXMDoxgK62RXjbDR9GEJbx0NP/ilJ1EuOTQ7j20reBsRgvbHoIO/e/DM/L4s7rP4iRiX5MFUdx5UW34gdP/jOuueQOdLQuxFOv/Aduv+6DcOT1IITglqt/DnsPbwLnHIQQrF15LS5edR1K5Ql88ft/hCgO4ToeOOfYMrgFn3riUzgwfgDvvPCd+M03/SYKXuGM3FuL5XzAKgMWy2mA8xjP7fkCBqd2Nvw+57Xh7ss+hZzXWvdd1s/hHTd+FP/x6F/hUP8OTJXGEIQV3Hvbr+PRF7+GfUc2o6djGe6+8aPYe2QTnt7wHaxYeCm27n0BN131HqxbfQM2734O+WwzbrzyHmza/TRe2/E4brvm57F597NYu+JaxHEEgINQB3uPbMLlF9yCKy+6FZ/79u/hF+7+BJYtWIsXNz2IxT1r8Pj6b+Let/w3FMvjeOrV7+Ddt3wM11/2TnhuBl958I8xPHYEr+98Ck35dtx900fx4DOfBwAcHtyNTbuexgfu+v+w78hmPPTMv+It174fz7/2A6xeegUO9W9HS6EDKxevQ7EyCQIgikMQQkCJA4c6oCRxVlLiYPPuZ/DEy/fjigtvQcYTLZTLURkfe/BjeHK/CK+sP7IePYUefPSqj9pWxxbLSWKVAYvlNECIg3WL7kYxGG74vefk4Tv5ht8t6l6NfLYZrU1dqIYVHOjbhv7h/Xj42S9gqjSKmIXoHzmALXueAyEExcoEODgyfh6rFl8G38vBczNwHR++l8XaFW/EN3/85zgyuBtN+TZQ6uD7T/wjGGe487oPwqEuFnWvQkuhEy1NXehuXwLX9RBEVUwUh9E/cgBPvnw/GI/hUg9BWMYLmx4EAUG5MoVKUMLhgZ14y7XvQzZTQHO+AwDQN7QXi3suQD7bghWLLsWjL34dbc3dqIQl7Dn0Oi5dfQP6hveBcYZViy/HLuntWNi5Ejdd9Z6G12alPL8nX7kfV669Dc35doyUR7B5YLPeJuYxnjv0HD561UdP8S5aLOcvVhmwWE4DhBAsab/ylH4PCKu2u30JMl4Ot1/3AZGlTwj+9bt/gPe+9XdQLI/joac/X/d7x3ERxQE4OPLZFizoXIEnXrkfb7joLSjkWvGuW34FAEDpdCsGimPnsy3oaOnF7df9J+SzTeCcY/2WHyPr53HL1e/F0aG9AIDmQgf6Rw5gce8FKFXG0VLoQEfrAmzZ8xyiOMTQ2GHks83I+gX0tC/Fhu0/xdtv+AgGRg5iz6HXcNXat6C1qQurllwOAnHuMYvAOQPjERhnGBg5gI6WXqxcvA4/Xf9NFMvjaM63oz3bjou7LsbTB58W504cvHHxG0/62lssFptAaLHMKoVcC/LZFgBAS6EdWT+HN1z0FkyURvDvj/wf/PDpf0EYVrGoezUefPpf8PLWn6CjbREAoKOlF5QKfX5B5woMjB7ET57/ChiPcdkFN+Fg33asXLwOhBC4jgfX8UBA0NbcA9fxQQhBZ+sCEELhOh7am3vQlG/DG9e9A9957G/w7Uf/BrsPvYZF3Wuw++BreODJf4Ln+vDdDN585bvxytaf4Js/+guMTw0jn23G8oWXoLt9Kb7+8J/h0Re/jjuv/wVQ6uCiFdciigJ0tS3C8kWXwPeyaCl0glIHruPBcVwUy+P49qN/jdHJATz8zBdx4Og2HDi6Fd/80V/gaw/9KRZ1rUJX22IAQN7L4+/v/nvcs/YeXLXwKvyPm/8HfuHyX7AhAovlFLCrFloss4jKkHeoiygOQQkFIRSMM1SqU/DcLDzXB+MxKtUSsn4eHFxv7zoeCCHgnCMIy+Ccw3Mz2LbvRew6+Cp+5pZfTQlJzjniOAR1XBAQvQ+AI44jOI4HAKgGJTDOkMsUABBUghIooXAdF4Q6ICAIoyqiOBSxfELgUEeP23V8eG5GL7kcswiu44FxBsZiPe5kXAxhJDwbAOA6PiihqIZlsDhCNtMESmn6PHiMiEXw5bYWi+XkscqAxXIOwTnHyPhR7D70Oi5cfjVam7qsxWyxWI6LVQYsFovFYjnPsb41i8VisVjOc6wyYLFYLBbLeY5VBiwWi8ViOc+xyoDFYrFYLOc5VhmwWCwWi+U8Z151IDQLH2y5lMVisVgsp4d55RmIoghDIwOoBOXZHorFYrFYLOcM80oZYIwjqEaYKPXZNcwtFovFYjlNzCtlQEAwVj4y24OwWCwWi+WcYd4pAwQExWAEHGy2h2KxWCwWyznBvFMGQCgqwbhe4MVisVgsFsupMf+UAQBhXEHMgtkehsVisVgs5wTzUhmIWYCYhbM9DIvFYrFYzgnmpzLAI0SsOtvDsFgsFovlnGBeKgOMRwjjymwPw2KxWCyWc4J5qQxwzhDGpdkehsVisVgs5wTzUxkARxiXwWEbD1ksFovFcqrMS2UAnIswgdUFLBaLxWI5ZeanMgAgsqWFFovFYrGcFs6oMhDHMY4e7cfmzdvAmOgYyDlHX98AHnvsKbz66iZE0ck1D7J9BiwWi8ViOT2cUWXg8OGj+MEPHsbDDz+KOBbKwMTEJL797QfQ2tqCrVt34IUXXj6pRYesZ8BisVgsltPDGVUGli5djHvuuRue5wEQXoGdO/dg8eIFuOqqy/GWt9yE11/fgjiOZ7xPAoASx3oGLBaLxWI5TbhncueEEFBKUp8NDg6ju7sLhBA0NzchCAIEQQjXrR8KYwxBkHQaDEMRUmjNLofLm1CtCoXAcRwQUvdzi8VisVjmLEJGUpA5IMDOqDLQCM4ZKBUOCXX+04UJOOeIogjqa+FBIMi7XSA8UQ7Evmb/YlosFovFMlOUMjAXOOvKQGtrK8bHJ8A5R7lcheM4OoxQi+M4aGoq6H9XqwHK5QrGw70gPMKlTbdaJcBisVgsllPkjKkknHOMjY1jaGgEQRBiYGAQQRBizZoV2L17HwYHh/HqqxuxYsUyeN6J6SQuzdicAYvFYrFYThNn1D+xadM2bNjwOnp6uvDUU89jeHgEPT3duOGGa/Hww49iYmISt9765hO27h3qy1ULbdchi8VisVhOFcJPpq5vBky3W0IIOOfgnGslYKbKQLUaYGRkDEcrz2G4sg23r/1/QMnciLdYLBaLxTJfOWM5A8cS8ISQU4r1W8+AxWKxWCynj3lpVjvEBePxSTUrslgsFovFkmZeKgOUCmXAegYsFovFYjl15qUykHgG2GwPxWKxWCyWec+8VAaUZ4Bbz4DFYrFYLKfM/FQGiAvOmc0ZsFgsFovlNDBPlQEHnMdg/OSWP7ZYLBaLxZIwT5UBESYQSYQWi8VisVhOhfmpDFAHHNwmEFosFovFchqYn8oAcQBwcFjPgMVisVgsp8o8VQZccM5tmMBisVgsltPAvFQGCHEAwJYXWiwWi8VyGpiXyoBanIix2DYhtFgsFovlFJmfygCUZ8CWFlosFovFcqrMS2WAEApCiM0ZsFgsFovlNDBPlQEHAEHMgtkeisVisVgs8555qQxQQkFAUI2mwGF7DVgsFovFcirMU2XAhUM9hHEZsOsTWCwWi8VySsxLZcChHjwnh0o0aYsJLBaLxWI5RealMkCJg4zbhGo4BVtbaLFYLBbLqTEvlQGAIOu1oBJNzvZALBaLxWKZ98xLZYAQIOe1IIiKYsEi6x2wWCwWi+WkmZfKAEDg0ixiFtiVCy0Wi8ViOUXmqTIgkght0yGLxWKxWE6dea0MxCwSngEbJbBYLBaL5aSZx8qAD8YjGyawWCwWi+UUmcfKgGeXMLZYLBaL5TQwf5UB4oHxCDEPZ3soFovFYrHMa+avMiA9A1Fcne2hWCwWi8Uyr5nHyoALzhkiVoXNILRYLBaL5eSZt8oAJcIzwHhk8wYsFovFYjkF3Nk4KOccjDEQQvR/J4pDXQBclhZy4MR3YbFYLBaLBbOgDMQxwwsvrMeePfvgui5uvfVG9PZ2n7BCQIkLxhnK4bj1DFgsFovFcgqc9TDBwMAgNm3ahnvuuRtXXLEOP/3p0ye1H4e6COMyXt7/TQRx6TSP0mKxWCyW84ez7hmglIJzDkKA5uYmUDq9PsL59BY/JS4ICIK4iJjZ8kKLxWKxWE6Ws64MdHV1oLu7E1/4wtfhui7uuuv2abdljKFYLENVC8Sx6DZYKpXB4GBly+2IeYAw4KhwW2JosVgslvkDIQS+751U3tzp5qwqA5xz7N17AADwsz/7M3jhhZfxyiuvY8mSxXCc+otBCEE2m9H/DsMQlUoVvu+BU46xYA/CuAzQEJ43K7mQFovFYrGcFHNBCVCcdQm6fftOrFq1HL293bjzzlvxuc/9G8rlMpqaCnXbUkrh+0kYQYUNXNcFcThK0QAq4SQYr+pww1y6uBaLxWKxzAfOujKwZs1KPPXU8/A8D/39A+js7EhZ/zOFUgeUeuBgKAYj2D30DC5deBcKmc4zMGqLxWKxWM5dzqoyQAjBBResRi6Xw/79B9HW1orrr78WjuOcxL4cuNQH5xzlcBx7h57D6u4brTJgsVgsFssJMivVBMuWLcGyZUtOaT8EFC7NgIMjjEtgiO1yxhaLxWKxnATzth0xIQQu9QFwBFEJnDOrDFgsFovFchLMX2UABJ6bB0BQiSbBOQPj8WwPy2KxWCyWecc8rscjuHLJfQiiIoan9lrPgMVisVgsJ8n89QwQgrbcIixsuRSjpYNiBUNYz4DFYrFYLCfKPPYMAABBPtMBDgaAgNswgcVisVgsJ8y89QwoPJqFWr+YsfiY6xlYLBaLxWKpZ94rA66TAQEBwBFzu2CRxWKxWCwnyrxXBjwnq1sQRyyAWtTIYrFYLBbLzJjXygAhBA71QeRpMBaCy/9ZLBaLxWKZGfNaGQAAh7ggRJxGxEJbXmixWCwWywky/5UB6mtlgPHIegUsFovFYjlB5r0yQIkLSsRCR7HyDFh9wGKxWCyWGTP/lQHqgBLRLoHxCFYTsFgsFovlxJj/ygBx4FAPgKgmsH0GLBaLxWI5Mea9MuDRLK5a9l605RaD8QgRq872kCwWi8VimVfMe2WAEAfdTWuQ8ZrBOcNI6QD6J7dZD4HFYrFYLDNk3isDgFjOmBIHjMc4MrYRrx78D7tOgcVisVgsM+TcUAaIAwIKzmMwHorcAZtIaLFYLBbLjDgnlAEAoISif3IHSsEYIlZFxKrgnKf+s1gsFovFUs88X8JYQAmF62QxWRlAKRhB1m1FGJfhOwUAAAfTLYvPBBxcLpZksVgsFsv845zwDFDqYUXndXCpj5iFYDxCNSpC9RxgNS2KOU6ft4BzDnDYsITFYrFY5i3nhDLgEg95v8PoN1DF4bHXUIkmwTkDYzVLG3Oc1jUMOOx6CBaLxWKZv8x7ZYAQAkIceDSrlYEwLmPj4QcwVR1CzMM6zwBg1zGwWCwWi0Ux75UBhefk4FBf/5uDgfNYeAZkm2IzNMDAwDk7fQqB1SssFovFMk85h5SBLFxDGQDniFmEmIdCKQBHbIQLYhbg6PgmVMOpUzwyt8smWywWi2Vec84oAw714NKM/jcHB+MhgqiEiAUA56lWxWFcxnN7v4DR0oFTPrYNN1gsFotlPnNOKAMqb2Bp+1VwaRaAENDVaAov7Ps3jJT2gwOIeYikwiAG45Fc9njmwpxzjiiuIoyrtneBxWKxWM4JzgllABAtiRe3XYGMW0BnYSUc4iKIyhgtHkAlnAAHE4JfKgOcMyHYWfWEqwE2HX0Qrx68X+/HOgYsFovFMp85Z5QBQIQKepovxKquN8F3C4hYVaxkGFel8GcyoTBGzAIAHGFcOeGYfxAVUYkmASg9wGoDFovFYpm/nBMdCBUO9XDl0vtQDsZACUUYl8F4jJBVpCIQa8EfsRCcM4RxBYwzOCdwHLsIksVisVjOJc4pZYCAghICh3ogxEEQl8A5QzkYQ8QCUU4I4RmoRpOywkB4CE4Enqog4DaB0GKxWCzzmlkJE3DOMTY2joGBIYRhdNoS8QihcIgLh3qgxEEQlcDAsG/4BYyXD+tyw6GpPXh+75cQxhVErCI9Bo3H0KhtMee8Js/AKgMWi8Vimb+cdc8AYxzPPvsCtmzZjmw2i+7uLrztbbeBkFNf6IcQodtQ4oISB2FcFiWFPECxOgzfLYiwQVxBJRwHAOExkG7/RgsOcc5QiSaR81r1GG2YwGKxWCznEmddGRgaGsbGjVvxC7/wXuTzOVQq1dOiCACQgpyDUheUuAjjsnbhTwVDaC8sBeNhqt9AFAeIeST+wcXW5nhiHuK1Q9/B1ct+Hr6b158rb4ENElgsFotlvnPWlYF9+w5g6dLFGB0dw8DAIJYsWTTttsJFn/63+pOx5AsluymhAAgoibQyUMh0Iu+1YaLSr3sERHGiDMQ80JZ+xAK4NJMKC3DO0DexBTsGfopLF74DgFgFUfxG5g4Y47JYLBaL5UQ4XQbxqXBWlQHOOSYmJrF3735kMj7Gxsbxyiuv49573wnHqb8YcRxjcrKYrCcgFYBisYRyuSK3IvB9D5QSREw0FYpYCHCKcjAJ382jK38hDo2/jKmWcZSCUUxWB/QxgrCKUrkEEpcRxRW4DkuFCoK4jCgOcGRsC1a23QYCgigOEccM5XJFVCWAIaKzfzMtFovFMn+glCKT8Y+/4VngrHsGPM/D6tUrcMcdt6BSqeBzn/syyuUympoKdds6joPW1mb97yAIMTo6jkIhn7qASquKmPgzZi4810e1NIGWXC+6W1Zg7+jTKLNBvHr0fjRluo3fMvgZD7lMBkEcw3N8EKj8AwIahQDhIIQjl8uCgMBxKBgocrksIkbBOYPnZBueb7E6hL3Dz2PtgjtT7ZItFovFYpkrnFVlgBCCZcuW4Mknn0UUxahWQxAihOt025vuE/V3QggoTf+Gcw5KXHAwUVVAXcQ8RM5rlbF+golKH8K4grHSIf27mEd69UImXf8MHISLsIMIVTD9HUCMMIE8tqwsaOTqKQYj2N7/KNb03DytwmCxWCwWy2xy1j0DS5cuRmdnB775ze8gCAJcffWVyGZPg5Akos8AuKgIyHqtoMRBc7YHvtsEh3gYLR1EbcqfWN6Yyc+lUsBCABScOrIvgRT+Qheo28ex4LK3AY5RugjMjZiRxWKxKOzcdH5x1pUBx6G4667bMTo6DsehaG1tOT1lhTrOLxYtumzRuxBERXQWVsJzsvDdHMZLR+p+x3SPAfHgxyxEEBWxZ+gZrOi8Hg71tGeAcZGYeCLti3Ub5GmVB61hWOYBdoK0nD+oOcs+6+cDZ73pkAgLOOjq6kB7e1udu/9U9w1CQAlF3m/HlUveg4Wtl4ASF56TRyWaACVp/YexGEFcFm2JmVjFsByOY2vfjzFR6ZOCXK5nwEMwFsnQQZysS3CMKgLVtGi6SgMm93++YV6XuVSFIbpLNh4TNzxC53JB6Vy7JxaL5cxzTi1UlEZ4CAgcUOLApSLhsOB3pLaKeYhXD30bL+77Miar/boPQcxCRHFFew44Z2AsQiWalK5/aI/BYHEPhov7phEgcUqIAEDMIrlAUuKR0NtzjsGp3eif2DYnJuRaoX06x7R78ClsOvLAjI59NjmmcsbNVtTnMrP/7FkslrPHOacMEFAQQpPkQwI4xNXKQD6TVgYYi1AJx7Fv5CUUq8NSUJd1p0LhAVBhglivhAiIToRRHGBH/6PYPfhUw/Go1sVmy+P9wy/g2T3/onMUaj0Le4eew87BJ0/3pTlplHA8Mr4RR8Y3njbhPFY+guHi3hkdW/x9LngSzoc2U+fDOVosFpNzaqEiBQFFyrIhBA71ARBk3ebUtqZgr4ST4DxGGJXBOcPuoWdQyHRKa1DkDIgcAPFv0co40iEEsR+eiier5EGVN0BAUI2LmKoOGV0M0wgvxBxqeSzTGvaPvAQCikWtlyVfnUIMXStDx9qGM91mWmxqcywslrOBejft23Z+cO55BoxyRAKq/3OpDwIC3y3AfLyZsaxxJRwH4xFCJtoYDxf3ohyMidUNeYxYLnusfhPLdQ0YZ/K7egEuPANc/iaW3RPVMafLI4j1d3MhVKBISizTceXaMMhMSa79MXIukA6lnPHrMXcu96yRXpXTcj6i3+85NP9YziznpGdAocIFlLhwnQwIofDdAgiIFl6Mx6KREPFQiSbBOEMYV6FaDYesAoAjiivom9iGjFtAEJfg0gxiHsoQABNeA3Cx51QLZakAKMFHRB4B47GedGutaq7LHJn0cswuahyMxyCEag9HMr6Ts9aPXWUx3Vj4GbdUUt4Iy7zEVBrPdOXH2TzWTDhd4xHzlX0PzhfOaWXAxKEZUOIg4xRACNECW+QEEGT9VlTCSQBAFItWxxxMhgw4KtEE1h/4OhzqIYhK6G5aLSoLIEIGsaoyIGpVQ64FJ+R/Kv7NZLkhGiQQ6jFxDsYZGKuCg8N38md1ohG5DGo8UolRVjznyYIQp3IMwysz898wgDinfOxjH4M31m14o7t1bqGsQZWfMReE24miynlByFlRpvsnt2NwchcuXfSOulVPZ4vTY0icvqd9rilMlnrOabWPSqFBCIEnlYHmbC8u7H0LMjJ3gMkEwYzbJJY8Bk9WNeRAEBf1/pisAoB0+8dclCKKUEGocwpiFogkRBbJ0kEukw9DqTjI8IKsSgDSSXFKgeA8xra+R/DS/q+Kz89C8lwYVzE4tVsqOkx7KcS4mE6IPJ5knEklgK7UONaAjvf9GeFYYYtjhzXmP0oROLkwwewneAJ9E1vxzJ7PIWbBWTneWOkQDo6+PCdCK0fGN2H30NM6nDdXqITjeGHfl/TS8Za5xzmsDBBQ4oBAhQmyoNRFU6YLq7tuRMYVayEo971HM4iZWM44iqtozy9Dzm9DEJW0K1tVBQDC+oik1c7BEbNIKxaqEoGBAdIDwKQScHRiC3YOPCH2oxMT05MIk8mJL+77CvYOv4BqOInEs5COnzeaeCMWiHGfxKQ8WenHEzv+FuVwXE9u+uxlEqXIj+CG0tJYMTje5FibD9AIJsMwZ5NjCvxzWQ9AkuNy8tf8BLpznqHS0Uo4gZHiAcQsOq37nY6ZJMKeLfontmP/8Itn/J3hnGOqOiyNo+MTxCUcGFmPIC6f0XFZTp5zWBkAHOoBEN0JXScDh3jwnBx8Jw9XrhOgSgZdJ4OJSj9e2vc1TAVDyPvt8J0cqoZnwGS8fATr938dkSw/FDkDsnIATHgZOEOsNXQOxiMUq8Moh2M6J6AhUkkYLu7FVHUwEcrGC54I43r2DD6DZ3b/8/T7Tx2qxishPR4qDwKGYEi6KSbC0vQcNNj7MY/NWJxKlpzZLk580j3dAscMoRxr/1rY1YxZl0jOEQFicqzW2WcC9Yw1DpcxDE7uQjkYNz47/r1kMpx13GfrNMGNJc2Pv62sLDoN15hxhv6J7SgFY8a+4xkp2TPhWPuJWYAndv4t+ia2zGxfSsnEmVEAj3ncs3C8udhA7UQ5p5UBl2Yhmg+JMIHn5uE7eWTcJuT9dgCQgjyGSzOIWBWjpQMoBaPwnKzMD2isDMQ8xGRlQJYWMh0iYIYrXa1poMIEMYtQjaYAmHkDqBMKqkKB8Uj2NUiSDc0tGQtRjaZSngXOOUrhKErhGDiPcWR8EyYrA6nv0w9tWpgz1RNBjj/5nUqUjPU4tHJyjO6Kx+JkOi/OvgCtf+GPZRk2PEdVInmMU2E8xkS5D1FcnW4Up1/JMSfsk7zOQlE6kXbdXIbS6olZiOf3fgH9k9vqj3Ksrp+pd+/0U/sOJX+fgTIAhpA1vqcnSswCPL/3i6nro97P4717M2P6c+LgOhQ6sz0xQ2mSezjFZ3emAvhseRbneyfZc1YZUCWGIlmFYEHrJbh5za/AdwtwnSyaMz1wiKc1dVf2IWA8RikYg0eFMlAJJ+EQt2FikFIkRJ5AJJIIWZyEEqDyD5LSwiAuyd+K34gQQ5jer/QiiF4DHAzJ/syXc6o6hEe2fgZT1YHU74OoJJWPGK8e/A8cGd9UM25l6de7hLlWPHjqxWVyglWKilkSOP10cayXg8vWzseLbdZ1YUiOO9PJ4AQtpSS5dKbu7unDN9OFUI43WYdxGY/t+CsMF/dNf9xTmHym92bIPJGTjDknnqMZ/wK1ipE5NqUUmzDj+U3viev3Wf0bxjNSK7xPRbFMKdBc5dfMjJkK0OOOgXNR0dRAaT8dTKekGQebudJhGB9q36eDuZCnca5wzioDJoQQZN0WtOWWghLRnnhByyVY0HqJzPhn8Jy8FPccYVyC62Th0ixKwSgc6oE0yGBX+QFKeNe2HlbC37SiladB5BWUAPCaRCfp6uOGElBTeaAmtYgFKAYjWplQnwdRUScAqt4I01H7Xar9su6LoJotmW59eZaco29iM/YMPZOecI3/rz+m+K24ZsemNgxRO4kfL1FqphZb+pinZlmZ45tOMNYKuLp9cI4orkqPUwMrlKtrcWLjDOMKBiZ3ImaBeD6jUuoZYDwGZuDGjuIAo6VDdYLtRL0KDKzhPTSTV2uHkpTy1p+78sLprp/6nWE127FT86Tz9D+SXiMz2enp8Vfo+afm/nEwufLqmfOiiflp5sm0KvcnUeRq7scZdLOfyDtyIl6ec43zQhkAaj0FwNL2K3HZonfqJEPPESEFhedk0NN8oegpIHsU1GL2DzD/VBO9sqRhxPKUMiASEAO5XTreKPabNDFSbvlYCWX5EiqrwJxMmVJAdJOjxpNtsn1U93voSdisImDJRGO4XzlE9va+4Re0hTyTF1pNDImiM9126vpNF7sMj/uy14ZXGm5jjvsEJoPEu2KatsYkOc1uji8wjVCTcX4jxX14af9Xk8St6fZvKHEmk5V+PLHzb1EKxzAVDOORbZ/FVHXI+N3MJs7x8mH8dPv/QSWarD3wCc2jjEWNQ3EylJL2UDXYrO7Z4PqZYsZ9SF9v8d5OVQewc+CJaRMNIxY0FFpyD8k58BjF6hB+uv2vUAyGj3G2ah+NBeGJot7xZOXVJMR4tsJpM65aUEr8Md/1M2Pln+i1EIbPGRnKnOa8UQZMVOgg4zbDc3IARLtis/7VpVk0Z7tBCYVLM9OECVhSgme4z9VDzQyrPJaJUqKJUdqrUBtjFU2JEi3anNSYPKbuaAiWmixjWUmgXfpQ25nWaiLKxUSYWJ1KgMQs1C2WxXGNvAUtLE1rlen8CZWENr11rZQjqVwgsURNCyH53zE8G1A5DPVu+hOdYJWSpazLaUauzzs5nhnGUXtQf2s09mNPiqntaqopSuEoDo1uQCSbYZnjMvMIGI/x3N4v4NDYa6k9MtlJU/wXYKo6JJ/N5BrMZCXN2MxnqXXDz1AhBIDh4l48uesf5fmY58LqrmbyfWINN5rok2fV7ORZfz4jxQN4/fD3klLiGvYPv4itfT9OKQvFYASbjz6ow31qDBGrYri49/jegWnCExzH98bUot5xnlLg5d9PW87AdN6eEwtHmLlU0x+tkeLWeLsTEvAz2DQdmjrJssx5rkCcl8qAIue3oj2/VHsGiBT8AIHn5oSCACoVhQaeAXCdOFgJpzA4tVMLcgDYO/w89g+/CPVycs5ld0Pxa1XJkBbUpsdBPJTlYFSsYggRUkgWTkosdvXrmIUI4zJUl0NlzSP1sJuWkBl64LpcMmahdAFL8acSsjgDDKEp9hcn49Fu3/oJyRTypnuzUahCj43zY5aI6RhxzYuo9lGNJlGtsTyPpyw0Evjmt+Z+xJ/JWGv3eWylYnrU86Luccgq2DX4FMrhBBiYDCEYz44c8sDkduwYeAycxxgtHdR13ck5y2dHKqrppC7Ic6h5JhueV6T3Z4a5knySmRHGFZSDUX2+IgQiFWFjvOlrw/X3iRBMQic6mdewmBsR81D2Cal/vjjnqIQT2HDwfoyVD+nPi9UhbDn6I9mTBIjiKirhRPq6TKMMJUKsgQIzg7VI1PWphBOyyih575h8B5hpZMxwf8f+nh3D2zO90ly/r1qvYYPro/Z1PEWeJ++eCqUqysEYBid3pZ7BGVd6nGICoBjLNHPKGQqBnE7Oa2XApRmxpLFcyIgSB4VMJwih8GgWlLhSQfAbewbARRkeOCJWwWSl3xCaYm2DyeoAhIs/lEIwie8zHskHmxkTiJpgQ/2ilcJRHBhZD1XCqEIRTLrIxVoHiXUe8zDxILAYjMU11l/yUArBkIQjYhlrjFigtwvjMl7Y9yWMlQ+DcbGg02RlQHo0mB5Xo7BFLSJsoqxHpi1VEyFc0opDMva04Odgui20+ZnKx3j5wLew5ehDMJWeoand2DnwOFKTk5xgTuR1VeWhKr7d2BMyjcfDTOJsYC1qG0mGOIrVYazf/zVMVQYRREU8s+dzGCsd0luqypOBqV3YO/Q8zNCOOV6d8MpDeY/TwludS8wCoRjWeIEUiUeMpTxIjbY9FqZbW4XAlPKZKJzGNUyFrKAVBn30aZ6/Rr0jhIctmjahL+ahbCwW6PskFI1Eke2f3I5dA0/KY4j3xvQ0pBRPfnwlcDrM67Bz4Em8tP9rqbVS9FYyrIa6s51+z8f7Pm6Qf2DOVzM6irrHML2djTFzoOqSRPUzYORTGRwd34zn932pZr5LjKxjjDA5rm6ydmyjoW4P05zTfMlBOK+VAb1mAQg8JwOXZtHkd4ESB76bhysVBJc2zhkA5yJRx3AvKe2/HI5hstKvNsTrh7+H4ak9euLh4JiqDKAcjkJZ4vpFQaJQKCIWIIwrqEZFbdEp60xZNkFcQsQCmVsg+wVAhArGy0ew4eD92qIR402WZpaiQrs5lUBmXCzTPDC5U3ocGPaPvITn935JWoGJhyPpSzB9rbNaAlpPYjxRkBRCuJsTfzIZqclAWaO6KsO4VpwnykwQl1CNJlPf909ux67BJ/WEkShhx3YrA2piSQv45DdxzVa1E1lSismmUc5Q86kSkNVoUoduYhZgeGqvfBZMiz/Slm7jXIYkwSzWngGebM9jBHEFjMfY0f9TvLD3S8k4jH1xLpXgBsqamU8yE5he38O8XslVNsNuR8c346V9X5HHk70EjAREZggIMwEx2Se00ibeD+H9qlVGFTGL5DueeOmSvBkZ/pONylLnX+fpEu/FTKzoYwke5XmLWTXxDBnCUb+HJ5BAeGzlZPpvk3s/vQBMe+C4fO7T96P+aGYFVvJ+pgwYFhlPSo2CJ889NR/o+fQYuSd6v8Z94tPPAyfK2ep5cSqct8qAyg8QvQiArNeK61b+IrqaV8MhLnynAId6UmEQZYe1JEKcgxIv1USnf2IbxsqH9bYTlT5MVYeMCYtj09EHsXPgCf1v5ZLjXK2QmDy8EQtwYPRlbDh4vyFMzeYqQBCXEbGKjt0LwSAm06GpPdgx8FPZDIlDvWymi3+kuE+/iEKYqnOsprT1iFURsYox4XLp7h2D8oKo66Kuk0Jlget+BZwhiMp1+QnK4ts1+CSGpvakBMVkdRCPbv9LlIJRuX1NeRWEVyS5lumwiBnrTizrWB+3kSA3fq6VP2W5a5e23pfZpMlUFKadWus/UQIeKlQ0rkNHjMs8C8OCUcdTHiVtaTf0DPBkO85QjUT8WwhHsSz3WPkwSsGI2Lf0MpmYXgWWek7TuQwNz9aw5mOd5Ggqc0nDK/N6TlUH0T+5A7VVKIkXQb0Pkb5PSWjMnNS5PjbjsU7krbUC1TMfy/dJ7CdOXdfYvK+cy9LiBhU6NcKtwVWZ9plTXgU19HQFk9nLJEnIPX6Pj2M84/q4yXM9vVJwLAs/MBS1JAdE/H+tp8b0ikZaEVRjTTZNh1CTcSjPjZqDa/Owju+TUdURqeqMY7j+GxHGFewffgmBVNTN45vjnIuct8qAwnV8HRZoyy1BzmuD62TgOmItA0pdONMkEEIKQYDDoW5K+2vUdlOsiqgeYoYorugYPeMRSsFYqvuf6fqOWYggKsomQ+rlDxPLDmZYQAncUL9cE5WjifWo/lNrJfAQEavi1YPfxuDULig3MbjQ5iMpINQ5x0w1WkoE+lj5MHb0P4YwKuH1w99H/+Q2DEzuTCZm9WueVBEo4fTi/q/IFqrJhKCE2YHRV7DpyANJKANiBcnR0iHtjq1NgoSyluTEEUvPQiKYmbzew6LtshFnTazR4wvuZJo0LdS0h0J9FkoFJP27Y2GcDyC7VhouWy1QObjxrKiSweT6stQeVUw7uaeieUwYl7Fn6FlUoyLCuJIoAnVnLWCGSzVRdLgh3JNrUOu1iXmoc2dMxSV1j+TfdRiGp5XXJL4sx2CMTfXvYGiQOGtsqLwSiVu/xgLlKrQnPGzJeJn2BphKknre6mPPhiTnvOH3DMevqkkLfXNJdSVYzTyk4yckJuGO47vP6/J/jHs2HcJLGeh7aBou9UeRyiyPEPNAHUE+w7V9JqK6/STCNl1dMRPMecfcl/xyBu9qQiWcwAv7/g1j5SOoVXC1gn4C+zubnNfKACFEVwo41Ifn5NDTfCGuWf4BZNwmONSHQz2RMzDNSluHxl4D4zEc4qZchFFcgUP91LbVcDJlHSiXPqRbfsvRhxDGFf1SpK2UQJY6pfsAAIa1AqZflETDZigGwzgw8nIiHDhHEJWStRV4jCCawlj5MA6MrNcTrLK2TA2fc6WEmMcUwiDmIYK4hH3Dz2Nr3yNYv/9rsvQsLRDEGGVeA49QrA5J5cmwbpC47qOa8sHk/NQiUZHU6uX3YMbkkLZqVXOomEV4af/X8OCmT+LAyMvGvTuxlz9toShlQk36ifA3xxjGFV0aqBUPbVzKa6An90ha71PgnGGkuF8fK/GyJG527Q1ConCZY1XejyiuaO9NzEOUw3G8eug7KAWjogVwOJ5WXmqEgVJa1bOYXDXjb4ZSVnvNtHBlIcANxU2Gd8w8Gm2Fq9AGkiXDRQhlKnkvkCgQrMZKr43dqpCYUga04Odcfw9wXb0DpBMn1b9NhPKbVuR0uKHmftRek9pkOMVE5SgOjr6if6vOVVUoaQGoha3puZrGEuVppbTera/3oOeCuh0AybM3jbeD8WRF15TVbTzv5h4TjxXXhkYcG54boKESoj7TPVHM6zwDxcD09tTl0Ezz+9rrxaXHToRWK6nnPvGyMOwdehbDU3uPO6azzXmtDAAiiRCESA+Ah6ZMN1Z0XKdDBM2ZHuS8NjRe01ZYbIzHcKinX1JATPgZtym1rekZUBN3FFfQP7kde4dfwKGxDTp3gNUIwJiHiONqYpErIQCuLSGOxAJVFhjnHIdGX8VUdQjKPay8ClFcRd/4FpTDcRSDEQRx0RD6KpGLa5ewGDdLuVdHSwdRlhnrMQsxVR0UCzKNbwJjYtGnUjCK3UNPY6S4DyphUTsLjZCIsvgSq5Drl1S9oGFUxsbDP5Cx2lDvAzAmFp78XnlCFFyWWsYsRDkYQzEYkRn3HObkWGvhJxYrw0SlXy7kJF7usdJhbOv7sWwdrMIMMCYJri12ANhy9CFsPfpwcswaQRtJhdDMBRAVERxDxT16dIlAUtUbXCaPxvr36lkYLu5FsTqkr6++p5zLuHkAlQy4pe9hVMIJrdSo62Ze4yTOzrVAQM25KIUEdZMmEETCG6EScJOESiNBUIfBmByzWQGRWHFK6Ou+GrJ3wfoDX9N5O4mQUM8F0++C2fKZcyZCYDyx/s1+A2bipLj26RDUZHUAB0Zflgp0qPfZ6D7XMt33h8c2YvPRh4z3Xob2mPIMJsqxeX3U83ms4ykBq+5X7TlNO2a977RCYaLeZ5GIKt55nbTZQEk0lV+1312DT2L30DM6V0YZHUlcPxHEgKmkJkKcQSVJ1yoQaUGu/twx8BjGy0eMc5pO2alVHDiUly42lMLaa7K9/zEcHnsdW/seSa27MdtYZcDJgIDCqwkFEFBQ4uC6Fb+I5Z3XNEwgzHqtIpGHxaDEE9nckzswUtyP0dJBZN0mmEpEGFeMEiLx4PdNbsP6/d/A1r4foRqVEsuo5sGNmcr+lla4nMABoByOi8YtSuhAZPlHcRWQk5tQbqh2DTMuytU2HnkAY6VDKFaHdWzddAEmL3PywifhhwDr938D/RNbAYhJ85UD30qaKYHJpUv/Dc/u+Tz6JrYC4AhiMwky1gIh5slEy5FYZGapVMQCUcJpaPJiXAxhXMHg1C6xfyOebbo5Rbw91l4M8ZmyXpJcBfN8E+tIHOflA9/ArsEnoWqtJyv92D34DCJWlQIu7UI3Y/WAUAqrUVErROlJimvBrBUdDgRyTYvUM2EobmriNV3HyTUI8dL+r2oPCGDkTch7rK9RjeKj3beGwqnHCOW6T8IUddZQarxJXsjuoafw/N4vaoFm+hWSMID4VFwD5aGKhEeNRzgythHF6jDM3BU1bsZjDE3twfN7v4jDY6+BIcaeoWdwZGyTPE+mLevapY6VR02FQiJWlXkDSmFNkhXTYQJgaGoPtvb9WIZhEsVdKWCp6g4jJAKeFjzJf4lnTnmQkjlAhb+SpDyVwKmf2RovRZr0s6cFqpF8mJwDr/sluFAcxNxkhFq0Ii8E95ajD2Foao9QPJXbnyslLl1OmOTFiHszNLUHw8W9Ke9MoqQm3hGVR8JYJMJDhndgOg+VHq/5foPj6PhmPLf380jCUkkYspZaZVCFydR4kpyVWL9LgEgwf/3w91AKRhrudzawyoDsIUCII/8Uwlu1LfbdAnyn0FAZyHmteoJwqItKNIlXD30bB0ZfQTEYRsZtTikYzCgXVCgLohKOy4mu0lAbVy+cetDUSwhwvH74u3hp/9dweOy1VJa4mcTXkV8OSt2kZJFz/aAngrEmvgoxGYTShS8+Z8YDH4PVJP6FLG1l7R56BofGXoMZy1PjV2ECxkLEcYBSdcTwjER68jQtQWU5JEIjCSdMBYN4cuffY7x8VCpGofRiqOsgz1kqUkFUSvZpWHuJCxapYyoiVpHCMBFeSkg9u+fz2DXwlL526lyEN8S0LhM3aMpVKyc4VS6pJrVqA2VAeaJMC1tcvxiJK1nE58O4LBQApqxdlcMgrWR5PDP5Sh2bg2N7/0/w+uHvJh4EJYjkeaiwgwpzqefBPD/lOeFgqIaTqIQTKQVQCzPjnifXyEg+ZcKrtf7A1zE4tQuJAK2PS4+U9mOouEc+i0/rVfaUS1pci1ALGHNy1wmELMBkuQ+VcMJQpsV25nMCLlqZmyV/OpTFQqyXClkSrkiEpimQTAs5ZklzJ8YjlMIRVEOhTIt7liTn1XrSlABqZBGr+2t+p989lQ8hwzbKqk4rFKaHJdDVC2oT5QEYmNyJ3YNPYaS0PzmGFM7pnA6uj62eY9MYkFfFELTM+F3iHYtkGajZyKzRuZvnIOaKZJ+Mx6jqNt3SIzWtMpBWMrRHTs476WMl81Yo30UzvDnbnPfKgEN9UAglgAh/AABoxcChPih1QUHhOTl0FVZpAZ9xm+TfuSFoI0xWBlDwO9HZtCIVXai1DEzUxBnE5YYPcSxLCxP3b1ISFcZl7Bt+AbsGn5L19QQgxKh0cLC84xoQkJT1qhrPxNKdnx5PhPHyEVSjKYwUDyRf8MSCr213a7rz1b9F+ZvKvDYsSCUo5SQ3Ue3HT3f8FUZLosGL8Fr8AMVgRFtXye+UdZq8pIzHMg8i0NelGk0h5kHKRais5kgKSPPaJxN72mIyc0E4VAJlInCFgiSaPU2U+3TYRHelNJLg1HOQLHttjiuS1pay1BMr1+x4Zz4TQVzCM3v+BYOTO+VYVS6GEOrC7V1FFAfSopYCMK4a44+1kFfCSY1JhXAGp3Zj58AT2DP0TLJ/bliTPMKhsdfw8oFvGAKV68mvFI5iuLhPCyw1ySdKBZP7VXkgiWVXjSaxZ+hp9E9sS1vbPMZo8YB06Splr77yQVnXZh4EgJTlr+66WW6rnocgLmH9gW/q0JdpjdfmDISy5C/p/Jk8N1PVIZEIypNEwOQxS5QDFZZQSoCaN2Ie4dnd/4IDo+vBwHTyMpfKhlAyVSKsqWAZCqfxvJn3Wn0GmdsAAEPFPXh2z+dTCkESzxfjVZVFDMnvxHHEddze/xjK4bhO8uU8RhiXcWjsNRmKYqnnLeXBM5RCM/Sl3mF12fTvePKuMiae6cPjr2vru9bI0gaFuobc9KQoj16UKlWshJMYLR005oN0+EE9OxuP/AAHRpI8D/Mac3CZU3DsVutnm/NeGci4zWjLLxZrFBAKSM+AUA0cOMSBQzwQQtGc7cUli+4CpZ78bZPONXBpRlr5DFPVAXQUVqAtvxTEuMS15YK1COE51VBhiLnIL1AT4cDkTuwZetb8NapREdWoCEd6NUKtWIicCICgEk7g2T2fx3j5iOH6TEIOal9hXMHLB76Fbf2PCBefJIhLKFWHtWWSdnuKl1I0cHKF4DSUjHRtPdPCJmYR9g+/iMnKAEqyG13MIlGqWOMZSKzldKx8otKnO9kpyyGQSXemiz4RwomwMRMR0xaAOTGYYZJIWyCmtaIazqRixVI4qaZAqnKj1vKNuVD2hPIS6nMTk1qYrEVgXkEeoX9iGybKR+R1S9z9erErHov9qrFJQSkWs0rcrjoMYJxr4pKPUQnHUQpGMDi5CyoBVAs0KRyDqKhzKZTLFOAYKx3EEzv+Dq8evF8/L2b8mxtKhfJo7Rp6Ctv6fgLOGXYNPoX1B76hLXxTMOwcfAJ7hp7XioUZptHvjrx+gamYyoRcADg6vklWaxgC1AjDiftaMTxViRVfq3gEUVErisoLZSb8ao8ST3duVAKOKU8AmH6WlOLGWIQgLkF59nRJpFY2k66ko8WDKMpS5lrPFgdDxAPtCdDXias6+1CeSwkTlT5ELMDGIz/A7qFnjOdQPiNMeJ5q6/jViqTqnRelyOK+i86O/47JSn/qXpnhIlORTIcpkhwJtZUZqlLJqUw21Xp5/zcwMLlTzykcDIdGX8PR8c36voyVD2HDof8wEqqZfm/EOyme42o0hc1HH8Qzu/9FK5Dm3BKzEIfHX4fq/jlePqwNk2RJbXF/griMRityzibnvTLQWViO2y76TfhOAcKMl2EC6goBSojwEoDq/ynPQN5vh0ezuGTB29GU6dYafcxC5PxWOHIRJIVyxZJpLjuHaDWqXtC0IhEhZBX9chSrw5ioHE39PmIVVMIJuE4WvpOX33Oh1hAXgCh5nKwMiAZFcVmPN2Jh6nhqct058ERqieRqNCW7Kpqx7eQ3nDPZrMmV1k16wlOad9KwSLyg4kVkKIWjCOMKthx9CJPyuKa7XAlJ8CSezsHwyoFv4uDoBrkvNUkmLnw1mQgFptaaq0jtP11DriZvlcmvxiISI8tJopkU3spVrpQ+LhOsYh5jcHIXntvzrygFI1qpS0qkktiwaiajhK2w2kPUxrUBoBSM46X9X0U5HJdKZDLhJa2hI51sqWLtgEpmTazcJASVuI7F/YwQxRUdplDPcDLJJbHQVHdMHuuJvhgMY3BqVyqpSlyvapK8KT+fqg7i4NgGjBYPYrR0UFw/FiQLfEmLOIqrchyBfoa2DzyGx7b/JY6MpZfsZtIaVcJvaGo3ntv7BRyW6zYcGnsNk5UBLdCUZa1yaOI4SJ0fAK1IpcIE4FIBZ1AJirXdMdPXLS34tDXKIlTDKUxVh6QiovIPQq18qOdN/R3yriklZGvfj3Fw7FUt2BKXuNiQsShpQCYTAJipqMrrLu5jgKGpPUYfkuSdjFiY8uboa45EaRPnnSzKJpTSGEfHN+HZPf9qhNzMUB3TSkziHWD63oyVDyU5PyqpUysKyntnzC2cQbVo3zv8LPYOPw8uBf54+SgOjW7QY1TXrBSMaQWSSQG/5ehDYDxEEJUMY0jcxzCuYPfAU/p+D8pOp5VoAkFUxGRlUCvr5nMyV3BnewCzDSEULhElgBTJMsUEFL5cxIgSqr0GysIGgJzXhmuWvx9t+cXY3v+oTqrKeW1Y3Ha5eLAJ0V6kmIWyd4HTcHIHCKphIngyXrO0NCJt9RBQpF3aCYzFUhnIoMnv0s16CKHS80F0aCCKq9g18JTuCMh4DNfJpFyPAvGCEeKkNP8kJslSn3EIz4CaTMzWrDELdYzM7JtvUg5GEbMQ+0de1AIosaqidPcxHqNvYivGK0eEgJAvvQp56Bgqkvp66NhyQhiXUI2mUAyGkXVboRJ/hJvzUQxN7cb1Kz+cGksYV1GNpuA7BS10RRJmmAgBaelUwjG8eujb4FyUdIrnwAXjkVwamxpWAhGTGjPLRpO4qUkpGNZ5D+JaiQldewakkNX32MhZCaJiEnOWYYJEKBk5DJwh1u582XGPhdpaUsJd55KwWFo9wpJlNE7uh+k25zHGyocwWj4or4UQHKOlg9je/yhyXqu+FiacM2zt+zFyXisS9yrXFqfZ6EuRhB3EdT089rpWBOROoZpTMR6DgCaJbkjyK8zz1olrNX0GisEIXOpjstKPvUPP4ZKFb4dLfS3QIpZ4jbTnSUloEP1u7x58CgOTO3DDqv8MVYkiwg9JrDw0lIGUwJReuzAqaQWacRdUxfN54qlKh/WkkJLPQMiURyLW2+uQm95Xcj5Joh3Rz5p63tS8oqp7OGKMlY9gvHwUjMdwCNXXQ10TlfOinhcit6lGU3huz7/iTat+CZ2FlYkiohIQtRGgqixkrwPGwUgk8zACBHEZrx/+Lg6Nvir7rqTzkTYdeQCuk8GlC++C8tIl74RRuaHuI2epHIChqd0YKe7Hjv6fYnH7Feif2IYbVv1nce8itXx94+6Xs8F57xlIIR0DyXLHKqFQ/N0hrkgKlMmEnpPF4rbL0ZxdgIzbgpiF8GgGFy+4EwW/Ey7NwCGecQAOl/pY030zWrIL04cmDnwnL61Q8XDlvBY4MiQR8xBBVNbaM2vwEHEwlMNxUFDk/FaUwjGAC88Aoa7OGeBg2DnwOEaNpB7GotQaDLU10R7N1BxLCloj3qUUlLbcEukZSGdqD07twqHRV8E5RyUcr3PnAkApGEUxGEq9JMoSjWVWNwy33ODUThwd35xY5DCSwLR7NsJEuU+v5lirDDAuJqaX9n8NxWBEuzcZj1AJJ1AOx7C9/zEMTu00kpqSVtBRLDo0buv/iS6B0pYJV/0kxIQUxVXDHZuEPLT7U3sTQh1PnKq5HorQqGUOtJcn0pOoUmiUeztiQRImiMuGJZQkSZkCm0mvBJOhHADon9iOF/d9RcY8oQWACkUwmeCXxPXV2gXqfFV+QDJZQyVOyrwYFRoxa/b1s8BF9YZoFpV4mWCEGmqZqg6gFAxrYVG37DIST5hqxmTmDCjPj/IqJRZ47boMSZJjMRjGobFXZd6KKqUz2mhrpUsJEmU1h6hGU6hGU/p5jZhQPPcOPa8VcqFYqF4VXHsOtAeNRdg3/AKGp/bWKR7q2gdRCXuHX9AeAq6Ek/S2xbHyFoX6+pi9M2rzS8xrtn/kJWzt+7FWBnSFkX5/GIrVYXFew8/VXQ/Ia5x4NlRoJUQpGEU1KmrlLZJNioaKezFZ7dfzTtKiWVVJJTlKKmdoaGovisGwVniU64RzhnI4pnvDJF7MJC/FrMDZ1v8I9o28kHoGYyYSs6eCIVTCcVTCCfF8c4ZqXJLv3fE7dp4trDJgQOT/Gn0uqgtcZL0WUKkMUOLAdbJwiAfXySBiAVwni56Wi/S6B56TTe1rQeslWNt7Ozryy1JHEAmKWYSsol+4rNeKrqbV0hsQa7e0KgusHaVLfZTDcekJcIWLC0x7BqCUAS46BuoXVGrr5hoM6dwGAqdWGeCm8FKfiRDI8s5r5PHSmu/Q1G7sG34BHLzxGvYASsEYNh35YcqjwDnDq4e+jc1HHzKSBsVy0EFclnG9UAtQ3RhG/i+ISnhi599i/8iLUK7C9LmIbP0oriKIi7JWOdYVDbGcWAcmd+jv1PUZLR3ErsGnAYgST8aU5aPCL7K5j7Q+I66ETYRqOInx8hE9aalrCkAL35gF2HLkoYbVBGa4I4zKqSqEJDYfCQtRh4MC8RzEFSOpS4ZGlEBNXC/gSCxEQOSMjMlYKOcMOwZ+ioOjG/DM7n/W55KEJxJhqp4pNVmmk83E5P3qoe9g5+ATULkPZoKoec5CEJhd8VTcuHE+zuGxjeiTq34yHqMa1ioDQhmqRBN4fu8XkrwIFsIhnrYChWJkJMMaHpPU3jgT94NFKAWj4lnmyktjtnGOtKdAhUpGivvw7O7PJYmzWkmp4sj4xlQnTtXFUfUr2T7wU6nMin2XwlGUw/GUB8CMW5eCEWw4+C3di0G716WnKH3ekVbm1PHVuQKQOQ4VfQ8Gp3ZjrHRIPzfqOh0Z36gNnmIwjDAuYe/Qc1rom8bFWPmwGJsuDWTYeOQBbD3647RCLruubjn6MI6Ob4ZSrBPPQLrpkcoHqUZTyaqeULk9PLH+ZXimWB3G0Ykter6MWYDBqV3om9gqErJZBUfHN6FvfGt6PkRSnl2NigjiIvaNvIAwLmnPQNTQQzw7WGVgJkgvAaUOHOLq2Dqlxt+JcP2rkkQqEw8zXrrxUM5rlUpERv8u6zaJcIWT1Zo5AOS9Vqxb+A4sab9SdzNknGHP0DM4NLohtV/VMKkcjIljU08LC6XMEEJ1opjJZGUAQTQlei6QxDMg9iu8BWJ9BiSfEVL38ioXnVrtkUsL13fyqW2Eq68ISuqjVEFcrEuWY5xjtHQQE5U+xEyVp0nrKK5KqzQRHvtH1qMYDGuBEbEA5XBC1KQbyV/J/sVCQIzH2t3OwXF0YrOoUZdCdnv/Y2JCNqyVICqhEo4BgLb+mUx+PDqxGS/u+4rMA5HCQOYLjJYO4tm9/4oX9n1Z55qoBChAxk/l2IT1SequV2TUgyulQ8S3eao5jpmsxHiErNeMkFV1y2wzF8NU8HRcOK6k3J9qggU4Do6+giNjGzFR6derdmqPh7QAE6UzsRxrOzcGUREjxX0oVoeh+kCYyYzm8yOEToNa+Gnjr0lNfhhXUDEUK0IccACx7BEyWjygn52IVZHxmnQpqPL2qAleW9K1R+Nce15e2v9VDEzu0NdXvN/csHqFoBV9J2KUQtEIK4xKiYdCXr9AlpCqYyjPwGS1H8/u/Tz2DQlBw4ww4sDkTrGkr0qW5ZFOaI2ldTsVDBlelsgQmMnCPcn15eokIazxPUbyaqAt/yR50exdIBab2jXwJETuh3r+1F1Kl+kNTYlVXxNFmcl5oB/qWTUVQdProj5Tiqlp3at27GFc1kaV8mApL4hS5jliDEzuxMv7v47dg08B4CiFY3h+7xfx/N4voByOy3yUKJVblH4ehLERxhVs63tEVFfIEN2xEsrPNlYZmAGq6JDI8sKlHVcZAlaEFFSCHiWObljkUBdL2q5EU6Zb78uhHohaCREULbmFyHqteqlklXcAAI6Tge8WcOnCu5DzWkS3RHAMTu2qE5gu9ZHz2xDEJVDipgQHkTkPRDUdqnG99k1sQ9/ENj0mAKIch3PpLSDy2IK81wYqcwjMl1c1iqHEESWaXFi4ZltmYZlNoRpNoinTVZdMGcbllMUrf4WIVTBWOizqf6HKyFQGeUn8Tk5eQ1O78czuz6FfWoPK9Xdw9BUcHns9pcCocQt3YIw9g89oQTdePio7Koo+AVPVQUxW+qAS9MT5CLdxKtmTR9g1+CReP/x9TFb6kzwMmfymyqvGSodlVn8krYvNGJjcYdTqc+01IA2UAfM6qclQJ/jxZKGmSB5fud7zXjsAjrJc6EknayHtaleKUxhXAJ62eMxjK0/BaOlgSqFRYzST5sxuk8k2DFv6fiQTRpP7Ol0XNyBpTSwHqi3H6WBy8h2Y3IGx0kH9uQp/BXEJuwaeRMgq0ktUBmMxsl6r9DiJ1QKT3Il0kqTCpRlQ6uoVPpW7X4XVdIwdKjQSYbi4F0/v/r+YrPSjJD0CQVxCKRjF+gPfQFGWxglFUiYQItb5ImFcxnj5aBI20B33gL3Dz2LP0LOGO1+8s4fHNmLHwE/BeIyx0mGZuBphW/8jGJzchWpcwlR1UD8buscDj7Fz8EnsGxFriUzI/gtJ8p54DivRZOqeJK56jr6JLalnhHORTDgmy4rV9upaKS9AzKoy5yZRLsXvVR5HqJ8HlWOlc43UPlTujM7rSRRfXb6tkyMDrQiVglEcHntdj1c/o0YlkmqLXcuxPKFzI0AgmLUEwjAMsWPHbqxYsQyFQv74P5hFCAgoFda+52Sxpvsm7B58OjU5C7c4pCB2dAXCRb23o1gdxlR1EAC0YGzLL8Yblv4selvWYsPB+1EOx+A6GZjlPg5xQXSrZB+FTJdo1NLAGsl5bfCcnBivVESS8VNQCCWlketVWExlafELoXZwdAMocZH3mxDGlZQy4FAPJBaeAVNICAGZJFwqy8n3cnqbICpi05EfImRl+G4hlWAJoE7JAaQVFFdRCsZwcPRlwziR1oh2RQZaeIRxWXoSEjfcWPkwBqd21bmTmXTjARyjpQPaEjHDKGoyK+va6MQNDyCVfMm4aIcbREW4NGuUeKomU2qCFJNJKJsYbT76IIKojDet+ogem2qvSgiFQ11EhryrLU1TuQqcM/RPbINabU9lXavJK+M2IWQVLWTMuGza25N4M1yagefmUApGdUw/uWdlOdYQhFMwHqN/cgcICDoKyw2LjSGIi8iQppp7wHF0PKkAUCEaKq32RqR7TETaIp0OtT5AEKcnZdfJIohLou5d5hJUQ9EhkhCKjNskW4Mrt7xpQdeHCcT7zVN9IXS5J5ISTuWpIaCoRkWMlQ7hqV3/V7w3UlmMmGgXrt5J891Qa4mIPKAkPi6uH9PXLYoDHT8318Tom9iK4eJeAEAlmtCW//DUPmTcZkxVB3Fo9FXtTUzWIGDYM/hMEk5U5yTDTP2TO+A5WXHPqV/3njS8NzzCxsM/QEtuAS5Z+HZ5Dtx4JkMtpMO4lHiuVMhDjiRJ5uTyfXpCKvky14ErD4BSCoxnXebNxIaXLtU8qkF+E+MRjoy9jiXtb0jKPlnj82ysDNQnM88ms+IZ4Jxj/frX8NWv3o/BwaHZGMKJQYioNJAvgLACiVYAxCZC+Iq2v450lTtyEk8sYxWXX9J2JZZ3XgvfyWnlwaUZHfcW+3T0bxa3XYElbZdDCRv1ecZtBkCwpudmrQw41E0lLibJkFS7SxvhuwX0Nl8k11TgIAAybkF4BmrCBIBI9jMVC6XJq/NW2rp5/uVwDEPFPaiEk/CdfF2Ghkq2M+G6XJDL2G/SKcx8SWtf2MNjryflU3qM6TwH9VnSgIgB0gJSL7CpDOhGKUgaugBcL3gFqLItZaWZHRw5IiP+rvatutbFLJIJiMpiSdzRDT0DzPQMMLkuhRjrlqMPQZXdmc2VYhYK5ZJ4OstflYEp70UqO1paqYVMF1Z0Xq/HXHutKfGgs+FZiK1HH8ah0Q3ashT749rzo65BIdMFj6bzasxkPUxj7Vfjos5JGCkdwP6Rl+o8PiaNs7aJzuk5PCqa4ABAJZpCKRiFQz34Tk4rZGZvAtWDoVbIKc+gOfkn60Vw41qofgVJC/FqNIlKOC5CO5EZwon0MdXf1cqSjvImGueu8lPU7yOZ2Dk0tQt941sQsSqK1WTejVmIiUofRkoHwHmMcjCKbf0/0RbyaOkAuLxnEQtQCkZqlLHE27Fr8AlZWZWUBR6PKK6gFI7WNVxTzzfnHOVwHMNTe1OrwSbhOp5e/AscFZkPkIR31LNcRsQCFKvD2HL0YYgVEsXbaa53opJwlbejEYzH2Nb/E4wU98uKm6BublE0ahoGfeS5wVlXBjjnGBgYxJYt27BixdLpvIBzDkIcGQpQTYmQUgYcmngGQFTCoQgXuIYw9BwhNITnwAHROQYuPJqRpYSJZ0AkBmawuutGLG67EqopCwCs7Lwei1rXiZg+9Y1QhasFsKPbLYv+CCpWlYwnq63+rNuEy5e8G82ZHnXSyPltIKAp5cKhHrh0u5svuxq3CpMAYtIT559uy1yNJoVnwIASFzAsApNk0k1PFuYkX+v1SCfniGBPbZdEse84tYrgodFX8crBb2H/yEtyv4nVWQkna8IE4nORbyFeJxGPTMq+xitHoYIVokzNvGYRKuGUbgAF7bLncnvh+VBJoSaRce4cDGFU0smipkBXnh0l0FROR+JWTbr2qclV7FMIlYlKn+ypkUmOVnOPMm5eN5pSQqsUjiFVm89DTFb6YZbItWQXwHPTnkFVoz9dDbZDffmeJOWOIuFueutTlcyZEEA/+2bSpUgkLYlqIOppz4Do5JgojUx6MNJj80Cpg2J1WH8Wy1p8Ve2jXPpK0NR6w7j01M0E18kgioPUudWGTFQC4MGxV7F76GmdPKdgLMKuwSfw+uHvIWJhzXLbDAdHN0D1rwjjMqqxKHdWeQMxS4erVNtmFSc/HpVwEtVwCpPlPkwZSoo5hv6JbWJ8xv6SygqOnQNPYNzouyKeM5UnIc6/b2IrXpXNhSJW1WucqKsWxZVUkyjV36M6jYtfEbFqnQFTy3Req/O6z0AYRvjJT57EzTe/Cc8++9Ixt2WMI44jrTBEkaz7jWIEgbJCAUqFpX7m4LqCgDEmxkNEh8I4VnFLB5R4yHsdIJzKHAMXnHEtmEXznwwYZ3DggcqWwWqid2gGY6XD+sV2qA9wAs4JOOegkJOtfPk8J59MApxoT4KZM+A5WRA4ABe5A3GcuNIBwHebRHwsqIrJnEF3YQSAjNMMKr0WLs0iYlX4TgFdhdXom9iSesgZj+RxSSqGTqknmzarqykygFUtucJzsrL0TlQlcDCdp1Hr3gUg3XoRWrILZEigVhgk51nwO9GWW1TXNRGAdtU3Z3oRxCUMTu7GwNSO5LyM+nzlGRCCIBGiDpEhFq66sknLjsc4MPKyHsuRsU2pCY1xhvHyEbRkF8jJjeDg6CtC6ZCCQn1OaaJ8quutqIQTGJjcqfNK1GSvkiJ9Ny+txBCekwMB0WOPmXEPDdcplxbXSPEAOgsr9PMlSqvS19Bz8rpbWxSHiGXPizCq6PsSxhXsHnwGrbkl+veUuHUVPMIbgmljsFm3BcUg7VGM4qBuTCaqgVEaAjfllZChHC6a/rg0A0pcnY1ueq3iONAVAyYuzaA5uwwHR18xxlZFFIt9TFb6sfXoj5D1mtGaW4zVXTc1FPzHyn8wcYiPKptqcG4JMQ8xVj6CsdJBxCxCEFVSinL/5HY0ZbpRqo6AEEd2PEyOrzwgnHPEcahb/aq5R5SsJn0cKuGErKqZWQ29WhdlvHIUw1N7UfA7wZAkQaok1NDImQCkks7Es6gqWRTj5aP6+R6Y3CHzf4YwXj46rbciYmr5b67DBGOlw2Ktk2Og8i1OBuEdOrPSa6acVWVAhAc2IJ/Pobe3B1EUo1KpII4ZHKfeScEYQ6mUTJxxLDO4g0D/HQA8z5UKwZkbN5dCOQxDRJFI6IpjjjAUD3xnbg3ecsHvoCWzCGFURgQOxgnCOAa4msQJwF1EIQenMShxEYUxwKUyQDLgYEkTFkYRRTEoZ4jiGEzFvNQExB0wpq4VB4W03jnRx3RpVsTDIgZwgoGpJLPYc/K4qOcO7Bp4Ap6TR9ZtFwoXT1oy59x25P1OLGp5A1qyS7Dp6HfhkAyas704PG40boEQJNRxwWIO0+nkEOUZSCYsh3rwaXPq9w7xwYhw4/c0rUXf5Ga4NINFrZdj38hzdfcljMRL2J5ZZuQHNJ4URUilFZOVPmlpO1CJZxPlPsQ8xMqOG3Bg7KU6S0DV7APJxMVYjDCKEEaBHHtSWSK8FebzmIxJLKyD1HebjjyAtT136ZCSSlQCVMxXegaQVgbM8stSMII9w88hJ5M7lTJQrI4iiIvozK/ASOkAorgKn4pum7o0LU4qTKI4CSNxzlGullAKRrCk9apU++LEIiboKqwEQFEm4wALEYSi7XElmsBUeVTuU96zuIogSAQ34dMr8snkLtz5SmiejDJgClfV8AmEoLOwCuOVI7rErCW7AHEcohxMwKEZcE6TqpQ40IpcEFVRDSp14QcCioLXDfOeh1EVQVDR5xPGFVSjIqaqo5goDWG6UMhMEF66Y/8+jkMcGnkNfRNb0ZFfiUpVtEymxAHjIjFWrf4I3c/E8LCpbo9xhCBS/TKS5yQMqwhDtQJkjHI4ccz8jXqMaxUHCMMIjAaIoqShURgF2juhtw0DhGGAalRGKRjTnzdne0VPEJ5U/BQrYyhXJ3SVTSPCKEAYpktWVeLnsVBjOxnieGYK09ngrCoDjDEcOdKHiYlJfPe7D+LAgUOIogi9vT3o6Gir295xKFpaktK8IAhRrQbI53PIZPzUtoScOd2Kcw7X8eA6DjKZDPxICLdsJotsVrrYkUErusA5RyUiyDhZxFx0l/O9DFyaQUdhOVpynchmfNGfgPrgtAzPzcB1fGRkol1nYQWGi3uR8bPwfR9ZLwsShuDwQAiB7kPgFxCjAhAC38sgy8W18twMsr6Y8D0nC8YjZPwMHMeVCxmJCXHtgjuwvONq7Bt5Hm3+YixuvwSip0AidBa0XYilnVeIFRyrohLCc31kM6p9s+G2RwxCCDJ+LpVjUMi0o5Dp1EmUgMhPKGRbU1ah5+bAeAgWR8j5QlEghOi/190XIlyvzbluYJw0tEQybhOCuAyHuvBcHyBivKu7bgDnHLuHnkY1ngQhDnKZZgAcEa9dtCmd7AaIUjRCI1CH6LE71JOLtsQAn3n9cMwjMFKR3SldnRUuLoBwxVJCdQOqxuNKQjKek8WUPIWJymG05BZiYdvFGCnvB0MEz8vACV0EsVAYqEO0u5LS9HtEHFEOWMi2SQVD3HNOVEiI4ILeW1ENp7DlaD8icFCHA0Qk0U1FfeAwJ0oG33f1ffBcX3vdpiPrNeOC7luw8cgDAIB8pg3DpfSzV4nGwWZoibpOBkEUgYCgq2k5jk5slMoAEeW54RgiXkbGy8NzfXE/RaAAMQ9AQBHxInYO/wTFcDi1b8dxkcuky4lDXoLriTyazsJKqKXIJyqHMV45iNbc4poRps/tWJh9TAgoFrddjiPjm1KWKkeMGOqZZoATgRIKz2tNFvGBGbpK8paEwiCU4VI4jFI0oPepj+twZDK+THTl6ef3BKEUyGR8UOKB0EQog8SpYwKA4xL4vo8IxZSbviO/HP0T2wyPCwdohBhV+W42vraHxtajLd+LE7bTqVFyeYI4Dp0TXgHgLOcMUErxnve8Ex/60Pvwn/7Tz2LVquW4885b0N7e2nD7pBMgqRP2x/ruTCDi7m7NsZ2G27o0K5Lo5DoGlLjIea24cfV/RXOuN2kPLHMHHOrBIS5cJwuX+lje+UaounLZ/1Avs6we1KzXKhsgOSAQk4Kq56dUNDwiIFjQcjEuW/ROUOrIhD21RLOLnqYLZNMkF65MAFPfqfN0pHAhhCDrtyLrtoBSkd/QyL2rz8mIb7dkF2Bp+1WpbQt+Ryqx0CEeVnZeLz4jxBgDlYmR0lthXHMVl23PL0V7folxrxz9+87CSng0K9aaIA4mKwMoVoeQ89qQ85Pnri23GM3ZXp09fzwmqwMYmNppJHP68JwMHOprN2ojGj4znOm+C6r/hGKkuA/VaMpoHJXQ6BgOdWUCqNw1OJozPfCcnI6DOjJnQKHKp8TfzT4UsiSKc/huIckDMeLrAOA7efQ0X6Br9s01F7Ye/VGq9a/Zbx6QvTpk5U1TpguNJmIKBxm3CRm3CRcvuBOdhZV1Ww0X98hQgHj/GvWwUIiERdU7JLkWBOJ5V8mjnpMFleW4HFyGsEJ4bg7FYAQHRl6pyxInoDL0kIxwvHQElUgkJ7Zke2XYSS5cJXtSmL/P1OXSOHUluGprlTSstlvQenHdc8J4LJvcyD4NsVA8W7K9Da+TWFRIPAPt+aXabT5ZHZBLMKsSUaUYm4sNnZyXoznbi7zfnsoNSoWvGrjhze6GpiLoUr9Ocd479ByGpvZALSW+rP1qnQdFZLhosjqIA6OvIGIzy9fQ4zC8DZS4dY3mjvnbOZQzcFaVAVEmR+E4DiilWLZsCQqFwlkR5qeKEt4ADCE/nTIgcwT0MsiuLPdT7YnTws4hYuVDjwpvQc5rlROTp5UA38nrhY8IKK5d/gEs67hKlhASGdOXSYPEFa2FCUHeb0dbfgkIKC7svQ0ZT1jZruPDd/Na4KcqHrRAIimrLeMUsG7R3VKJyKQEikIJrTU9N6Mlu0CMh/opIZd1W9BRWC6TLeX1pQ7acovEOctzFyMgWrEhIKmJ8ujEVlA4aM8vw+K2K/TnbbnFWNByiTy28KaoJM1KNKGFqznBNmW6kHWboevqZ0AYJx3XHOrBc3JwaUZn7ddS8Dtw5ZL3YFHrZanPRR3ylF790kQsQCRK0Cg9viOPEgfLOq7W5w+o5EYHqpbaFICQSZWqvXWqMyAXuQcOdeE5Gf0OqK5v8ueyGsaR41OlZiLfoxJNpNen4MnqcmK8QknLeM24dOHd0zxTBO2Fpbh+5YexsusGNGd7cCzrzXOydcLARD2LSgiYSq1LfdEEKC7Bd/LCMpaTfTWaEqWZTgHlYKyhgCLEEUnCJMmbqUaTOmfBoZ7ugy+vMsqyaRUlLi7ovRXN2d70eGkmVSqsjwXhNTEOrnN2TFRFCCBySw6MvgJCHFyy8O2p90ZhllAKAyK5J8PFvaityAmiEibKfRiY3GkoiY3vT8ZtaqgQt+YWoT23NKUAcF15wRoqvknCZyWlnDrUSzV1890CxsqHUZYJrZxzdDdfkMwxxIHv5gBwHBnbOOP3X2H2K/CdHAp+1wn9fq4wq02Hbr31zejpmR8XzqV+qipAVQzUkvZYUC2EqKwTV6WHentQZL0Wka3t5uA7eaFZEpIoFdJiIdKiI4Qg77XrpYLFcR29tDKlns7gFysIiuZIzZkeZN0WAEKw+24BBI7UpBMFRvcr0ImAan0GB6u7b0R7fimyXgs6Cyvqz19WEixrv1orA66TXqPh4oVvxequN2vVSv1SKEx+ygrualqNrNsiFEZCkHWTyY/zWLd8Tpdv+vLlhs6cV0InuU9q3QmBQ1wtbM1J3uyeWEusF25RFkEOnpMRpXc1kxcBRW/LWqzqugHLOq5JnTcAVMLGyoBCL7F9HChx0VlYga6mVfozz8mBEqorURySlL82Z7rRP7kDh8c3Judu9BkQykBGJrh68pox7bJVz6bouCmubyxXXGxkIamuhKqaQEzcWTjEEWWsDYQIIRQezemlxn23AN+d/r402oeJusaqH4e+rkSMpxJNYqoyiJbsAhDi6OZHavlgz82jFIwCMvXrgu5bkPPa5PWncB1xvdryi+FQH1mvBUNTu8E5h0MzugNgLZSIJmWiKRRBS3aBbHWefr7NM824zVpYJ43Q0lupzH5A9BQ4NLpBrrPS1PAeKc9Yxm1Cc7a34XOX9PEHDoy+jBf3fQXb+x9DLBVX4amr9eY6WNX1ZrgNFDUqlV3x/KnnJGm93Cgmz6TSGbJKyiNBZVM3QHZm9TuNcYtqIt8t6BLbzsJKWaI988RNE9NLojrVzpSj41uOUXZ4dpk1ZeBsuvhPB5cvvgeXLroLALTWP91SxAqzH4ESSG6NlUwIwUW9d+C6FR/CotbLcPMFvyaEnw4TJCEJFQ8GRN2/uY3oLSAeQiHYvCQEIRUDz8lqSyLnt8GjQkhc2PsWrOh8o7SgaToOqbosyimWQKze6DsF9DRfWHfOLbmF2lIUL6Q4/8SyEeNQy0OnrpfswiiaPAkPyuK2y5H322TYhSDrpUNKGbdZKhtGAyiaVFM4smW08s7obdRKlMZvHOKl7umStitxYe9b6qwtpVSMFA+gFI7q/TVne1HICOXWTMCixMHli38GlyxUSYLJWLNeC0TjmSk58Tea9KGfn+Ohxmpej8S6Ey1XhadFnH/Wa0GxOpQsESyzqVVIqhpNyTARhe8WZJtrnkrWU+23W3NLhGdEdnAUymZ63KqcTk3eDvHE+IgjqhwaeQagSmOFR66rsBLLO9447TUgNfe2FlFhAzRnumW77XSYIIwrcKiH1twiESZgoQ49EVB4sqpG/AZoLyzV7wwhDjyaRd5vx4rO6/GGJfdhUdtlsocBh0PdGs+AOW6ilQmHuLhy6X1oynYj57XhiiX3pMI/anvVylydt5nIal7z2rUtlAeuUb6Gan+9oOUSLGm/ouE2wt0ut2cBKtEE1Joda3puxorO6+pUsu6mVVjQsrbhvKnmKr0YEJLyVRXeqiViAQ6PvSY6aRpzCZHN4QDx7HgphV6MOuPk4RAXeb8d1638ReT99rr9zxSzUdyx39N6WTc4tbPhuiOzgW1HPAMIIShkOpD3OxJF4DgTjvodIQTt+WW4oOdW3TdACFS5DSg8JwPPySHrNaOzsFK6cQlcJ6snZbXeQEa+/MqCdp2Mdg8qAari41Ra2sqqcGkGOU/E/Vd0Xq8n+fb8Eul6BQgc/fIowaUUEhACEOknIDKMUaMU9TZfKNdsIHCkYDNLHUV5pac/TyZiKifCrDGpiW0ybotuXbyk/Upt9RIQLG67TIZXjD4IUulS52x6ZwDAdwopi0qNRYVl1HhWd98oSupAUpZo3m9H1muVi7GIODWlLi7quQ0X9txaZ5kS4qCzaRWybjMoZH8J+d0FPbego7AM1WiyLlxjkvc79Dlm3OZUrNhEWSXUuB6+WwCRngGxOqVy+csOe7w2NsvFOAlBNSrCd3LyOc3KeDhLuYOV12XdonegKdutqy+ybjMyXhOWtr9B33MmG9coYSos/bzOl2jsbTNi+7I1dm1uRe32x/IOqByUFV3Xy7VCkpCJEpIFv0MqTaJng3qXKKmN6RPtfRLnQ5H12vDmVf8Fi1rXYUHrxVJ5CMABmZOQtNoFku6hRHrHPPneqzBFxs2jPb+sQeiD6Gsn/iWU6NpryDlDpWZxJod6wDHyntpyi3FBz80ikQ/125j9KPRnsv+GCLm1iOtJPFzY+xbRxr396tRiaADRyZNKiOrnKtWBMG5oPUesisNjG7Fz4EnkvDZ0Na2W98BJ3g9C0JpbKMuYk3PPeq1yDnVSPVpOBjNMYPZYqSXvt9e93+aaCrONVQZOAkJoannh49GeX4q1vXdqIa8sbUBZmUZnQyL+JhKRMjrerdY/yHlt2tpSVoqKsatkQuEGdkGNB11NZlmvFXm/Hcvar9aTpvIcqPH4NUlJajyirXFSCubIRMVlHdfoSUUv3iQnbTEeV1v2vpvXL6rv5rUlrJSsjJPXk7/QPAiaMl24VFrVnYUVWNL2Bhk6cLCo7XKtmKiJ2TESiJQL2wwTdDWtStXNq/N0qZHZLsevJk0zPLG88zosbbsSANeTrEN83Ta6zuOBpEVz4hkQfpa81yaXri7CoR6Wd1yLxW2XAxBCqyO/DJS4uKhXKBpNmR4s77gW3U1rGj5rSjCZk1tGKQPgiHmUJO0RonNIFElVQeIZUM2hHOqju/kCACRV001lXotSQFWXy7zfjqZMNy7sfYveh2pKo+KyVOXDUFcm3ja2GlXISikp6aXBa7Y3ntFGCIVa3hOQlHBSIbqmbLdWUNV1Vd4l09IkhKSeLXEdhMfFfAZV2atDXN2nAhAC4sKeW9HTfKEUiCJMR6kjwzuu/DNJSE4UB2H1eioHQlrXjXJLasMSx/IMAOLdzLhN8h2vV1AbudOZbK5EkITgCHGwqPUyXLfiF8UKrIYRRQnFwlZVweQbXiV5BHmNyuFYaj0J85wiVkU5HEPOa8XS9jfo66BDQSBY1XVDKjci67Ug57ch4zYZ+S4zm8sbodZEUceezki8eMGdeowKVbI6F7DKwEnQnO3BnRf/7oxdS8cKh4iJzgibGIJDCQ3tIiUOWnILQA1XVPJSK+ubyGS2LDoLK6VV6OiJoinThYzMDKfEEdaBYUcRQuG5KmeAaiFOU252pTiIGPdFvW/RwkcpAOrvajyOXP755jUfQ5vM/M+6LTrZRrjyPe02T7wHjg51CCHnYWn7G3TWtHD5UlDiIes247qVvyjGI19ulyohnYzfrclLUMehchEp38mDymZHyopWLtqC34klbZfra6QtXOkNUWEOc7/iTyWAHUNwSJewIxoFuU4GS9qu0BOX7+Rl5ryYLDsKK5BxC3BoInxqcwxUDDzxMhB40sJUq9F5MtmUgKaUHEA1+lFhAo5SMKK9Iirc4UrrVtw3op8v1QtBuE2B1d0349rlH9DJrICY/MT6BpG8bg6as73oLKyESz2t5KYW2pLCXeXhKO/XdNTmg6S+A9GeATOJV/xJdIgu67ZoQaHur8q1UPdej44k2f5UKfZGwrEoORVNemotw9bsQixpf4Ou9FGeOUe+Ww714bsFGUJxUMh04eplPw+1mqjv5vVCaOq6zSyc5EF7KRvQlOmSYc1Myqquu56GFV4OJ1AOxhOBSIC23CL4jsj38GTSMSUOCn5nyopWBk0SGkh3uaztHukQD2plSXE+6p0gqfwulYBsGm45vw2eI0I55iqzxllNa90n1y85b7XqqLgeFL6Ta5hr5LuFuvdVLIhUrdt2NrDKwElAiaOXIj4d+6q1JIHkxVaiWk1uF3Tfgrzfob0F6vdKQSCEIu+3o+B34uYLfk2+1I4uJ1vTcwuuXvY+LZiUwFcCnhAiPQMqJKD+lFYSSawJ5Vanxu8d4gFSwfEcsYJb1muRAlkss6wFtZPBJQvfjoWtl0IlLuYznVoBAJH5D/o/R+dCiGvgardjV2EFrlhyD7qb1kiXqq+PkawVIQSJSrw0Jwgq+9BT4qAp063dtb6bl3kL4niru29E1mute9lVlYhDklCDcOMW9HHl1dSTNQWF7xT0vtTkb1q9KrdCKUjpc4FOfEqeJxcUFB2FZbj1gl9HS7ZXuPmJ6dGRddwyD8BkpLhPNr5ypHt5Ap6Tl89fkougJ2etwCrF1ZGVCRwZt0mWvwoB35pdBAKKSjgBtXonIS4Wtl6CK5e8B66ThaiGuAbrFt2dvAtSuKtzMENN5huTbD+9IkAMN7/5DqjvleKS9ZqlQiXOy3dyOqym1lLQnjua9gwkq5gqD5+oUADn6YRFQHsLk8ohqqshMl4TLln4dizvuAaUeKAypybntUlFUpQir11wB1qzC3WCcq3HMus2o+B3pq6RUohN75h5TdvzS6W30UV7fpleA6V2zmvJ9MjFhWT/CdkTQ81bi9suT4W0HOph3aJ3Yk33Tdqrop4fRyYQMh7qKprGCI+WuSKmWEhO7EuFAMSm9cnDyrPSkl2o262nk7pJndKWXDfhiW3O9mJV1w0A1NLlsi8DKNb23okLem6pG7Moea0J4YDr5l+zjVUGZhGVGNgoxqzq/2vdna25Rbh4wVt1bDiZzKh+IZpkEpsnWwgnSS0EHs1K17CaxB0pwJMeA52FVaJuHImgN9c7UMIs67agLbdEWu1CWCSWsepRkEPWa4FHs0mcTnkWQLGk7QrtPnSdrCirJIkipFzanpNFS3aBtqAKfqcx8RG05Bbhgp5bpDB29MSmM92ldwKA9IykE62UwiHayfZqoZv3O3DDqv+C5mwPPCcrFBfZ3GVBy8XGPZPrUMiJRn3mOwUthLRLUilw1JH5B0Kge1IYJiVPxnXQFkyyKiZQU1qGJCTj0gy6mlbhwt63IOM1y+cgucfCu0BlBn/9NGAqNUpZUV4AU3klAAh1dJiKUhcRD0WfF8MNTwhBb8tFcKiLcjghY93mOYn/HOIh77fJ8lrRpKe3+aLUdQCpbcBEsLLz+kTJMpRc00pWz4ISBuo6msqb8LQ58polilnWa9HKispXyPvt+h3RuRpmuEALZ18nmdGaJFXTm6f2lXWb0Zztge/k0ZFfJrtKymdIPvOtuUW4csl7kHGb0ZZbiozXjIzXkhqzOCOKy5e8G1cte2+qGkrcr7QQVGtEEOKg4HfBkYr2Rb1vwdoFd8hrllY0HOqj4HfCDDckC5URrewk5+tgYeulaMktAgwhrMKYw8W9eHHfV/Hsns/Lig2BUtDF/sVzGxtrk5jvVt5vT8KP+t1Jxq3W2FjZdT2uWHwPaht6EaOSyyTjNmNN903wnJxIfJVKZTEYkXlKwvgRIYj0e6l6wdQbkGKl1MYLaZ1dZm0JY4ugUaJTc6Ybt6/9bRQynalMU/VSre6+UU9gVFtNBBmngIWtFyeudxk70xa13I+aCNQ+RXhCFkoRiqZMN1pzizFVGRSTJReCSyknnpNDyCrobFqJha2XIIjLYvJFUtcPCKHbnBGT2uK2y9HdvAaek0UllC5mFb6QVo/v5NCeX4KVndeneh0QiMZFN1/wa8h5bSgFI+huXoPLFr9Luk+VC5pALSilF2qSwlHEpX05rmbDNSjOXPV6eOOKD6ISTWJgcod2/WvPijx3Shw0ZxegNbcYfRNb5X0QCpwK6yhLLuMWUIkmUoJHKHBC4GbcZj1xKMvTdIGnQkXSe+I6GZmISOuUAZVAFkvBvaTtSuT9dkxU+qA8TK5sjqRizoSQVFM2l2axvPONmKj04dDoBl0VAJW/gkR4ygAPKFTzLOG+BYEOLUl/CDJeMyjxUApGkHGaEMZlQ2GQOQE0HU65bNG70NN8gSxZU8evn6xF+MyRPRmIts4JIWCxDEkQsbaHS0XSrScTI80wgSObR2XdZmHxU5HIWvA7UQ7HoHJLAKEMVMKJxFsDqoUQlaE9yHJFlWynPHrJ/fXke+ND9STpbbkIrflFuvIoZBWtRKhw2/UrPyTDOVQLL+WtTClKhMhnLN3PxZWlyqZw8p08KuG4KDv0muBSHzGrAtLtrkKNZodBShztRWGy459Z+aHupZljIkIgSbWT+o1DPLEWhlw90WRJ2xXYPfSMqPQgLny3gIhVdNdRpSQ7VIQLzVJpgnS5n3pPE+9B2uOh8rUocbG841rRxrmwDMvar0F7fin2Dj8HsaKlMExiFmBJx5UYmtoty4tzxvHkzCq9Oo0SNrccfRiek8PFC946rVfrbGA9A7NMo3wCh3poyy0yLMD671NuSTFNojnbi1sv/A1hsSi3rrQ4PBm/B6Q1Yhw37camSJIWDUtUTdQQiXVUZf/LmOLaBXdKazexfBa3XYmb1vyqdn+rfAB9LCkIVPyaEhcd+RV4w9Kf06GKRGmhuiOc62TRmluEtb136nJA5Y2gVFhkqrSIynrqvN+hvRY5WapoXlbXEZNdS24hOgsrcOPqX0bea4eu5qDK4k6S3NJhBldb04QIIV3IdIpkNah4upyYpEW2tP0qZLwmbXkLL4LKl0jc72bfijeu+CDWdN+sj1NrgahwkCj+SEpL1bNEZSmfqBJJci5MPCeLFR3XoqfpQgBEV7AkQjtJIFXKizp/1xHJciq5VT2jStlznQwmK/3yWXDSViRxpZCQnxGSViiM7HdTYVLPZCLUxfOqwiGAyAFY0n4lMm5BnrPwkhFixPlBkPfbcOOaX0FHfrlOxAVEEqt671TctzW3GBf03ArfESWXHYXlWNn5Jv2eERnmShJ0a5WpJNlXPztwtIcKkNUwUCHBxFOk8kDUfb1k4V1Yt+idUEKuOdMj+iToa1NzXOpD1fYrMq4KV/nwnbwWXmp+UcqK+ZwsbFsH3y0koReQxNNIkrLTBJksKHtfqIRbM7yhFAEzEVQlTmfcAtryS5B1mzFSPIBAdlYUSlIzFrZeiozbpK8fVc8nSY9bPI/5ZG4z8yxkJVfOa8UVS+5Bc7YHS9uvxvLON+qKLYd66G2+CKu7b4RLs1jTfZMIibotOtdIXHuzy2HjahnGo7pF32YD6xmY48wkkQUyWUdM1GZjHWUx87TgqtunFMw81haO6XGoTchKkgmVJe7oRLekxDGZVLTSAWU1Uz0O3bmOqMmeyAYeKlfCSf4uJweXJq2Tze3VPlXCn7DUMrhx9X+FQ30cGd8EUQnSCjMJU/wuieMTmkFvi3BNc8ZSlqpK3jOtU8/JCeWFJOWSXYVVuGb5+7C9/1EMFffoc0ky7z1c0HsbPJoRSYFEuH4B4ZLOe0JhSco3xaTVlOmWi8y4snGMivkTLGm7Am35JdIqdaTyofIUxHVU+SSq1FILUcMzoO657wrLR1UjiPNO+k441BfWqVI6ZXhmvHxEj115dtRk6Dt5jMeHkfNbQUoqdKJW7XSxoOUSNPmdxiQtrjtXf+q2r8YzTKDLHwGhpOW8VixquwwFvwPFYBSek8HyjutQDIbESqEwKnDUvqRnoF2WseqqFEKQ89rgEHFeIqFSKAiLWtfp58dzcrKrp/T+aKVECXqlZBsWukqqk8oZpeb3RPYWyen9KaVVv5/CJ4OmTBdyXhsmqwMo+B3oLKwE4yF2DjwhVqyUXgTzuKpFuTqWJ5XRltxC7aVyiAsQJsN7TiqWnnFbsKb7JkQswEW9d+KVA98UgpRmtIKuFJ0YgX7HHKms57x2ZN1m8XwoxcAg57chiIqIWCBzb0SZ9ptWfgRb+n6UyimgxEFzpgdXLXuvVs5cmsHFC94qu7Mm859qF20mRdfmDLg0g4hU4NIsbrng44YiqDqaushnOtCaW4zFbRV0Na2GQzwUMsLoaM72wnWyKPidsuJhXCvltRBC0VlYddy5/kxjlYE5DTlmBzwAepKZLitYbANwLRbSrZWTI4l4oFkSZLr7ksxrIuLBOrs6CVcIjdlNlSYqCw8AwI39aMVACBCK9JjUJKysNjNOXnuuprKhPAVq8hCeAWE9qxLLrNdseDyEHNQvIlELhyReFLHfdCdD5VIGgMsXvxsrOq+XE66Y8PN+O3KyrlgngxLDI0KoXg+iPb8Mb17zX9FRWA5AdF1c3vlGHB57HUkHRaPqBMID4jpZHfIACC5ZeBeyXrO24kDMeyyO25zthu/mtfD2ZfkaUIUKBYhnjqSbASFd0gapSERx1bDshWJXjaaQ99pBqSsVUR8LWi9Fc3aBdikX/E59rRKh72LdortRDIYxXj6KdGKdCxjPoe/k4Tt5BHFJTN5O4rHJuAVcseQ9OmnOkWWSMQvRlOnCVHVIKp4y18Z4rnTJrozvqvuf81tlcqdoVKNixOoFU9tpJVW+kyrpTyc/GucrQhZZ/Q4kuT1AIriT0Ihypev3U4U41Dspw1kXL3wrmjLd2HzkQf38B9EUKHGwpvtmtOWXoDW7MOWBENctj4zXhOtW/GLq3NRcoJRHReKpiNCc7dHvYbKSYvJMkFiEuCJWBYGD1uxC3Hbhf0NFLn7lEA8t2V6hOEjXf0t2AUaK+0CQJOK61Ifn5lMeCgCg0ouhEmM9Rywc1t10gRTkhjJgtKPWntCapEvPySCIhYKacZvl6oWxMR8KD1FTphvt+SVQJZ2qv0LB78TS9quwqO0yhFEZrxz8ltwmneuS99tx5ZL3YEXndZhtrDIwh1HC41gksehjbZcW/bWlRzry6yQCN6UwaIWjJnFMC7fEfazcnmYuQrIfGHHOpFlRSmEwzwuGQCNJI5npGqUIoSBedNW4xZw0XCcD18nCdwpaqEOqAzo5SU0RJHEdAyqmm1iyANGJSFmZuBWzUJ9b1mtN9XtQnRAB4TFpznTL2CWQ81vRnO3Rq6yJyU94VLQgqRHslIgyTF06iqS8Ecb1TZIGxfXMee1ayOnWudQFYnGOPc0XiMx+aZGqSU83beLqjkrriVX1v3UyKiBjzkJZcKiHq5e9Fy7NIuMWcGHPbWjJLcSB0Zd10pWZ3Ag1SRv/uY6vJ2MAWNByMS7ouQ1bjj4o7oV0/Yqr48gSzOQ3SWmaEtzK05E8T4kXLC2UqQzH6AnfaxWZ/LLhl/AyuMn1l/tSz4pIOHOhexvI/Sctm6n2OOlmWWYSoPEbnahZY6VzqWhQ6oFCVdk4UjnPg/EIVyy5F11Nq9GSXSDzNRytrIIQ+G6TbFOeT5QNeZ2U8ugZzZ500qzuz0H08x3zMPV9KD0nMQugEqezXqtuJkSJg66m1bhy6XvwyoFvIee3Y033TdhLffRNbNGen5zfVpckCQCOPFdXllqL5mrN4l4ZVSEZt6CNlcTbRXX3VvWMOKoxlzEvQL4vhIhqMpf6WNh6CapREYRQtOeWoCW3QCsvb1h6H8K4guHivrRXkThY3nENRksHkXWbsbr7xln3CgBWGZj3OMQzYvEzpJHeQAgc4huWR9rqqP0RJen2virL3FRMRG5C4yYypruUKve5qWxIQa0qANRkCiJfzCQykDoxFdf2aFZ7AlQooS23FG9e/VGZM5D2plDD8jSTl8z8ACotMd1TQbWGljFhJWQd6iPvtwIkiaubLtqC34UbVn9U96nQikYqlCMT0qRrnZIki19Z6U2ZLpGPIQWLWl9BZ9OLKwjlFVAeC21tEpEDkVReiOS0pkwXgrgsS9xcw7KlyfWHsJ6qUaIkmsmPOa8NDvVE2RVPLLDOwgrELASlLm654Neg3NxakDgZqFXnzOeEEhcRZ/qZo9TVXUEr4YSuIlHfqXNk4PIZ5lphUWNRlnV30xqUgzGMlQ/r65Y8UVRa1wXt5s26TVjR+Ua0ZBeIRjk80vcreQrVMYSi4jrZlLLoUA+cM2nBivK9FZ3XwaVZcMQgtPbZNJexTivFOqgg8zIiFujzF7F4cQ9Xd91otFEmehyuk8Wi1nVY2n4lLl7wVmTc5iQJlFDpOVNKQXqlUUpcEMeBQ3zjucjBJ/lEkSCezgnQ+TPEEY+TCp/Ihl09zRcJZdptQXfTGrRkF2CifERWn1C5XgSFJ6tRAGFgUJqUGVPqoqOwArdc+OsoZDrEW0B9+E4ON6/5Nb1uiVL6xXvRgazXgko4IeYPJ1GAtaHFE8+gaFXtwqUEIRFJjW9a9Z+1gUQJBSdJJ9hEARfhvbW9d2LL0YdOeFGkM4lVBuY5HYUVuGPt79Qte3osakME5sQBQE++4hs5IequefIXjYQ8SOpzQmjdkaD2R5KEppzfJjuwmTXPyrsg11vQiTiAEkp1+zWztB1fNxFSZNwCOgsrtfBsynbj4oVvw/b+RxOrkhBww7pLj0fFakUNviqTVBnhXGawv3H5f0JHYTl0DoVhMav95mT9vdq3anmbXH8hBDubVuL2tb+lr69ibe/tCOOKtOI+iFIwKnIBqLDkQ1KGlNz6XnhOTredVhOUEAY+FrRcjKXtV6M1t1BcN1ZBU6ZLCAevWScFKi/Kqq4bELEAxeqwvk5EZrtn3Cb0tqxVqpkW7OruK4FS8LsQxmVxr7RgExUO6jfm2gHam0MAwilWdb0ZWbcZz+75V/huHtcu/wA2HLw/bZ1LRYODAQRwScYQSuJ5XdS2Dk2ZLrx66Ns6gVGR81qwpucWWRYr3M2uk8V1Kz4EQCxeFMVVwxtmZNHzJJyUcQsoh+Na8QXxwEisFcn2/FLkfFENEHOmn0f9TMpGUKpahxACB0JJ4OBQT63nZOXaEiqsoBYzU162RHFT72HGzeP6lR+S74io32ckqfhRQswMUwCQCyiJFsLmWiNqoSblfk9yIhIFKfH+qDc6nUyslIfmbC/evPqjaMkuwGWLfwZtuSWgxMWy9quwb+h5TAVDYCyE6s+hPBoezYD6HfrZUZ4NsWx5ci3Uc7ms42oMTu3G9v6f6GdT5MOk5znXyWBF53ViXRaSeBLFMWQPDqm0EkSGgSATRaVxoCp6VP7KXMAqA/Mch7rI0ek7hM0U38lLF5gZ8xStkql66A3hWJuBm2Q8p91tjaR2rYXR27wWvc1rk98K36225MRvkoWOZlJ94zs5rOx8k1wBzphQ1UTAOZoyXbhkwduxe/Bpw2VMQWu6nQGiCctFvbcbPRmSDHbdaEnGbTubVulcD+VBMc/X9ALoUBDnoGaSpprUnBzyfnuq5poQ0XTFd0X5V2/zRVDdG4V7Vk5QJGkk49IMblj1n9GaF73gfbeAglzvwKUZdDdfoGuoRVxXNHa6fPE9AIAqj7U13dO8Bqu7b8LB0Vf0fQLEs9hZWIGc34YVHW9UN1J2FkwUHSAJFymPi3mPPCcjq1RapJDRd17/KeLtvk7Ac6iPJe1vwLa+nyQCV/2GiPJY1YdeJOGp+L04p96Wi3HTml+VrYaT+59xm3Bhz63IuAU0+V168lblmK6smDATS9VolWtalZAKq9ORz4MIuagyX/UcEUL0WCETWoSHg6Ap04287Nyn9sFYDIZYrnYhwkcuZVrQq4RD7QnRykCSOKee0XQYMPE6CIWhGb3NF8lwixiYyB+gIFKR1UoQSaockiXbneReqP+p0ILOSyCGEph4E5oyPXBpFsvar9YlpqLLquj0F/CS8FJQ0wtGwVlyH1VOhkM8MFF4mxgbhMAlSZIhgViXRCi+aaXMgYerlr5XXxtVjp2e54S/i8l73ZZfgquXvU/3J9DvNyEgJ7DC4ZnGKgMWAGrxlgTxsMpHvIH0NasCxAcAkHaHT6fxUumeFvtR1lu9Ja4midrvZoJLs7hm2c83PLaIIUvt3bD2xZiF9VurxLTmFqM1t1gLLBCkJi5xnWQyEjfaHNOk/lmdB+FJOMPE/MxMtDT/TLal+raYCphDkuQtFU5Qvy9kOmU5HcHyjmuxpO0NcKiPNd03yYS+JF/EFN7CDZt0P3zz6v8KAoKj45ug4s1KOPa2rNUKhfi9KGesRXs/asqthEDIoLNpJe5c+/8es+1wIsiIFn4q8dRwiSRCigshKdzWSXMsFc/Nes2JcmZeaemav7D3NrkWRdrzIBLJenSZnBKsXG1DxVLRE+Wj2soGAVyS1esPqCoXaCElxs0Jl0Kc48ol94IQR9T+q2dEP67JmJNFunyjp4Zwees+DlIZ62xaietXfliHO/Q565wTcU1bc4tw3cpfxIaD98OlopeEWMhJCHh1PS/qvR3NmW4EUQk9TReIECaRhgVJ3mt1HDNUCCRl02aYSCl8RCp16tw9JwvfySNigZEkyBu+M0nliFljI8MEskrJc/J6Xkq6fNa8dw3f2Uah0KRKJudlkW+9XB9fKTP6WZhmnjzbWGXA0lDQJskzBLWCSHyfbquqs3KR1qQbQaknnLeENNxOa+1Id0g7EcQ+G/w25d1QTZuSBkTKTdt4f8nfhVUou7gZJXTSd6i3951cg9bV9RNI7TVoyS7AorZ1yb5JElc39qJ/m1KaZJMoNdkqKzzdjc+TcWuOi3rvQDWaTF33lHJIYPQMEBM757xOYRHfpxMvaz0CJpSIkkHlxTDPyyEeHK9VeikS13btPUi55aHyNtINZGqvc9ZrxW0X/gZasgvkZ8m9bzT5O0ivk1H7PYWLtQvuAOcMxepQ4soHkS58gmUd1yJigcjDkG7jm9b8KrKeyD539X0W+zPHoSxplWzJWAhlsYt7C6iuUaYys6rrBixtv0oL0tS1pyqhtBW51lZwzpPFdgBAK8dif8rzQYkL3ymgyqfgyUoIxmWIi4jSVypr/m9c88tQSYhm8yzzeUi8BMq7pFpyq7CCOQeklQfXycB386hEk9rDYq5hYL4TKsdGXL/YOHaSi3PxgjuR99vw8oFvQSd7HldQN674Up4Q3ZZan58vjyyVn5Oc384EVhmwNIbI12Cad6E2AS9JyDr2wy2E1QwOD6dmIjhxGgkgYvyPy/i3EiCm4Dz++ORyy3B0fkMjlra/AQtbL9UWrB4XP/YxFrZeit6WtcLtD9SNyczHqG9p3Vh5o06jGmfjnI3Yv2raJP5N6napXLq6017624ZjqEeKmlpl0AwlQJXkNd5f0mQpsfLTyon8u/HMOdTVZZycJ7kayno2H88klCDGW1vSpo4hljqWoQiZWKqvhMxLWNh6qehiKNsUFzLC5S8WuTHPMa0IKFe0rlMnwhrXCgznqYRDhe8W4LsFcM609asQCY/111KeUN13ZmWB7xYQ8SDpmInEQ5MoS0Rb2cnxZKOn1LyRlFwCQsEWHfw8fS91grHx3lDioCW7AFm3BVPVIWMtiHSoTdHVtAo3rvkVeE4OAS+mtlHKiO8WkPPajbmscW5SLeLcjWulvGnEATeVHiOMSohShueOCJ47I7HMKcw4YkPPANIavprMZiRIZ+DyJyRZ9OdEQwTH3m8SP+WcwSEeVna+CQW5noOQezM7nkN9ZLymVD5Ao21qvxfC9djHEOEE6Em8tv1uSuA18KCkJ1w0DMWYUOLW3NPjXwOd2Z7alMh7V1svX8/puK+6hFKf3/EV0lpUvogSsEKAC2ObwCjvRH1Zbs1ohPfA6MmhFBoRThHNv9SCU1phOI6XSHl7TKvY9JbMTAlPhChQ/4ynjslrhTW0sig8AzmEcSWp1ycUDk138jP3JwSjSuRr/K4kniRX9MEw1sYgWvFxAarO28GlC9+BiFVxZHxj3RoI9funun1wMkboZGGF2R7bTHKedr/HOiZx5HPJU9ure6YWTJsrzJ2RWOYU6XyABhZ2zeSjY4wnUuJ4DAqZLrzZKL87XSi3p8KhHt6w9GfrtpoJzZkevPXi30PeazuJUczEPZJ4X2qXPjWtZ7OvgyIdayfH0z30Gg4nQtJCN22NEaPa41Sfh0aWuEneb8cVS+41ViJU1mSj9ekbX4QktFGr4KrwyvSCu8HOavaRrsxJrTyJYwuT4409sUDrBXAtZkiHH2fb2mdL/F7cV5f68N0mhKxqeI/E9a5d+Cg1dE4aHlMp54l3RnQPTfpyJMaIQz2dj0NA4OhcAmdGQlUJehW7V/tJj0eV5ZJTFtQijMNSb7oZ8rhs0btOaf+nG6sMWBrSlOnCgua1Wos1afRSO9QXtbfHmbxnikM8dBaWN1iq9tRolOeQ+n6GioCyftUKkScxkhlskcR6jzV5O9StW+/9hEdzEuGYpOa9Nr6d2EKnYv3PxEOR89pwYc9tKddsIytxukQvIGnV3Pi5TrxTjUrN6rbXnQgTEq+O8DKotRxmqnQCDZS/OsXleL+n6i8zPqo+JhLvxgW9t2Fl1w04Or5RJ9SqbWvzUvR+6rxH6W/FH0npaU/TBaLlc61ixZP3QTV4Ul6xmYQTVaOnJDGwXklWy7TjGM/LTEiF38y+JaqdNyGyzHHuYJUBS0MWtV6ORa2XA5jZhNOWW4zbL/qtY7rMT5hTeBmn3aWelaZ31Z/OsMSp7H+m4RQq681NTiTX4mTPl9KapiwwhELywQmIvBOnVom5dOFdjS06QnQmevrzY3svavtDHA+zLXLyOyW4ARiNeE6WYyk20/7GsIRPRm3UPUG8FsADmrK31X1/Mgm/KgxpeiLXLXpnQ0UwHXqQOSLUxdXL3ofW3MLjH4tQgHOpsDUm4zZjYcslOpwDAFEUg3MG1z2+B6bxeM1qGWVcndk55mSwyoClIScqIFQjjbmMykA/N6mJfTaqpDjNZJyC7v9fOxYyTWz6dFObLNhRWA7OuW5zO7N9NBasJ6Mk1Qr56aooTsWDJkr6TkQZqKnyOcHjNUwgrYu1ix4TWa/5hA5ACEmFw8T1Or7XI8kxoFjcdtlMTgKEi7yNpFdCPa25RXjz6o/q77dv34Wnn34ecRyjt7cHd9xxCzIZH3v3HsCaNStBGyRuNjxHCT0DBs7pwioDlvMK7dY8h5SChpPmGfZuAGJ9gK7mNTX5DCfmvj4VTksC4jFd2Ce4rxmOR7WkPeljHKcSpfHvVBOjM3NfPCeLmy/4+AzL8VIjq/N0zNQjdiIQyJ4NvHFCdHq/4vtSqYyHHnoU99xzF7q6OrF3737EcYyxsXE89thTWLJkEbJZ0agtiiJUKlXk8zlQSsE5dAljuVxBLpeVa78klUtzDasMWM5PZIb2ucrpFHLTQakLv2YKmSt1040n/LlxvwuZzrqE0BPhxL12xvZn6BKYFSQn9Dsj7n+m0QrBDL0qnHNEUYQ4ZigU8rjssksQhhF++MNHcODAIXzzm9/BXXfdAcYYfvzjn4JSimw2g3e9620oFst44IEfIZvNYmJiEtlsBj/7s++CIxdsOtMes5Nh7vosLOc9Z8G2PONHmA1UgtLsTDhz45rWdo87mTj7meKKJfdi3aK7Z3sYcwSltJ6d58bsM3I88vkc7rzzVnz/+w/ha1+7H/v2HYTrOrjppuuxcGEv3ve+96Czsx0/+tFjuPHG6/CBD/wsWltbsXnzdsRxhP7+Qdxxx8348IffB0opNm7cio7Ccty+9rfhu01n4WxPDOsZsMxZTibDfcb7PguW8/nIXBG40y2SNdsQQuCS05hkO89RPT/OZiz9RBIAr7jiUqxZsxIbN27Bt771XbzznW9Fe3sbKKXwfQ+lUhlHjvRhw4aN2LhxK0ZHx+B5QqwWCnm0tbXBcSiWL1+Kvr5+0FOqQDqzWGXAMmc580lwc1FczF/mYhw0YS46Zi26xG8OOqkZY4iiCIVCHtdffw2CIMT27bvwpjddK9YnAeA4DnK5LN70pmvR3Cys/Uwmg/Hx8dS+pqamkM/Xty2eS8y9O2CZFTjn+j/L6cde2zPL8a5vbVMmy8w40/OC6vY3VzxKJn19A/i3f/smNmx4Hdu27cTWrTuwevUKZLNZlMsV7N69F4wxXHTRGjz//HqMj09g//6DCMMQADA6OobXXtuEjRu3YuvWnVi37uI5/QzOvTtgmTWq1QCM1S/SYzl1oihGEISzPYxzEsYYKpVq6jPrBzh9lMuVM6cMEArfyZ22ZmWnkwULenD77TfjyJF+bNu2EzfddD3WrbsYra3NuOOOm7Flyw5MTRVx5523YtmyJXjlldcxOTmlwwRNTU0olcrYu3c/7r33bixY0FN3jGo1QBRFdZ/PBoTPI3OlWg0wMjKG9vZWZLNzu6Z9vsE5x8TEJHK5HHz/9Hb9O9nxcM5097czsX/g7Lm2y+UK4jhGU1PhrBzvfCKKYkxOTqGtrUV3fuMQyxXPZUtsPsA5x+joOFpbm+E0WOjqVGGcoRQMI+e11a0IOReoFY9mJ0jzs9p/9/cP4N///fv4lV/5CBxn+nJboTx4c0KezT11zGI5C1ghcQ5j9kO2zGmoXPZ4rjLdPDHdSpsK3/exfPkSiJ5K82OumbfKwDxyaMwz5l5se66N51Q4l85lLlJ7fe31Pn3Yazlz2tpacffdbxPtTObJdZuXykCxWEa1Wj3+hpYTIghCxDHTbq3ZhAMiTAB6NprpnXFEf3OOOI6Pv7HlhGCMIwxDjI9P6meFy46z58CjM6twDkRRhImJKVBqr+bpJgwjeN7cCI/MK2XAdV04joMoijBHci7OKThnYIwhDO1Lf7pR1kEc2wTNMwFjDNVqMNvDOCdhjIFzm/x6JphLXoN5pQxQSuC6Dlpamq2WegaYmJhCNpuZEwmE5xqVShVxHKNQmNu1xvOROI4xOVlEa2vLOeFFmktwzjE+Ponm5qY54TE815iamvmCWmeaeaUMKCglM1otyjJzxNrbAKXUXtszgKpzt9f29MMYl9fW9hI43SjL1c65Z4a59Ljau2uxWCwWy3nOvFMGzkStq0VA6bmRrDcXmatd1s4FlEfLcmYQ4QE7MZwJhCd2blzbedV0qLaxg+X0Ya/tmeVsNzk6n7DP7pnDXtszy1yaF+aVMmCxWCwWi+X0Y31rFovFYrGc51hlwGKxWCyW8xyrDFgsFovFcp4zL/sMWM4cNmHozGGv7ZnDXtszy1xKdDvXmCvP7rzxDHDOEUURpqaKus+75dTgnIMxhiiK9PU8ePAwnnnmhVke2fyHc45yuYxSqQTGxLUtlcp48MFH7PoEpwjnHEEQyLkgkstdczzyyOMYGRmb7eGdE3Au1nsQrYg5Xn11E7Zu3THbw5rXKBlWrQaoVgMEQQjOOfr6BvD448/M9vDmj2egVCrje997EJVKFfl8Dj/zM3chn8/N9rDmLZxz7Nt3EE8++Sza21vxrne9HQAwMTGJQ4eOzPLo5jdxHOPpp1/A7t17EYYR1qxZidtuuxFhGGLXrn248067PsHJwjnHnj378cQTz4Bz0Rnv3nvvRktLM/bu3Y9LLlk720Oc93DOMTIyiq9+9X7cffedWLVqBfr7B1Eo2Pn2VHnkkcexZ89+eJ6H5uYC3vOed6JYLGH//oOzPbT54RngnGP9+g1ob2/DL/zCe1EoFLBhw0brHThFmpsLWLx4IcbGJhp+Xy6X0dc3YK/zCcI5R0dHGz7wgfvwgQ+8Bxs3bsHo6HhqmyiKceRIHyK74tYJ09zchHvueQc+9KGfR3NzM15/fUvqe845BgYGUSzOnb7v84k4jvHoo08iCIK6xZ/EWgUTGBkZtfPCSTA0NIK3vvU2/MIvvBf33vtO+L6f+r5areLo0X4wdvYNhnmhDADA3r0HcNFFF8DzPFx00Rrs3bt/toc0ryGEoKurE4sXL2wYpwrDCN///o9w+PDRWRjd/MZxHKxbdzGy2Syy2Sw8zwPnycvNOcfzz7+El17aYGOwJwghBD09XWhtbUGpJMIwHR1txhYcBw4cwg9+8ONZmVDnOyok4Ps+li5dXPf91FQR//EfP8DExNQsjG5+o8Jbk5OTOHjwMBhLhwujKMZDDz2KvXv3z8q8MC+UAc45qtUqstkMACCXy6JSqVjN9AzBGMcTTzyDlpYmXHnlOiuwThC1KBHnHNu370JzcxPa2tr099u378LOnXvw1rfeattrnyTbtu3C5z//FXDOsXr1Cv352Ng4Hn74Ubz9/2/v/l6bvOI4jr+fJmnatOZH05gfDVVUtDqRbahjuCpNi4gIgyEb3hS88C/zQhm4iaxj+KOdVuumdKVz4tzQWpOutjFN2rTamic5u8gi+wHDVrSG5/O6TG4ODyeHzznnm+d7JEVra8v6DbAO1a4Hxsd/IZXq/s8rnkslm4GBi+zZ8x6bNiW1LqxBLBYln5/n3r3fOH36S54/fw5Un/3Nm7cxxrB//4cKA//H7Xa/PFItlWzcbrcm4xvy4MEEY2N3cLlceuf7GtUKg65dG+Hw4Z6X7V8XFha4cuUaxhhcrrop2XnndHVt49SpfiKRMFevjmAMlMsVLl36nmJxSW2412hwcBjbtrlx40cmJ9OMjo6TzxcAuH37Jx49SuPxaN6uhWVZHD3aRyrVzbFjh3G5XGQy1ZPXdHqKW7dGcbnWr2tsXaz0lmURi20kk5nGGEMm8wfxeGy9h1XXahXY//4MwOfzcfLkCSYmHjM1Na0TmFUyxvD0aY7z57+lt/cQ0Wjkb99WC96CwQCjo+N6tqtU3b0WAGhp8dHZmeTJk1mgOp/37n2f3t6DXL58lXJZ1wSrdeDAR/T1HWL79q0EgwESiRjNzdXCwXg8Rn//54yM3KJYXNTcXaVSqfSyjqX2Ly63uxqsGhsb6e//gmw2x8TE43V5tnUR8SzLYt++Dzh37hvy+Tzp9BTHj3+qk4HXUC5XGBoaZnIyQzb7lAsXviOV6sayLDo7O2hvD3Pw4McMDg5z4sRneDzaab2q5eUVzp79mpWVFcbG7jA29jO7d+8kmUzQ1hYkkYgRCPg5c+Yrurq2EQoF13vIdaNWb1EoLBAMBnj48BE9PZ9gWRZebyNbtmwmGo1w9+6v3L//O7t27dA68YosyyKRqG6yjDGMj98lkYjR1OTF5Wpg69bNxOMxdu7cwfXrP3DkSJ+6nK5CPj/PwMBFotEIhcI8fr+fZDJBOj1FR0ecSKSdnp5uhoaGSSbjeL3etzq+umlUVKtinZnJEo1uJBDYoB/5a6hUDNlsllKpevVSLcyKYNs2y8vLBIMBKhVDLpejra0Nt1t326/KtsvMzmb/UcDm92+gpcXH3FyBcLgNy4K5uQI+XzPNzU3rONr6YoyhXK4wMzPLwkKRSKSdcDgEQC6Xx+/fgMfjZmnpGbZtEwj4tU6sQW29bWry4vV6KRYXaWhooLW1hVKpRD4/T3t7+J1pv1sPjDEsLi4xPT2Dx+OhoyOOx+PmxYsXLC09IxQK/nWqOEcoFHjrG7C6CQMiIiLyZtRFzYCIiIi8OQoDIiIiDqcwICIi4nAKAyIiIg6nMCAiIuJwCgMiIiIOpzAgIiLicAoDIiIiDqcwICIi4nAKAyIiIg6nMCAiIuJwCgMiIiIOpzAgIiLicAoDIiIiDqcwICIi4nAKAyIiIg6nMCAiIuJwCgMiIiIOpzAgIiLicAoDIiIiDqcwICIi4nAKAyIiIg6nMCAiIuJwfwKmg/9EzWFL4AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-x2a3cj8iQx"
      },
      "source": [
        "### Generation (2.5 points)\n",
        "\n",
        "Complete the code in the `generate` method of the Bigram class and generate a mini story using the trained Bigram language model. The model will take in the previous word index and output the next word index.\n",
        "\n",
        "Start with the following seed sentence:\n",
        "    \n",
        "    `\"once upon a time\"`\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNbnTltl8iQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2885682-895f-4e69-ec66-2092d424a3fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# TODO: Specify the path to your trained model\n",
        "model_path = \"/content/drive/MyDrive/ECE_239AS.2/Project3_skeleton/models/bigram/mini_model_checkpoint_90000.pt\"\n",
        "model = BigramLanguageModel(BigramConfig)\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN-EkGV58iQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bcd43e2-ba48-4cfd-f566-2a95a97860c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating text starting with: torch.Size([4])\n",
            "Once upon a time and happy to be goodbye and made a loud fox with his plan, long tired and dad looked on the sharp someone the animals. Timmy was a time, one would sleep a new bottle Joe. She dance and said on a little boy named I the bunny and agreed to see the parts woke the dog named he saw a time, pets flowers got problem all day, a time, but they liked that it to buy him play.\n",
            "One day, but that Lily. The yard was strong. It helped the full of the water. We worry, a picture over it.\" Do she was a picture, \" coined log too. The other.Once upon a time, the park, she angry. She saw a enYour woke together all fireplace sharing and had raised butter as let of his cat remembered from go how loud womanHis friends it!\"\n",
            "M pear opened was very sad and said, they can chest had a time.\"\n",
            "His. It was so stick as picked, \"No,\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "gen_sent = \"Once upon a time\"\n",
        "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
        "print(\"Generating text starting with:\", gen_tokens.shape)\n",
        "gen_tokens = gen_tokens.to(device)\n",
        "model.eval()\n",
        "print(\n",
        "    tokenizer.decode(\n",
        "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBFw2RII8iQx"
      },
      "source": [
        "### Observation and Analysis\n",
        "\n",
        "Please answer the following questions.\n",
        "\n",
        "1. What can we say about the generated text in terms of grammar and coherence?\n",
        "\n",
        "The generated text at first glance looks like English as it strings together words and common short phrases. For example, “Once upon a time,” “One day,” “the animals”. However the grammar is quite fractured. Sentences often start in a familiar “storybook” style but then derail into ungrammatical fragments and there’s no real narrative thread or logical progression. Characters and actions appear out of nowhere, refer back to themselves incoherently, and repeatedly reset.\n",
        "\n",
        "2. What are the limitations of the Bigram language model?\n",
        "\n",
        "A true bigram model conditions each word only on its immediate predecessor. It has no mechanism for capturing longer‐range dependencies and therefore struggles to gather the context of the text. It models only pairwise co‐occurrence counts, so any structure or pattern that spans three or more tokens is entirely lost. Lastly, it can’t generalize grammatical rules well as each pair of words has its own parameter so it learns no abstractions beyond memorization of specific pairs.\n",
        "\n",
        "3. If the model is scaled with more parameters do you expect the bigram model to get substantially better? Why or why not?\n",
        "\n",
        "Simply adding more parameters won’t fundamentally solve these issues. A bigram architecture still only pays attention to one previous token, so it can’t learn to model longer dependencies or higher‐order grammar, no matter how wide or deep the network. You might improve the quality of individual conditional distributions, but you won’t gain true coherence or context beyond two‐word windows. To see substantially better long‐range structure, you need to move to at least a trigram or n-gram model (conditioning on more history) or to a transformer or recurrent architecture that can attend to or remember arbitrarily long contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUjysA8E8iQx"
      },
      "source": [
        "## Mini GPT (90 points)\n",
        "\n",
        "We will implement a decoder style transformer model like we discussed in lecture, which is a scaled down version of the [GPT model](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf).\n",
        "\n",
        "All the model components follow directly from the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. The only difference is we will use prenormalization and learnt positional embeddings instead of fixed ones.\n",
        "\n",
        "We will now implement each layer step by step checking if it is implemented correctly in the process. We will finally put together all our layers to get a fully fledged GPT model.\n",
        "\n",
        "<span style=\"color:red\">Later layers might depend on previous layers so please make sure to check the previous layers before moving on to the next one.</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amQVIve08iQx"
      },
      "source": [
        "### Single Head Causal Attention (20 points)\n",
        "\n",
        "We will first implement the single head causal attention layer. This layer is the same as the scaled dot product attention layer but with a causal mask to prevent the model from looking into the future.\n",
        "\n",
        "Recall that Each head has a Key, Query and Value Matrix and the scaled dot product attention is calculated as :\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "\\end{equation}\n",
        "\n",
        "where $d_k$ is the dimension of the key matrix.\n",
        "\n",
        "Figure below from the original paper shows how the layer is to be implemented.\n",
        "\n",
        "![image](./Images/Single_Head.png)\n",
        "\n",
        "Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsbRRy5U8iQy"
      },
      "source": [
        "Please complete the `SingleHeadAttention` class in `model.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6mJ-kG08iQy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "053452fd-3924-4f5b-e100-b832f8a467e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TEST CASE PASSED!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model = SingleHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.embed_dim//4, MiniGPTConfig.embed_dim//4) # configs are set as such for testing do not modify\n",
        "tests.check_singleheadattention(model, path_to_gpt_tester, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "679obsvh8iQy"
      },
      "source": [
        "### Multi Head Attention (10 points)\n",
        "\n",
        "Now that we have a single head working, we will now scale this across multiple heads, remember that with multihead attention we compute perform head number of parallel attention operations. We then concatenate the outputs of these parallel attention operations and project them back to the desired dimension using an output linear layer.\n",
        "\n",
        "Figure below from the original paper shows how the layer is to be implemented.\n",
        "\n",
        "![image](./Images/MultiHead.png)\n",
        "\n",
        "Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsojCYVX8iQy"
      },
      "source": [
        "Please complete the `MultiHeadAttention` class in `model.py` using the `SingleHeadAttention` class implemented earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apD5fjhy8iQy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "06d7a8dd-649b-42d2-a24e-3c2b42d4dddf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TEST CASE PASSED!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model = MultiHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n",
        "tests.check_multiheadattention(model, path_to_gpt_tester, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBy5l53f8iQy"
      },
      "source": [
        "### Feed Forward Layer (5 points)\n",
        "\n",
        "As discussed in lecture, the attention layer is completely linear, in order to add some non-linearity we add a feed forward layer. The feed forward layer is a simple two layer MLP with a GeLU activation in between.\n",
        "\n",
        "Please complete the `FeedForwardLayer` class in `model.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkVR8ouV8iQy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "18109a6b-2047-4c99-f253-965dd60d6dd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TEST CASE PASSED!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model = FeedForwardLayer(MiniGPTConfig.embed_dim)\n",
        "tests.check_feedforward(model, path_to_gpt_tester, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Su08CI98iQy"
      },
      "source": [
        "### LayerNorm (10 points)\n",
        "\n",
        "We will now implement the layer normalization layer. Layernorm is used across the model to normalize the activations of the previous layer. Recall that the equation for layernorm is given as:\n",
        "\n",
        "\\begin{equation}\n",
        "\n",
        "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\odot \\gamma + \\beta\n",
        "\n",
        "\\end{equation}\n",
        "\n",
        "With the learnable parameters $\\gamma$ and $\\beta$.\n",
        "\n",
        "Remember that unlike batchnorm we compute statistics across the feature dimension and not the batch dimension, hence we do not need to keep track of running averages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzd6jjdh8iQy"
      },
      "source": [
        "Please complete the `LayerNorm` class in `model.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSSwt4sa8iQy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "64dbf28b-5255-430e-9473-35837370b86b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TEST CASE PASSED!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model = LayerNorm(MiniGPTConfig.embed_dim)\n",
        "tests.check_layernorm(model, path_to_gpt_tester, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DmK9qYo8iQy"
      },
      "source": [
        "### Transformer Layer (15 points)\n",
        "\n",
        "We have now implemented all the components of the transformer layer. We will now put it all together to create a transformer layer. The transformer layer consists of a multi head attention layer, a feed forward layer and two layer norm layers.\n",
        "\n",
        "Please use the following order for each component (Varies slightly from the original attention paper):\n",
        "1. LayerNorm\n",
        "2. MultiHeadAttention\n",
        "3. LayerNorm\n",
        "4. FeedForwardLayer\n",
        "\n",
        "Remember that the transformer layer also has residual connections around each sublayer.\n",
        "\n",
        "The below figure shows the structure of the transformer layer you are required to implement.\n",
        "\n",
        "![prenorm_transformer](./Images/Prenorm.png)\n",
        "\n",
        "Image Credit : [CogView](https://arxiv.org/pdf/2105.13290)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI8NxhTM8iQy"
      },
      "source": [
        "Implement the `TransformerLayer` class in `model.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP6j7gUp8iQy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "414a4b61-91ee-47e5-dd23-b2c38ee460d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TEST CASE PASSED!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model =  TransformerLayer(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n",
        "tests.check_transformer(model, path_to_gpt_tester, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UeZsaO08iQy"
      },
      "source": [
        "### Putting it all together : MiniGPT (15 points)\n",
        "\n",
        "We are now ready to put all our layers together to build our own MiniGPT!\n",
        "\n",
        "The MiniGPT model consists of an embedding layer, a positional encoding layer and a stack of transformer layers. The output of the transformer layer is passed through a linear layer (called head) to get the final output logits. Note that in our implementation we will use [weight tying](https://arxiv.org/abs/1608.05859) between the embedding layer and the final linear layer. This allows us to save on parameters and also helps in training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j5es8P78iQy"
      },
      "source": [
        "Implement the `MiniGPT` class in `model.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmunRuP58iQ3"
      },
      "outputs": [],
      "source": [
        "model = MiniGPT(MiniGPTConfig)\n",
        "tests.check_miniGPT(model, path_to_gpt_tester, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nXU5ni48iQ3"
      },
      "source": [
        "### Attempt at training the model (5 points)\n",
        "\n",
        "We will now attempt to train the model on the text data. We will use the same text data as before. If needed, you can scale down the model parameters in the config file to a smaller value to make training feasible.\n",
        "\n",
        "Use the same training script we built for the Bigram model to train the MiniGPT model. If you implemented it correctly it should work just out of the box!\n",
        "\n",
        "**NOTE** : We will not be able to train the model to completion in this assignment. Unfortunately, without access to a relatively powerful GPU, training a large enough model to see good generation is not feasible. However, you should be able to see the loss decreasing over time. <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You do not need to run this for more than 5000 iterations or 1 hour of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2zr5KHM8iQ3"
      },
      "outputs": [],
      "source": [
        "from train import solver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9hdAOgt8iQ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "outputId": "b22ad07f-7e5d-4b14-f261-2eb004754c9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of trainable parameters: 3.32M\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaulcano356\u001b[0m (\u001b[33mpaulcano356-university-of-california-los-angeles\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/ECE_239AS.2/Project3_skeleton/wandb/run-20250518_205340-964tfnm3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3/runs/964tfnm3' target=\"_blank\">faithful-waterfall-14</a></strong> to <a href='https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3' target=\"_blank\">https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3/runs/964tfnm3' target=\"_blank\">https://wandb.ai/paulcano356-university-of-california-los-angeles/dl2_proj3/runs/964tfnm3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function initialized.\n",
            "Optimizer initialized.\n",
            "Starting training loop...\n",
            "Running evaluation at iteration 0...\n",
            "Iteration 0, Train Loss: 10.82659912109375 Eval Loss: 10.829351425170898\n",
            "Running evaluation at iteration 10...\n",
            "Iteration 10, Train Loss: 10.666894912719727 Eval Loss: 10.628310012817384\n",
            "Running evaluation at iteration 20...\n",
            "Iteration 20, Train Loss: 10.40039348602295 Eval Loss: 10.363215923309326\n",
            "Running evaluation at iteration 30...\n",
            "Iteration 30, Train Loss: 10.177623748779297 Eval Loss: 10.089073085784912\n",
            "Running evaluation at iteration 40...\n",
            "Iteration 40, Train Loss: 9.901958465576172 Eval Loss: 9.821290302276612\n",
            "Running evaluation at iteration 50...\n",
            "Iteration 50, Train Loss: 9.45096492767334 Eval Loss: 9.530232238769532\n",
            "Running evaluation at iteration 60...\n",
            "Iteration 60, Train Loss: 9.336501121520996 Eval Loss: 9.166419410705567\n",
            "Running evaluation at iteration 70...\n",
            "Iteration 70, Train Loss: 8.832380294799805 Eval Loss: 8.858261489868164\n",
            "Running evaluation at iteration 80...\n",
            "Iteration 80, Train Loss: 8.55285930633545 Eval Loss: 8.588495349884033\n",
            "Running evaluation at iteration 90...\n",
            "Iteration 90, Train Loss: 8.196763038635254 Eval Loss: 8.327168178558349\n",
            "Running evaluation at iteration 100...\n",
            "Iteration 100, Train Loss: 8.001980781555176 Eval Loss: 7.999632930755615\n",
            "Running evaluation at iteration 110...\n",
            "Iteration 110, Train Loss: 7.554712295532227 Eval Loss: 7.724031257629394\n",
            "Running evaluation at iteration 120...\n",
            "Iteration 120, Train Loss: 7.544680118560791 Eval Loss: 7.436660194396973\n",
            "Running evaluation at iteration 130...\n",
            "Iteration 130, Train Loss: 7.253585338592529 Eval Loss: 7.120119857788086\n",
            "Running evaluation at iteration 140...\n",
            "Iteration 140, Train Loss: 6.926267147064209 Eval Loss: 7.050881862640381\n",
            "Running evaluation at iteration 150...\n",
            "Iteration 150, Train Loss: 6.553880214691162 Eval Loss: 6.849546813964844\n",
            "Running evaluation at iteration 160...\n",
            "Iteration 160, Train Loss: 6.399935245513916 Eval Loss: 6.739768600463867\n",
            "Running evaluation at iteration 170...\n",
            "Iteration 170, Train Loss: 7.063208103179932 Eval Loss: 6.484920644760132\n",
            "Running evaluation at iteration 180...\n",
            "Iteration 180, Train Loss: 6.039620399475098 Eval Loss: 6.470128154754638\n",
            "Running evaluation at iteration 190...\n",
            "Iteration 190, Train Loss: 6.421772480010986 Eval Loss: 6.35600962638855\n",
            "Running evaluation at iteration 200...\n",
            "Iteration 200, Train Loss: 6.181188583374023 Eval Loss: 6.283134412765503\n",
            "Running evaluation at iteration 210...\n",
            "Iteration 210, Train Loss: 6.271562576293945 Eval Loss: 6.118592977523804\n",
            "Running evaluation at iteration 220...\n",
            "Iteration 220, Train Loss: 6.0078125 Eval Loss: 6.107282495498657\n",
            "Running evaluation at iteration 230...\n",
            "Iteration 230, Train Loss: 6.198170185089111 Eval Loss: 6.171777009963989\n",
            "Running evaluation at iteration 240...\n",
            "Iteration 240, Train Loss: 5.781867504119873 Eval Loss: 6.176481103897094\n",
            "Running evaluation at iteration 250...\n",
            "Iteration 250, Train Loss: 5.492490768432617 Eval Loss: 6.139210605621338\n",
            "Running evaluation at iteration 260...\n",
            "Iteration 260, Train Loss: 5.603744983673096 Eval Loss: 5.985781717300415\n",
            "Running evaluation at iteration 270...\n",
            "Iteration 270, Train Loss: 6.369792461395264 Eval Loss: 6.264334678649902\n",
            "Running evaluation at iteration 280...\n",
            "Iteration 280, Train Loss: 6.1239800453186035 Eval Loss: 6.006226062774658\n",
            "Running evaluation at iteration 290...\n",
            "Iteration 290, Train Loss: 6.177565097808838 Eval Loss: 6.059665870666504\n",
            "Running evaluation at iteration 300...\n",
            "Iteration 300, Train Loss: 6.118157386779785 Eval Loss: 6.21924729347229\n",
            "Running evaluation at iteration 310...\n",
            "Iteration 310, Train Loss: 5.644916534423828 Eval Loss: 6.1916710376739506\n",
            "Running evaluation at iteration 320...\n",
            "Iteration 320, Train Loss: 6.353790283203125 Eval Loss: 6.179004859924317\n",
            "Running evaluation at iteration 330...\n",
            "Iteration 330, Train Loss: 6.323944568634033 Eval Loss: 6.092150497436523\n",
            "Running evaluation at iteration 340...\n",
            "Iteration 340, Train Loss: 6.451669692993164 Eval Loss: 6.031239986419678\n",
            "Running evaluation at iteration 350...\n",
            "Iteration 350, Train Loss: 6.232482433319092 Eval Loss: 6.003343152999878\n",
            "Running evaluation at iteration 360...\n",
            "Iteration 360, Train Loss: 5.785444259643555 Eval Loss: 6.1143479347229\n",
            "Running evaluation at iteration 370...\n",
            "Iteration 370, Train Loss: 5.7433295249938965 Eval Loss: 6.020253467559814\n",
            "Running evaluation at iteration 380...\n",
            "Iteration 380, Train Loss: 6.092154026031494 Eval Loss: 5.928869915008545\n",
            "Running evaluation at iteration 390...\n",
            "Iteration 390, Train Loss: 5.542782783508301 Eval Loss: 5.933558082580566\n",
            "Running evaluation at iteration 400...\n",
            "Iteration 400, Train Loss: 6.281729221343994 Eval Loss: 5.991702032089234\n",
            "Running evaluation at iteration 410...\n",
            "Iteration 410, Train Loss: 6.002040386199951 Eval Loss: 5.906711101531982\n",
            "Running evaluation at iteration 420...\n",
            "Iteration 420, Train Loss: 6.119947910308838 Eval Loss: 6.042576122283935\n",
            "Running evaluation at iteration 430...\n",
            "Iteration 430, Train Loss: 6.021986484527588 Eval Loss: 6.044734477996826\n",
            "Running evaluation at iteration 440...\n",
            "Iteration 440, Train Loss: 6.329803466796875 Eval Loss: 6.026510286331177\n",
            "Running evaluation at iteration 450...\n",
            "Iteration 450, Train Loss: 5.812108039855957 Eval Loss: 5.999003458023071\n",
            "Running evaluation at iteration 460...\n",
            "Iteration 460, Train Loss: 6.056821346282959 Eval Loss: 6.043340444564819\n",
            "Running evaluation at iteration 470...\n",
            "Iteration 470, Train Loss: 6.453121185302734 Eval Loss: 5.996259927749634\n",
            "Running evaluation at iteration 480...\n",
            "Iteration 480, Train Loss: 5.952121734619141 Eval Loss: 5.920356702804566\n",
            "Running evaluation at iteration 490...\n",
            "Iteration 490, Train Loss: 5.775819778442383 Eval Loss: 5.95075626373291\n",
            "Running evaluation at iteration 500...\n",
            "Iteration 500, Train Loss: 5.834394454956055 Eval Loss: 5.861731147766113\n",
            "Running evaluation at iteration 510...\n",
            "Iteration 510, Train Loss: 5.975436210632324 Eval Loss: 5.953499174118042\n",
            "Running evaluation at iteration 520...\n",
            "Iteration 520, Train Loss: 5.656062126159668 Eval Loss: 5.98328275680542\n",
            "Running evaluation at iteration 530...\n",
            "Iteration 530, Train Loss: 5.859516620635986 Eval Loss: 6.105200242996216\n",
            "Running evaluation at iteration 540...\n",
            "Iteration 540, Train Loss: 5.680232048034668 Eval Loss: 5.989100646972656\n",
            "Running evaluation at iteration 550...\n",
            "Iteration 550, Train Loss: 5.694892406463623 Eval Loss: 5.895363473892212\n",
            "Running evaluation at iteration 560...\n",
            "Iteration 560, Train Loss: 6.238131046295166 Eval Loss: 5.7592840671539305\n",
            "Running evaluation at iteration 570...\n",
            "Iteration 570, Train Loss: 5.79581356048584 Eval Loss: 5.862336349487305\n",
            "Running evaluation at iteration 580...\n",
            "Iteration 580, Train Loss: 5.499978065490723 Eval Loss: 5.6241147994995115\n",
            "Running evaluation at iteration 590...\n",
            "Iteration 590, Train Loss: 5.680787563323975 Eval Loss: 5.698487567901611\n",
            "Running evaluation at iteration 600...\n",
            "Iteration 600, Train Loss: 5.90579891204834 Eval Loss: 5.83184289932251\n",
            "Running evaluation at iteration 610...\n",
            "Iteration 610, Train Loss: 5.339432239532471 Eval Loss: 5.879595470428467\n",
            "Running evaluation at iteration 620...\n",
            "Iteration 620, Train Loss: 5.406073093414307 Eval Loss: 5.860569334030151\n",
            "Running evaluation at iteration 630...\n",
            "Iteration 630, Train Loss: 6.493029594421387 Eval Loss: 5.859756660461426\n",
            "Running evaluation at iteration 640...\n",
            "Iteration 640, Train Loss: 5.882605075836182 Eval Loss: 5.669009971618652\n",
            "Running evaluation at iteration 650...\n",
            "Iteration 650, Train Loss: 5.800528049468994 Eval Loss: 5.682280254364014\n",
            "Running evaluation at iteration 660...\n",
            "Iteration 660, Train Loss: 5.769748687744141 Eval Loss: 5.747358179092407\n",
            "Running evaluation at iteration 670...\n",
            "Iteration 670, Train Loss: 5.8395562171936035 Eval Loss: 5.640377569198608\n",
            "Running evaluation at iteration 680...\n",
            "Iteration 680, Train Loss: 5.21782112121582 Eval Loss: 5.702678203582764\n",
            "Running evaluation at iteration 690...\n",
            "Iteration 690, Train Loss: 5.819035053253174 Eval Loss: 5.841827011108398\n",
            "Running evaluation at iteration 700...\n",
            "Iteration 700, Train Loss: 5.757712364196777 Eval Loss: 5.563888454437256\n",
            "Running evaluation at iteration 710...\n",
            "Iteration 710, Train Loss: 5.959622383117676 Eval Loss: 5.633672571182251\n",
            "Running evaluation at iteration 720...\n",
            "Iteration 720, Train Loss: 5.743228912353516 Eval Loss: 5.876668071746826\n",
            "Running evaluation at iteration 730...\n",
            "Iteration 730, Train Loss: 5.577410697937012 Eval Loss: 5.726203441619873\n",
            "Running evaluation at iteration 740...\n",
            "Iteration 740, Train Loss: 5.773542404174805 Eval Loss: 5.573177671432495\n",
            "Running evaluation at iteration 750...\n",
            "Iteration 750, Train Loss: 5.61208963394165 Eval Loss: 5.434552001953125\n",
            "Running evaluation at iteration 760...\n",
            "Iteration 760, Train Loss: 5.3574676513671875 Eval Loss: 5.721337175369262\n",
            "Running evaluation at iteration 770...\n",
            "Iteration 770, Train Loss: 5.842258453369141 Eval Loss: 5.604591131210327\n",
            "Running evaluation at iteration 780...\n",
            "Iteration 780, Train Loss: 5.25826358795166 Eval Loss: 5.597780179977417\n",
            "Running evaluation at iteration 790...\n",
            "Iteration 790, Train Loss: 5.811458110809326 Eval Loss: 5.66722846031189\n",
            "Running evaluation at iteration 800...\n",
            "Iteration 800, Train Loss: 5.820224761962891 Eval Loss: 5.750481224060058\n",
            "Running evaluation at iteration 810...\n",
            "Iteration 810, Train Loss: 5.632311820983887 Eval Loss: 5.506317377090454\n",
            "Running evaluation at iteration 820...\n",
            "Iteration 820, Train Loss: 5.357529163360596 Eval Loss: 5.3886490821838375\n",
            "Running evaluation at iteration 830...\n",
            "Iteration 830, Train Loss: 5.779451847076416 Eval Loss: 5.435258197784424\n",
            "Running evaluation at iteration 840...\n",
            "Iteration 840, Train Loss: 5.751056671142578 Eval Loss: 5.443612384796142\n",
            "Running evaluation at iteration 850...\n",
            "Iteration 850, Train Loss: 5.123918533325195 Eval Loss: 5.65353627204895\n",
            "Running evaluation at iteration 860...\n",
            "Iteration 860, Train Loss: 5.752663612365723 Eval Loss: 5.531959486007691\n",
            "Running evaluation at iteration 870...\n",
            "Iteration 870, Train Loss: 5.767421245574951 Eval Loss: 5.52919020652771\n",
            "Running evaluation at iteration 880...\n",
            "Iteration 880, Train Loss: 5.939640998840332 Eval Loss: 5.484390115737915\n",
            "Running evaluation at iteration 890...\n",
            "Iteration 890, Train Loss: 5.3557658195495605 Eval Loss: 5.354265117645264\n",
            "Running evaluation at iteration 900...\n",
            "Iteration 900, Train Loss: 5.381500720977783 Eval Loss: 5.654307413101196\n",
            "Running evaluation at iteration 910...\n",
            "Iteration 910, Train Loss: 5.5810441970825195 Eval Loss: 5.488127279281616\n",
            "Running evaluation at iteration 920...\n",
            "Iteration 920, Train Loss: 5.074453353881836 Eval Loss: 5.4811779975891115\n",
            "Running evaluation at iteration 930...\n",
            "Iteration 930, Train Loss: 5.125211238861084 Eval Loss: 5.424153232574463\n",
            "Running evaluation at iteration 940...\n",
            "Iteration 940, Train Loss: 4.954667091369629 Eval Loss: 5.453696632385254\n",
            "Running evaluation at iteration 950...\n",
            "Iteration 950, Train Loss: 5.412858486175537 Eval Loss: 5.287264251708985\n",
            "Running evaluation at iteration 960...\n",
            "Iteration 960, Train Loss: 4.751209735870361 Eval Loss: 5.263820362091065\n",
            "Running evaluation at iteration 970...\n",
            "Iteration 970, Train Loss: 5.061643123626709 Eval Loss: 5.226536226272583\n",
            "Running evaluation at iteration 980...\n",
            "Iteration 980, Train Loss: 5.534771919250488 Eval Loss: 5.420263814926147\n",
            "Running evaluation at iteration 990...\n",
            "Iteration 990, Train Loss: 5.3872175216674805 Eval Loss: 5.341123867034912\n",
            "Running evaluation at iteration 1000...\n",
            "Iteration 1000, Train Loss: 5.733799934387207 Eval Loss: 5.2579552173614506\n",
            "Running evaluation at iteration 1010...\n",
            "Iteration 1010, Train Loss: 5.653983116149902 Eval Loss: 5.27232551574707\n",
            "Running evaluation at iteration 1020...\n",
            "Iteration 1020, Train Loss: 5.637441635131836 Eval Loss: 5.317327213287354\n",
            "Running evaluation at iteration 1030...\n",
            "Iteration 1030, Train Loss: 5.190561294555664 Eval Loss: 5.117014932632446\n",
            "Running evaluation at iteration 1040...\n",
            "Iteration 1040, Train Loss: 5.5092997550964355 Eval Loss: 5.101741552352905\n",
            "Running evaluation at iteration 1050...\n",
            "Iteration 1050, Train Loss: 5.088434219360352 Eval Loss: 5.046601963043213\n",
            "Running evaluation at iteration 1060...\n",
            "Iteration 1060, Train Loss: 5.192928314208984 Eval Loss: 5.1329758644104\n",
            "Running evaluation at iteration 1070...\n",
            "Iteration 1070, Train Loss: 5.062613487243652 Eval Loss: 5.236895799636841\n",
            "Running evaluation at iteration 1080...\n",
            "Iteration 1080, Train Loss: 5.6457977294921875 Eval Loss: 5.129563570022583\n",
            "Running evaluation at iteration 1090...\n",
            "Iteration 1090, Train Loss: 5.535526752471924 Eval Loss: 5.125016355514527\n",
            "Running evaluation at iteration 1100...\n",
            "Iteration 1100, Train Loss: 5.156348705291748 Eval Loss: 5.227194833755493\n",
            "Running evaluation at iteration 1110...\n",
            "Iteration 1110, Train Loss: 5.2937750816345215 Eval Loss: 5.059395456314087\n",
            "Running evaluation at iteration 1120...\n",
            "Iteration 1120, Train Loss: 5.1887946128845215 Eval Loss: 4.930925178527832\n",
            "Running evaluation at iteration 1130...\n",
            "Iteration 1130, Train Loss: 5.130166053771973 Eval Loss: 5.2318768978118895\n",
            "Running evaluation at iteration 1140...\n",
            "Iteration 1140, Train Loss: 6.008033275604248 Eval Loss: 5.087565851211548\n",
            "Running evaluation at iteration 1150...\n",
            "Iteration 1150, Train Loss: 5.063877582550049 Eval Loss: 4.972843790054322\n",
            "Running evaluation at iteration 1160...\n",
            "Iteration 1160, Train Loss: 4.788173675537109 Eval Loss: 4.971087169647217\n",
            "Running evaluation at iteration 1170...\n",
            "Iteration 1170, Train Loss: 4.70881462097168 Eval Loss: 5.090768146514892\n",
            "Running evaluation at iteration 1180...\n",
            "Iteration 1180, Train Loss: 5.157703399658203 Eval Loss: 5.150588274002075\n",
            "Running evaluation at iteration 1190...\n",
            "Iteration 1190, Train Loss: 4.684908866882324 Eval Loss: 5.221879959106445\n",
            "Running evaluation at iteration 1200...\n",
            "Iteration 1200, Train Loss: 4.797239780426025 Eval Loss: 4.8396097183227536\n",
            "Running evaluation at iteration 1210...\n",
            "Iteration 1210, Train Loss: 5.309298515319824 Eval Loss: 4.85252799987793\n",
            "Running evaluation at iteration 1220...\n",
            "Iteration 1220, Train Loss: 4.4476823806762695 Eval Loss: 5.1128164768219\n",
            "Running evaluation at iteration 1230...\n",
            "Iteration 1230, Train Loss: 4.9435224533081055 Eval Loss: 5.070680332183838\n",
            "Running evaluation at iteration 1240...\n",
            "Iteration 1240, Train Loss: 5.267454147338867 Eval Loss: 4.962308025360107\n",
            "Running evaluation at iteration 1250...\n",
            "Iteration 1250, Train Loss: 5.018604755401611 Eval Loss: 5.104775476455688\n",
            "Running evaluation at iteration 1260...\n",
            "Iteration 1260, Train Loss: 5.041382312774658 Eval Loss: 4.84643144607544\n",
            "Running evaluation at iteration 1270...\n",
            "Iteration 1270, Train Loss: 4.38626766204834 Eval Loss: 4.814480829238891\n",
            "Running evaluation at iteration 1280...\n",
            "Iteration 1280, Train Loss: 4.776182174682617 Eval Loss: 4.914524173736572\n",
            "Running evaluation at iteration 1290...\n",
            "Iteration 1290, Train Loss: 5.662168979644775 Eval Loss: 4.961935329437256\n",
            "Running evaluation at iteration 1300...\n",
            "Iteration 1300, Train Loss: 5.294888973236084 Eval Loss: 4.989327526092529\n",
            "Running evaluation at iteration 1310...\n",
            "Iteration 1310, Train Loss: 4.556525230407715 Eval Loss: 4.775976657867432\n",
            "Running evaluation at iteration 1320...\n",
            "Iteration 1320, Train Loss: 4.958683490753174 Eval Loss: 4.7913275241851805\n",
            "Running evaluation at iteration 1330...\n",
            "Iteration 1330, Train Loss: 4.585784435272217 Eval Loss: 4.799481153488159\n",
            "Running evaluation at iteration 1340...\n",
            "Iteration 1340, Train Loss: 5.008510589599609 Eval Loss: 4.922011041641236\n",
            "Running evaluation at iteration 1350...\n",
            "Iteration 1350, Train Loss: 4.576493740081787 Eval Loss: 4.691230535507202\n",
            "Running evaluation at iteration 1360...\n",
            "Iteration 1360, Train Loss: 4.54388952255249 Eval Loss: 4.860505151748657\n",
            "Running evaluation at iteration 1370...\n",
            "Iteration 1370, Train Loss: 4.977176666259766 Eval Loss: 5.030625247955323\n",
            "Running evaluation at iteration 1380...\n",
            "Iteration 1380, Train Loss: 4.460062026977539 Eval Loss: 4.718480205535888\n",
            "Running evaluation at iteration 1390...\n",
            "Iteration 1390, Train Loss: 4.506350994110107 Eval Loss: 4.832790040969849\n",
            "Running evaluation at iteration 1400...\n",
            "Iteration 1400, Train Loss: 4.954324245452881 Eval Loss: 4.845602321624756\n",
            "Running evaluation at iteration 1410...\n",
            "Iteration 1410, Train Loss: 5.739263534545898 Eval Loss: 4.819293260574341\n",
            "Running evaluation at iteration 1420...\n",
            "Iteration 1420, Train Loss: 4.880913734436035 Eval Loss: 4.566967391967774\n",
            "Running evaluation at iteration 1430...\n",
            "Iteration 1430, Train Loss: 4.526363849639893 Eval Loss: 4.85775237083435\n",
            "Running evaluation at iteration 1440...\n",
            "Iteration 1440, Train Loss: 4.3940300941467285 Eval Loss: 4.6405112743377686\n",
            "Running evaluation at iteration 1450...\n",
            "Iteration 1450, Train Loss: 5.085806846618652 Eval Loss: 4.69692645072937\n",
            "Running evaluation at iteration 1460...\n",
            "Iteration 1460, Train Loss: 5.38359260559082 Eval Loss: 4.761598014831543\n",
            "Running evaluation at iteration 1470...\n",
            "Iteration 1470, Train Loss: 5.093804359436035 Eval Loss: 4.738969230651856\n",
            "Running evaluation at iteration 1480...\n",
            "Iteration 1480, Train Loss: 4.719748497009277 Eval Loss: 4.586051893234253\n",
            "Running evaluation at iteration 1490...\n",
            "Iteration 1490, Train Loss: 5.046985626220703 Eval Loss: 4.785350608825683\n",
            "Running evaluation at iteration 1500...\n",
            "Iteration 1500, Train Loss: 4.645967483520508 Eval Loss: 4.870154857635498\n",
            "Running evaluation at iteration 1510...\n",
            "Iteration 1510, Train Loss: 4.422970294952393 Eval Loss: 4.737600708007813\n",
            "Running evaluation at iteration 1520...\n",
            "Iteration 1520, Train Loss: 4.501025676727295 Eval Loss: 4.676760864257813\n",
            "Running evaluation at iteration 1530...\n",
            "Iteration 1530, Train Loss: 4.380066394805908 Eval Loss: 4.642160177230835\n",
            "Running evaluation at iteration 1540...\n",
            "Iteration 1540, Train Loss: 5.13896369934082 Eval Loss: 4.631750464439392\n",
            "Running evaluation at iteration 1550...\n",
            "Iteration 1550, Train Loss: 4.104544639587402 Eval Loss: 4.802093362808227\n",
            "Running evaluation at iteration 1560...\n",
            "Iteration 1560, Train Loss: 4.485861301422119 Eval Loss: 4.7145609855651855\n",
            "Running evaluation at iteration 1570...\n",
            "Iteration 1570, Train Loss: 5.077298164367676 Eval Loss: 4.77694878578186\n",
            "Running evaluation at iteration 1580...\n",
            "Iteration 1580, Train Loss: 4.741330623626709 Eval Loss: 4.468658137321472\n",
            "Running evaluation at iteration 1590...\n",
            "Iteration 1590, Train Loss: 5.079446315765381 Eval Loss: 4.728518772125244\n",
            "Running evaluation at iteration 1600...\n",
            "Iteration 1600, Train Loss: 5.074526309967041 Eval Loss: 4.524990177154541\n",
            "Running evaluation at iteration 1610...\n",
            "Iteration 1610, Train Loss: 4.5272135734558105 Eval Loss: 4.776302814483643\n",
            "Running evaluation at iteration 1620...\n",
            "Iteration 1620, Train Loss: 4.657153129577637 Eval Loss: 4.5096116065979\n",
            "Running evaluation at iteration 1630...\n",
            "Iteration 1630, Train Loss: 4.697720050811768 Eval Loss: 4.577529621124268\n",
            "Running evaluation at iteration 1640...\n",
            "Iteration 1640, Train Loss: 3.7273647785186768 Eval Loss: 4.472653722763061\n",
            "Running evaluation at iteration 1650...\n",
            "Iteration 1650, Train Loss: 4.795608997344971 Eval Loss: 4.560997772216797\n",
            "Running evaluation at iteration 1660...\n",
            "Iteration 1660, Train Loss: 4.543877601623535 Eval Loss: 4.239857006072998\n",
            "Running evaluation at iteration 1670...\n",
            "Iteration 1670, Train Loss: 4.381110668182373 Eval Loss: 4.505445265769959\n",
            "Running evaluation at iteration 1680...\n",
            "Iteration 1680, Train Loss: 4.2337775230407715 Eval Loss: 4.515539693832397\n",
            "Running evaluation at iteration 1690...\n",
            "Iteration 1690, Train Loss: 4.428956508636475 Eval Loss: 4.6558103322982785\n",
            "Running evaluation at iteration 1700...\n",
            "Iteration 1700, Train Loss: 4.4806227684021 Eval Loss: 4.734718990325928\n",
            "Running evaluation at iteration 1710...\n",
            "Iteration 1710, Train Loss: 4.281054496765137 Eval Loss: 4.614948773384095\n",
            "Running evaluation at iteration 1720...\n",
            "Iteration 1720, Train Loss: 5.036431789398193 Eval Loss: 4.557017755508423\n",
            "Running evaluation at iteration 1730...\n",
            "Iteration 1730, Train Loss: 4.319510459899902 Eval Loss: 4.512400841712951\n",
            "Running evaluation at iteration 1740...\n",
            "Iteration 1740, Train Loss: 3.9567949771881104 Eval Loss: 4.583320999145508\n",
            "Running evaluation at iteration 1750...\n",
            "Iteration 1750, Train Loss: 4.612478733062744 Eval Loss: 4.417835712432861\n",
            "Running evaluation at iteration 1760...\n",
            "Iteration 1760, Train Loss: 4.633038520812988 Eval Loss: 4.321346163749695\n",
            "Running evaluation at iteration 1770...\n",
            "Iteration 1770, Train Loss: 4.477390766143799 Eval Loss: 4.450182771682739\n",
            "Running evaluation at iteration 1780...\n",
            "Iteration 1780, Train Loss: 4.98318338394165 Eval Loss: 4.540543794631958\n",
            "Running evaluation at iteration 1790...\n",
            "Iteration 1790, Train Loss: 4.796864986419678 Eval Loss: 4.682740211486816\n",
            "Running evaluation at iteration 1800...\n",
            "Iteration 1800, Train Loss: 4.473599433898926 Eval Loss: 4.39325532913208\n",
            "Running evaluation at iteration 1810...\n",
            "Iteration 1810, Train Loss: 3.8742568492889404 Eval Loss: 4.540401864051819\n",
            "Running evaluation at iteration 1820...\n",
            "Iteration 1820, Train Loss: 3.9439501762390137 Eval Loss: 4.502902889251709\n",
            "Running evaluation at iteration 1830...\n",
            "Iteration 1830, Train Loss: 4.507267475128174 Eval Loss: 4.422426438331604\n",
            "Running evaluation at iteration 1840...\n",
            "Iteration 1840, Train Loss: 4.7140631675720215 Eval Loss: 4.487895083427429\n",
            "Running evaluation at iteration 1850...\n",
            "Iteration 1850, Train Loss: 4.961275100708008 Eval Loss: 4.468683028221131\n",
            "Running evaluation at iteration 1860...\n",
            "Iteration 1860, Train Loss: 4.498204231262207 Eval Loss: 4.470220947265625\n",
            "Running evaluation at iteration 1870...\n",
            "Iteration 1870, Train Loss: 4.966119289398193 Eval Loss: 4.228180265426635\n",
            "Running evaluation at iteration 1880...\n",
            "Iteration 1880, Train Loss: 4.017515182495117 Eval Loss: 4.521519827842712\n",
            "Running evaluation at iteration 1890...\n",
            "Iteration 1890, Train Loss: 4.487913608551025 Eval Loss: 4.351496458053589\n",
            "Running evaluation at iteration 1900...\n",
            "Iteration 1900, Train Loss: 4.278074264526367 Eval Loss: 4.468064975738526\n",
            "Running evaluation at iteration 1910...\n",
            "Iteration 1910, Train Loss: 3.5632236003875732 Eval Loss: 4.338157200813294\n",
            "Running evaluation at iteration 1920...\n",
            "Iteration 1920, Train Loss: 4.973905563354492 Eval Loss: 4.4046892642974855\n",
            "Running evaluation at iteration 1930...\n",
            "Iteration 1930, Train Loss: 5.311169624328613 Eval Loss: 4.488542175292968\n",
            "Running evaluation at iteration 1940...\n",
            "Iteration 1940, Train Loss: 4.6881585121154785 Eval Loss: 4.396342945098877\n",
            "Running evaluation at iteration 1950...\n",
            "Iteration 1950, Train Loss: 4.365625381469727 Eval Loss: 4.325513100624084\n",
            "Running evaluation at iteration 1960...\n",
            "Iteration 1960, Train Loss: 4.365744590759277 Eval Loss: 4.505995845794677\n",
            "Running evaluation at iteration 1970...\n",
            "Iteration 1970, Train Loss: 4.130420684814453 Eval Loss: 4.440910148620605\n",
            "Running evaluation at iteration 1980...\n",
            "Iteration 1980, Train Loss: 4.878407955169678 Eval Loss: 4.402695989608764\n",
            "Running evaluation at iteration 1990...\n",
            "Iteration 1990, Train Loss: 4.236135482788086 Eval Loss: 4.370891809463501\n",
            "Running evaluation at iteration 2000...\n",
            "Iteration 2000, Train Loss: 4.453033447265625 Eval Loss: 4.373795819282532\n",
            "Running evaluation at iteration 2010...\n",
            "Iteration 2010, Train Loss: 4.267185211181641 Eval Loss: 4.479854989051819\n",
            "Running evaluation at iteration 2020...\n",
            "Iteration 2020, Train Loss: 4.486459255218506 Eval Loss: 4.684004116058349\n",
            "Running evaluation at iteration 2030...\n",
            "Iteration 2030, Train Loss: 4.4045515060424805 Eval Loss: 4.388758611679077\n",
            "Running evaluation at iteration 2040...\n",
            "Iteration 2040, Train Loss: 4.452884674072266 Eval Loss: 4.454039788246154\n",
            "Running evaluation at iteration 2050...\n",
            "Iteration 2050, Train Loss: 4.224826812744141 Eval Loss: 4.383144569396973\n",
            "Running evaluation at iteration 2060...\n",
            "Iteration 2060, Train Loss: 4.7474188804626465 Eval Loss: 4.315500783920288\n",
            "Running evaluation at iteration 2070...\n",
            "Iteration 2070, Train Loss: 4.470314025878906 Eval Loss: 4.24651575088501\n",
            "Running evaluation at iteration 2080...\n",
            "Iteration 2080, Train Loss: 4.651319980621338 Eval Loss: 4.264315342903137\n",
            "Running evaluation at iteration 2090...\n",
            "Iteration 2090, Train Loss: 4.543511867523193 Eval Loss: 4.242269921302795\n",
            "Running evaluation at iteration 2100...\n",
            "Iteration 2100, Train Loss: 4.637816429138184 Eval Loss: 4.335990309715271\n",
            "Running evaluation at iteration 2110...\n",
            "Iteration 2110, Train Loss: 3.911242961883545 Eval Loss: 4.310687494277954\n",
            "Running evaluation at iteration 2120...\n",
            "Iteration 2120, Train Loss: 4.116922855377197 Eval Loss: 4.47002854347229\n",
            "Running evaluation at iteration 2130...\n",
            "Iteration 2130, Train Loss: 4.719876289367676 Eval Loss: 4.2630387306213375\n",
            "Running evaluation at iteration 2140...\n",
            "Iteration 2140, Train Loss: 4.086214065551758 Eval Loss: 4.328443217277527\n",
            "Running evaluation at iteration 2150...\n",
            "Iteration 2150, Train Loss: 5.100712776184082 Eval Loss: 4.477641940116882\n",
            "Running evaluation at iteration 2160...\n",
            "Iteration 2160, Train Loss: 4.667530536651611 Eval Loss: 4.339734268188477\n",
            "Running evaluation at iteration 2170...\n",
            "Iteration 2170, Train Loss: 4.069814682006836 Eval Loss: 4.293328094482422\n",
            "Running evaluation at iteration 2180...\n",
            "Iteration 2180, Train Loss: 4.593031406402588 Eval Loss: 4.448437261581421\n",
            "Running evaluation at iteration 2190...\n",
            "Iteration 2190, Train Loss: 4.124368667602539 Eval Loss: 4.062413358688355\n",
            "Running evaluation at iteration 2200...\n",
            "Iteration 2200, Train Loss: 4.894436836242676 Eval Loss: 4.257958436012268\n",
            "Running evaluation at iteration 2210...\n",
            "Iteration 2210, Train Loss: 3.926833391189575 Eval Loss: 4.212484693527221\n",
            "Running evaluation at iteration 2220...\n",
            "Iteration 2220, Train Loss: 4.232659816741943 Eval Loss: 4.356377172470093\n",
            "Running evaluation at iteration 2230...\n",
            "Iteration 2230, Train Loss: 4.2413740158081055 Eval Loss: 4.372874116897583\n",
            "Running evaluation at iteration 2240...\n",
            "Iteration 2240, Train Loss: 4.097793102264404 Eval Loss: 4.2374530792236325\n",
            "Running evaluation at iteration 2250...\n",
            "Iteration 2250, Train Loss: 4.725489616394043 Eval Loss: 4.497687554359436\n",
            "Running evaluation at iteration 2260...\n",
            "Iteration 2260, Train Loss: 4.2188215255737305 Eval Loss: 4.388937020301819\n",
            "Running evaluation at iteration 2270...\n",
            "Iteration 2270, Train Loss: 3.1757683753967285 Eval Loss: 4.2204211235046385\n",
            "Running evaluation at iteration 2280...\n",
            "Iteration 2280, Train Loss: 4.426540851593018 Eval Loss: 4.165272879600525\n",
            "Running evaluation at iteration 2290...\n",
            "Iteration 2290, Train Loss: 3.834137201309204 Eval Loss: 4.288999152183533\n",
            "Running evaluation at iteration 2300...\n",
            "Iteration 2300, Train Loss: 4.666557788848877 Eval Loss: 4.26873676776886\n",
            "Running evaluation at iteration 2310...\n",
            "Iteration 2310, Train Loss: 3.681715488433838 Eval Loss: 4.1061417818069454\n",
            "Running evaluation at iteration 2320...\n",
            "Iteration 2320, Train Loss: 4.499055862426758 Eval Loss: 4.464135360717774\n",
            "Running evaluation at iteration 2330...\n",
            "Iteration 2330, Train Loss: 4.382584095001221 Eval Loss: 4.294766426086426\n",
            "Running evaluation at iteration 2340...\n",
            "Iteration 2340, Train Loss: 4.378939628601074 Eval Loss: 4.191617822647094\n",
            "Running evaluation at iteration 2350...\n",
            "Iteration 2350, Train Loss: 3.9913330078125 Eval Loss: 4.544856429100037\n",
            "Running evaluation at iteration 2360...\n",
            "Iteration 2360, Train Loss: 3.4301280975341797 Eval Loss: 4.251429557800293\n",
            "Running evaluation at iteration 2370...\n",
            "Iteration 2370, Train Loss: 4.208658695220947 Eval Loss: 4.16457166671753\n",
            "Running evaluation at iteration 2380...\n",
            "Iteration 2380, Train Loss: 4.393821716308594 Eval Loss: 4.149875164031982\n",
            "Running evaluation at iteration 2390...\n",
            "Iteration 2390, Train Loss: 4.331258773803711 Eval Loss: 4.123360800743103\n",
            "Running evaluation at iteration 2400...\n",
            "Iteration 2400, Train Loss: 3.7654895782470703 Eval Loss: 4.338349485397339\n",
            "Running evaluation at iteration 2410...\n",
            "Iteration 2410, Train Loss: 3.620269536972046 Eval Loss: 4.376668071746826\n",
            "Running evaluation at iteration 2420...\n",
            "Iteration 2420, Train Loss: 4.263979911804199 Eval Loss: 4.187650918960571\n",
            "Running evaluation at iteration 2430...\n",
            "Iteration 2430, Train Loss: 3.998509407043457 Eval Loss: 4.394693112373352\n",
            "Running evaluation at iteration 2440...\n",
            "Iteration 2440, Train Loss: 4.341217994689941 Eval Loss: 4.134746336936951\n",
            "Running evaluation at iteration 2450...\n",
            "Iteration 2450, Train Loss: 3.963088274002075 Eval Loss: 4.152726721763611\n",
            "Running evaluation at iteration 2460...\n",
            "Iteration 2460, Train Loss: 4.071836948394775 Eval Loss: 4.1612440824508665\n",
            "Running evaluation at iteration 2470...\n",
            "Iteration 2470, Train Loss: 4.007524490356445 Eval Loss: 4.235689234733582\n",
            "Running evaluation at iteration 2480...\n",
            "Iteration 2480, Train Loss: 4.177679538726807 Eval Loss: 4.002745342254639\n",
            "Running evaluation at iteration 2490...\n",
            "Iteration 2490, Train Loss: 3.9438648223876953 Eval Loss: 4.34213490486145\n",
            "Running evaluation at iteration 2500...\n",
            "Iteration 2500, Train Loss: 3.918670654296875 Eval Loss: 4.5430803298950195\n",
            "Running evaluation at iteration 2510...\n",
            "Iteration 2510, Train Loss: 3.7504777908325195 Eval Loss: 4.127523303031921\n",
            "Running evaluation at iteration 2520...\n",
            "Iteration 2520, Train Loss: 4.513680934906006 Eval Loss: 4.168321204185486\n",
            "Running evaluation at iteration 2530...\n",
            "Iteration 2530, Train Loss: 3.8641562461853027 Eval Loss: 4.112120747566223\n",
            "Running evaluation at iteration 2540...\n",
            "Iteration 2540, Train Loss: 4.018199443817139 Eval Loss: 4.053048372268677\n",
            "Running evaluation at iteration 2550...\n",
            "Iteration 2550, Train Loss: 4.386847496032715 Eval Loss: 3.918368434906006\n",
            "Running evaluation at iteration 2560...\n",
            "Iteration 2560, Train Loss: 4.360377788543701 Eval Loss: 4.2754922151565555\n",
            "Running evaluation at iteration 2570...\n",
            "Iteration 2570, Train Loss: 4.8627610206604 Eval Loss: 4.2479335308074955\n",
            "Running evaluation at iteration 2580...\n",
            "Iteration 2580, Train Loss: 4.913539886474609 Eval Loss: 4.193730306625366\n",
            "Running evaluation at iteration 2590...\n",
            "Iteration 2590, Train Loss: 4.313238620758057 Eval Loss: 4.158823537826538\n",
            "Running evaluation at iteration 2600...\n",
            "Iteration 2600, Train Loss: 4.366755962371826 Eval Loss: 3.9932543992996217\n",
            "Running evaluation at iteration 2610...\n",
            "Iteration 2610, Train Loss: 4.552786827087402 Eval Loss: 4.297841238975525\n",
            "Running evaluation at iteration 2620...\n",
            "Iteration 2620, Train Loss: 4.4042863845825195 Eval Loss: 4.059577894210816\n",
            "Running evaluation at iteration 2630...\n",
            "Iteration 2630, Train Loss: 3.9443414211273193 Eval Loss: 4.208801364898681\n",
            "Running evaluation at iteration 2640...\n",
            "Iteration 2640, Train Loss: 3.8403613567352295 Eval Loss: 4.193224716186523\n",
            "Running evaluation at iteration 2650...\n",
            "Iteration 2650, Train Loss: 4.306401252746582 Eval Loss: 3.930129313468933\n",
            "Running evaluation at iteration 2660...\n",
            "Iteration 2660, Train Loss: 4.27549934387207 Eval Loss: 4.190603971481323\n",
            "Running evaluation at iteration 2670...\n",
            "Iteration 2670, Train Loss: 4.135852813720703 Eval Loss: 4.058877038955688\n",
            "Running evaluation at iteration 2680...\n",
            "Iteration 2680, Train Loss: 4.610623359680176 Eval Loss: 4.222296643257141\n",
            "Running evaluation at iteration 2690...\n",
            "Iteration 2690, Train Loss: 4.474567413330078 Eval Loss: 4.186837267875672\n",
            "Running evaluation at iteration 2700...\n",
            "Iteration 2700, Train Loss: 3.8354151248931885 Eval Loss: 4.204907822608948\n",
            "Running evaluation at iteration 2710...\n",
            "Iteration 2710, Train Loss: 4.456419467926025 Eval Loss: 4.292530202865601\n",
            "Running evaluation at iteration 2720...\n",
            "Iteration 2720, Train Loss: 4.7526774406433105 Eval Loss: 4.28302686214447\n",
            "Running evaluation at iteration 2730...\n",
            "Iteration 2730, Train Loss: 3.791604995727539 Eval Loss: 4.331203317642212\n",
            "Running evaluation at iteration 2740...\n",
            "Iteration 2740, Train Loss: 4.399761199951172 Eval Loss: 4.1931610107421875\n",
            "Running evaluation at iteration 2750...\n",
            "Iteration 2750, Train Loss: 3.387150287628174 Eval Loss: 4.225623631477356\n",
            "Running evaluation at iteration 2760...\n",
            "Iteration 2760, Train Loss: 4.792189121246338 Eval Loss: 4.420087552070617\n",
            "Running evaluation at iteration 2770...\n",
            "Iteration 2770, Train Loss: 4.07683801651001 Eval Loss: 4.063521361351013\n",
            "Running evaluation at iteration 2780...\n",
            "Iteration 2780, Train Loss: 3.9273786544799805 Eval Loss: 4.074357509613037\n",
            "Running evaluation at iteration 2790...\n",
            "Iteration 2790, Train Loss: 3.8648908138275146 Eval Loss: 4.068300127983093\n",
            "Running evaluation at iteration 2800...\n",
            "Iteration 2800, Train Loss: 4.259952068328857 Eval Loss: 4.158208847045898\n",
            "Running evaluation at iteration 2810...\n",
            "Iteration 2810, Train Loss: 4.593207836151123 Eval Loss: 4.0188902616500854\n",
            "Running evaluation at iteration 2820...\n",
            "Iteration 2820, Train Loss: 4.545645236968994 Eval Loss: 3.9481407165527345\n",
            "Running evaluation at iteration 2830...\n",
            "Iteration 2830, Train Loss: 4.188858509063721 Eval Loss: 4.0620097637176515\n",
            "Running evaluation at iteration 2840...\n",
            "Iteration 2840, Train Loss: 3.405205011367798 Eval Loss: 4.1779917001724245\n",
            "Running evaluation at iteration 2850...\n",
            "Iteration 2850, Train Loss: 3.3598523139953613 Eval Loss: 4.091723895072937\n",
            "Running evaluation at iteration 2860...\n",
            "Iteration 2860, Train Loss: 3.879646062850952 Eval Loss: 3.9560611724853514\n",
            "Running evaluation at iteration 2870...\n",
            "Iteration 2870, Train Loss: 4.248583793640137 Eval Loss: 4.281935119628907\n",
            "Running evaluation at iteration 2880...\n",
            "Iteration 2880, Train Loss: 3.810856819152832 Eval Loss: 4.1920978307724\n",
            "Running evaluation at iteration 2890...\n",
            "Iteration 2890, Train Loss: 4.122197151184082 Eval Loss: 4.125907850265503\n",
            "Running evaluation at iteration 2900...\n",
            "Iteration 2900, Train Loss: 4.287987232208252 Eval Loss: 4.161708998680115\n",
            "Running evaluation at iteration 2910...\n",
            "Iteration 2910, Train Loss: 3.984342575073242 Eval Loss: 4.184858441352844\n",
            "Running evaluation at iteration 2920...\n",
            "Iteration 2920, Train Loss: 4.304056644439697 Eval Loss: 3.990016722679138\n",
            "Running evaluation at iteration 2930...\n",
            "Iteration 2930, Train Loss: 4.504941940307617 Eval Loss: 4.061147308349609\n",
            "Running evaluation at iteration 2940...\n",
            "Iteration 2940, Train Loss: 4.249455451965332 Eval Loss: 4.101014351844787\n",
            "Running evaluation at iteration 2950...\n",
            "Iteration 2950, Train Loss: 4.428665637969971 Eval Loss: 3.9676838874816895\n",
            "Running evaluation at iteration 2960...\n",
            "Iteration 2960, Train Loss: 4.173152923583984 Eval Loss: 3.993666887283325\n",
            "Running evaluation at iteration 2970...\n",
            "Iteration 2970, Train Loss: 3.6269803047180176 Eval Loss: 4.036268281936645\n",
            "Running evaluation at iteration 2980...\n",
            "Iteration 2980, Train Loss: 4.171341896057129 Eval Loss: 4.023172235488891\n",
            "Running evaluation at iteration 2990...\n",
            "Iteration 2990, Train Loss: 4.524181365966797 Eval Loss: 4.280140995979309\n",
            "Running evaluation at iteration 3000...\n",
            "Iteration 3000, Train Loss: 4.053831577301025 Eval Loss: 4.1766911268234255\n",
            "Running evaluation at iteration 3010...\n",
            "Iteration 3010, Train Loss: 4.375054836273193 Eval Loss: 3.960577392578125\n",
            "Running evaluation at iteration 3020...\n",
            "Iteration 3020, Train Loss: 4.759902477264404 Eval Loss: 4.14586226940155\n",
            "Running evaluation at iteration 3030...\n",
            "Iteration 3030, Train Loss: 3.9703125953674316 Eval Loss: 4.076808738708496\n",
            "Running evaluation at iteration 3040...\n",
            "Iteration 3040, Train Loss: 4.094785690307617 Eval Loss: 4.168162846565247\n",
            "Running evaluation at iteration 3050...\n",
            "Iteration 3050, Train Loss: 3.847316265106201 Eval Loss: 4.144403195381164\n",
            "Running evaluation at iteration 3060...\n",
            "Iteration 3060, Train Loss: 3.747152090072632 Eval Loss: 3.9747710943222048\n",
            "Running evaluation at iteration 3070...\n",
            "Iteration 3070, Train Loss: 3.864166021347046 Eval Loss: 4.0939603567123415\n",
            "Running evaluation at iteration 3080...\n",
            "Iteration 3080, Train Loss: 3.8835535049438477 Eval Loss: 3.9751590490341187\n",
            "Running evaluation at iteration 3090...\n",
            "Iteration 3090, Train Loss: 4.179879665374756 Eval Loss: 3.9536437511444094\n",
            "Running evaluation at iteration 3100...\n",
            "Iteration 3100, Train Loss: 4.438929080963135 Eval Loss: 4.124177169799805\n",
            "Running evaluation at iteration 3110...\n",
            "Iteration 3110, Train Loss: 3.6181225776672363 Eval Loss: 4.220866847038269\n",
            "Running evaluation at iteration 3120...\n",
            "Iteration 3120, Train Loss: 3.8788700103759766 Eval Loss: 4.087633323669434\n",
            "Running evaluation at iteration 3130...\n",
            "Iteration 3130, Train Loss: 3.8779492378234863 Eval Loss: 4.105732131004333\n",
            "Running evaluation at iteration 3140...\n",
            "Iteration 3140, Train Loss: 3.5532240867614746 Eval Loss: 3.8678728580474853\n",
            "Running evaluation at iteration 3150...\n",
            "Iteration 3150, Train Loss: 4.006368637084961 Eval Loss: 3.9854262828826905\n",
            "Running evaluation at iteration 3160...\n",
            "Iteration 3160, Train Loss: 3.8574156761169434 Eval Loss: 4.046054649353027\n",
            "Running evaluation at iteration 3170...\n",
            "Iteration 3170, Train Loss: 3.840189218521118 Eval Loss: 3.9971910238265993\n",
            "Running evaluation at iteration 3180...\n",
            "Iteration 3180, Train Loss: 3.9385275840759277 Eval Loss: 3.935137391090393\n",
            "Running evaluation at iteration 3190...\n",
            "Iteration 3190, Train Loss: 4.115381717681885 Eval Loss: 3.8397539615631104\n",
            "Running evaluation at iteration 3200...\n",
            "Iteration 3200, Train Loss: 4.265368461608887 Eval Loss: 4.126459264755249\n",
            "Running evaluation at iteration 3210...\n",
            "Iteration 3210, Train Loss: 4.773739337921143 Eval Loss: 3.988869047164917\n",
            "Running evaluation at iteration 3220...\n",
            "Iteration 3220, Train Loss: 4.497635364532471 Eval Loss: 4.177440166473389\n",
            "Running evaluation at iteration 3230...\n",
            "Iteration 3230, Train Loss: 3.813788652420044 Eval Loss: 3.8222519159317017\n",
            "Running evaluation at iteration 3240...\n",
            "Iteration 3240, Train Loss: 3.8671181201934814 Eval Loss: 4.139735078811645\n",
            "Running evaluation at iteration 3250...\n",
            "Iteration 3250, Train Loss: 4.149798393249512 Eval Loss: 4.230685424804688\n",
            "Running evaluation at iteration 3260...\n",
            "Iteration 3260, Train Loss: 3.7540676593780518 Eval Loss: 4.129268860816955\n",
            "Running evaluation at iteration 3270...\n",
            "Iteration 3270, Train Loss: 3.996107578277588 Eval Loss: 4.16000075340271\n",
            "Running evaluation at iteration 3280...\n",
            "Iteration 3280, Train Loss: 4.19788932800293 Eval Loss: 3.7620641469955443\n",
            "Running evaluation at iteration 3290...\n",
            "Iteration 3290, Train Loss: 4.379022121429443 Eval Loss: 3.8576988220214843\n",
            "Running evaluation at iteration 3300...\n",
            "Iteration 3300, Train Loss: 4.128283500671387 Eval Loss: 4.057330322265625\n",
            "Running evaluation at iteration 3310...\n",
            "Iteration 3310, Train Loss: 4.077886581420898 Eval Loss: 4.060524344444275\n",
            "Running evaluation at iteration 3320...\n",
            "Iteration 3320, Train Loss: 4.055421352386475 Eval Loss: 3.7900500297546387\n",
            "Running evaluation at iteration 3330...\n",
            "Iteration 3330, Train Loss: 4.521790981292725 Eval Loss: 4.026699352264404\n",
            "Running evaluation at iteration 3340...\n",
            "Iteration 3340, Train Loss: 4.062806606292725 Eval Loss: 4.1169356346130375\n",
            "Running evaluation at iteration 3350...\n",
            "Iteration 3350, Train Loss: 4.364577770233154 Eval Loss: 3.8520161151885985\n",
            "Running evaluation at iteration 3360...\n",
            "Iteration 3360, Train Loss: 4.61063289642334 Eval Loss: 3.8043261528015138\n",
            "Running evaluation at iteration 3370...\n",
            "Iteration 3370, Train Loss: 3.4440667629241943 Eval Loss: 4.042042255401611\n",
            "Running evaluation at iteration 3380...\n",
            "Iteration 3380, Train Loss: 3.8067846298217773 Eval Loss: 3.8542378664016725\n",
            "Running evaluation at iteration 3390...\n",
            "Iteration 3390, Train Loss: 4.199352741241455 Eval Loss: 3.7958476543426514\n",
            "Running evaluation at iteration 3400...\n",
            "Iteration 3400, Train Loss: 4.02388858795166 Eval Loss: 4.0669771194458\n",
            "Running evaluation at iteration 3410...\n",
            "Iteration 3410, Train Loss: 3.533327102661133 Eval Loss: 4.024662137031555\n",
            "Running evaluation at iteration 3420...\n",
            "Iteration 3420, Train Loss: 4.571839332580566 Eval Loss: 3.887470746040344\n",
            "Running evaluation at iteration 3430...\n",
            "Iteration 3430, Train Loss: 4.170677185058594 Eval Loss: 4.1030200004577635\n",
            "Running evaluation at iteration 3440...\n",
            "Iteration 3440, Train Loss: 3.8406057357788086 Eval Loss: 4.005803275108337\n",
            "Running evaluation at iteration 3450...\n",
            "Iteration 3450, Train Loss: 4.455300807952881 Eval Loss: 4.021210169792175\n",
            "Running evaluation at iteration 3460...\n",
            "Iteration 3460, Train Loss: 3.9485208988189697 Eval Loss: 3.81348876953125\n",
            "Running evaluation at iteration 3470...\n",
            "Iteration 3470, Train Loss: 3.7037699222564697 Eval Loss: 4.227805948257446\n",
            "Running evaluation at iteration 3480...\n",
            "Iteration 3480, Train Loss: 4.2631449699401855 Eval Loss: 4.023043513298035\n",
            "Running evaluation at iteration 3490...\n",
            "Iteration 3490, Train Loss: 3.6864407062530518 Eval Loss: 4.004006338119507\n",
            "Running evaluation at iteration 3500...\n",
            "Iteration 3500, Train Loss: 4.6276679039001465 Eval Loss: 4.101772689819336\n",
            "Running evaluation at iteration 3510...\n",
            "Iteration 3510, Train Loss: 4.8193254470825195 Eval Loss: 4.124058985710144\n",
            "Running evaluation at iteration 3520...\n",
            "Iteration 3520, Train Loss: 3.8529324531555176 Eval Loss: 3.841957521438599\n",
            "Running evaluation at iteration 3530...\n",
            "Iteration 3530, Train Loss: 4.15687370300293 Eval Loss: 4.208169150352478\n",
            "Running evaluation at iteration 3540...\n",
            "Iteration 3540, Train Loss: 4.124467849731445 Eval Loss: 3.895446014404297\n",
            "Running evaluation at iteration 3550...\n",
            "Iteration 3550, Train Loss: 3.871478796005249 Eval Loss: 3.88971529006958\n",
            "Running evaluation at iteration 3560...\n",
            "Iteration 3560, Train Loss: 3.352766990661621 Eval Loss: 3.957528758049011\n",
            "Running evaluation at iteration 3570...\n",
            "Iteration 3570, Train Loss: 3.6236233711242676 Eval Loss: 4.00899567604065\n",
            "Running evaluation at iteration 3580...\n",
            "Iteration 3580, Train Loss: 4.29671049118042 Eval Loss: 4.241154360771179\n",
            "Running evaluation at iteration 3590...\n",
            "Iteration 3590, Train Loss: 3.946866512298584 Eval Loss: 3.8659494400024412\n",
            "Running evaluation at iteration 3600...\n",
            "Iteration 3600, Train Loss: 3.9130303859710693 Eval Loss: 4.197473883628845\n",
            "Running evaluation at iteration 3610...\n",
            "Iteration 3610, Train Loss: 4.5846638679504395 Eval Loss: 3.9647735357284546\n",
            "Running evaluation at iteration 3620...\n",
            "Iteration 3620, Train Loss: 4.088784217834473 Eval Loss: 3.8299734115600588\n",
            "Running evaluation at iteration 3630...\n",
            "Iteration 3630, Train Loss: 3.398611545562744 Eval Loss: 4.161537122726441\n",
            "Running evaluation at iteration 3640...\n",
            "Iteration 3640, Train Loss: 4.003336429595947 Eval Loss: 4.005193853378296\n",
            "Running evaluation at iteration 3650...\n",
            "Iteration 3650, Train Loss: 3.7879638671875 Eval Loss: 3.8989628076553347\n",
            "Running evaluation at iteration 3660...\n",
            "Iteration 3660, Train Loss: 4.028319835662842 Eval Loss: 4.054842066764832\n",
            "Running evaluation at iteration 3670...\n",
            "Iteration 3670, Train Loss: 4.005124092102051 Eval Loss: 3.83614604473114\n",
            "Running evaluation at iteration 3680...\n",
            "Iteration 3680, Train Loss: 4.262066841125488 Eval Loss: 3.827688193321228\n",
            "Running evaluation at iteration 3690...\n",
            "Iteration 3690, Train Loss: 4.01692533493042 Eval Loss: 3.911845827102661\n",
            "Running evaluation at iteration 3700...\n",
            "Iteration 3700, Train Loss: 4.30118465423584 Eval Loss: 3.9705456495285034\n",
            "Running evaluation at iteration 3710...\n",
            "Iteration 3710, Train Loss: 3.8513262271881104 Eval Loss: 4.017710828781128\n",
            "Running evaluation at iteration 3720...\n",
            "Iteration 3720, Train Loss: 3.894038200378418 Eval Loss: 3.929119849205017\n",
            "Running evaluation at iteration 3730...\n",
            "Iteration 3730, Train Loss: 3.559877634048462 Eval Loss: 4.0853698492050174\n",
            "Running evaluation at iteration 3740...\n",
            "Iteration 3740, Train Loss: 4.281981468200684 Eval Loss: 3.9938614130020142\n",
            "Running evaluation at iteration 3750...\n",
            "Iteration 3750, Train Loss: 4.390125751495361 Eval Loss: 3.8394992351531982\n",
            "Running evaluation at iteration 3760...\n",
            "Iteration 3760, Train Loss: 4.595968723297119 Eval Loss: 3.8767239809036256\n",
            "Running evaluation at iteration 3770...\n",
            "Iteration 3770, Train Loss: 3.1717493534088135 Eval Loss: 3.7616719007492065\n",
            "Running evaluation at iteration 3780...\n",
            "Iteration 3780, Train Loss: 4.214670181274414 Eval Loss: 3.978309893608093\n",
            "Running evaluation at iteration 3790...\n",
            "Iteration 3790, Train Loss: 4.336551666259766 Eval Loss: 3.875560736656189\n",
            "Running evaluation at iteration 3800...\n",
            "Iteration 3800, Train Loss: 3.9082202911376953 Eval Loss: 4.215608143806458\n",
            "Running evaluation at iteration 3810...\n",
            "Iteration 3810, Train Loss: 4.038425445556641 Eval Loss: 3.9327454566955566\n",
            "Running evaluation at iteration 3820...\n",
            "Iteration 3820, Train Loss: 4.439512252807617 Eval Loss: 3.9218074321746825\n",
            "Running evaluation at iteration 3830...\n",
            "Iteration 3830, Train Loss: 3.608750343322754 Eval Loss: 3.953957176208496\n",
            "Running evaluation at iteration 3840...\n",
            "Iteration 3840, Train Loss: 3.8402652740478516 Eval Loss: 3.943916988372803\n",
            "Running evaluation at iteration 3850...\n",
            "Iteration 3850, Train Loss: 4.22037410736084 Eval Loss: 4.010155200958252\n",
            "Running evaluation at iteration 3860...\n",
            "Iteration 3860, Train Loss: 3.6822779178619385 Eval Loss: 3.7361261367797853\n",
            "Running evaluation at iteration 3870...\n",
            "Iteration 3870, Train Loss: 4.253532886505127 Eval Loss: 4.029421401023865\n",
            "Running evaluation at iteration 3880...\n",
            "Iteration 3880, Train Loss: 4.348076820373535 Eval Loss: 3.9677212715148924\n",
            "Running evaluation at iteration 3890...\n",
            "Iteration 3890, Train Loss: 2.337599277496338 Eval Loss: 3.910494589805603\n",
            "Running evaluation at iteration 3900...\n",
            "Iteration 3900, Train Loss: 4.012928009033203 Eval Loss: 3.688750123977661\n",
            "Running evaluation at iteration 3910...\n",
            "Iteration 3910, Train Loss: 4.423912525177002 Eval Loss: 3.913129281997681\n",
            "Running evaluation at iteration 3920...\n",
            "Iteration 3920, Train Loss: 4.211021900177002 Eval Loss: 3.869111657142639\n",
            "Running evaluation at iteration 3930...\n",
            "Iteration 3930, Train Loss: 3.817169189453125 Eval Loss: 3.9138626575469972\n",
            "Running evaluation at iteration 3940...\n",
            "Iteration 3940, Train Loss: 4.165115833282471 Eval Loss: 4.043711996078491\n",
            "Running evaluation at iteration 3950...\n",
            "Iteration 3950, Train Loss: 4.085910797119141 Eval Loss: 3.772568440437317\n",
            "Running evaluation at iteration 3960...\n",
            "Iteration 3960, Train Loss: 3.8743810653686523 Eval Loss: 3.9671136140823364\n",
            "Running evaluation at iteration 3970...\n",
            "Iteration 3970, Train Loss: 3.5066497325897217 Eval Loss: 3.815057563781738\n",
            "Running evaluation at iteration 3980...\n",
            "Iteration 3980, Train Loss: 3.681187152862549 Eval Loss: 3.918547248840332\n",
            "Running evaluation at iteration 3990...\n",
            "Iteration 3990, Train Loss: 4.4954447746276855 Eval Loss: 3.9529560089111326\n",
            "Running evaluation at iteration 4000...\n",
            "Iteration 4000, Train Loss: 4.173912525177002 Eval Loss: 3.90564911365509\n",
            "Running evaluation at iteration 4010...\n",
            "Iteration 4010, Train Loss: 4.372478485107422 Eval Loss: 3.9614502668380736\n",
            "Running evaluation at iteration 4020...\n",
            "Iteration 4020, Train Loss: 4.212750434875488 Eval Loss: 3.8427366256713866\n",
            "Running evaluation at iteration 4030...\n",
            "Iteration 4030, Train Loss: 3.8829421997070312 Eval Loss: 3.8095811367034913\n",
            "Running evaluation at iteration 4040...\n",
            "Iteration 4040, Train Loss: 4.661560535430908 Eval Loss: 4.016736483573913\n",
            "Running evaluation at iteration 4050...\n",
            "Iteration 4050, Train Loss: 4.559096336364746 Eval Loss: 3.8120040893554688\n",
            "Running evaluation at iteration 4060...\n",
            "Iteration 4060, Train Loss: 3.4629311561584473 Eval Loss: 3.6997789621353148\n",
            "Running evaluation at iteration 4070...\n",
            "Iteration 4070, Train Loss: 3.6779069900512695 Eval Loss: 3.8687456607818604\n",
            "Running evaluation at iteration 4080...\n",
            "Iteration 4080, Train Loss: 3.2990317344665527 Eval Loss: 3.8812911033630373\n",
            "Running evaluation at iteration 4090...\n",
            "Iteration 4090, Train Loss: 3.923076868057251 Eval Loss: 3.8829736948013305\n",
            "Running evaluation at iteration 4100...\n",
            "Iteration 4100, Train Loss: 3.3866078853607178 Eval Loss: 3.8233972072601317\n",
            "Running evaluation at iteration 4110...\n",
            "Iteration 4110, Train Loss: 4.92573356628418 Eval Loss: 3.897625517845154\n",
            "Running evaluation at iteration 4120...\n",
            "Iteration 4120, Train Loss: 4.217045783996582 Eval Loss: 3.962170720100403\n",
            "Running evaluation at iteration 4130...\n",
            "Iteration 4130, Train Loss: 3.3379604816436768 Eval Loss: 4.104532146453858\n",
            "Running evaluation at iteration 4140...\n",
            "Iteration 4140, Train Loss: 4.3711676597595215 Eval Loss: 3.771002745628357\n",
            "Running evaluation at iteration 4150...\n",
            "Iteration 4150, Train Loss: 3.777937889099121 Eval Loss: 3.934698534011841\n",
            "Running evaluation at iteration 4160...\n",
            "Iteration 4160, Train Loss: 3.9607205390930176 Eval Loss: 4.058898138999939\n",
            "Running evaluation at iteration 4170...\n",
            "Iteration 4170, Train Loss: 4.35084342956543 Eval Loss: 3.8499035835266113\n",
            "Running evaluation at iteration 4180...\n",
            "Iteration 4180, Train Loss: 4.3572564125061035 Eval Loss: 3.9153159141540526\n",
            "Running evaluation at iteration 4190...\n",
            "Iteration 4190, Train Loss: 3.495009660720825 Eval Loss: 3.952867317199707\n",
            "Running evaluation at iteration 4200...\n",
            "Iteration 4200, Train Loss: 4.1525750160217285 Eval Loss: 3.861364006996155\n",
            "Running evaluation at iteration 4210...\n",
            "Iteration 4210, Train Loss: 3.9235970973968506 Eval Loss: 3.911561465263367\n",
            "Running evaluation at iteration 4220...\n",
            "Iteration 4220, Train Loss: 3.1188156604766846 Eval Loss: 3.9849613904953003\n",
            "Running evaluation at iteration 4230...\n",
            "Iteration 4230, Train Loss: 4.1133713722229 Eval Loss: 3.782361054420471\n",
            "Running evaluation at iteration 4240...\n",
            "Iteration 4240, Train Loss: 3.969465732574463 Eval Loss: 4.014412856101989\n",
            "Running evaluation at iteration 4250...\n",
            "Iteration 4250, Train Loss: 3.3652756214141846 Eval Loss: 3.728314733505249\n",
            "Running evaluation at iteration 4260...\n",
            "Iteration 4260, Train Loss: 4.418834686279297 Eval Loss: 4.179669594764709\n",
            "Running evaluation at iteration 4270...\n",
            "Iteration 4270, Train Loss: 4.202810287475586 Eval Loss: 3.808389949798584\n",
            "Running evaluation at iteration 4280...\n",
            "Iteration 4280, Train Loss: 4.172814846038818 Eval Loss: 4.061711311340332\n",
            "Running evaluation at iteration 4290...\n",
            "Iteration 4290, Train Loss: 3.7049977779388428 Eval Loss: 4.018100714683532\n",
            "Running evaluation at iteration 4300...\n",
            "Iteration 4300, Train Loss: 3.712554693222046 Eval Loss: 3.9231482982635497\n",
            "Running evaluation at iteration 4310...\n",
            "Iteration 4310, Train Loss: 3.45294189453125 Eval Loss: 3.944512224197388\n",
            "Running evaluation at iteration 4320...\n",
            "Iteration 4320, Train Loss: 4.356752395629883 Eval Loss: 3.7592692375183105\n",
            "Running evaluation at iteration 4330...\n",
            "Iteration 4330, Train Loss: 4.044499397277832 Eval Loss: 3.947278380393982\n",
            "Running evaluation at iteration 4340...\n",
            "Iteration 4340, Train Loss: 4.307860851287842 Eval Loss: 4.0019948244094845\n",
            "Running evaluation at iteration 4350...\n",
            "Iteration 4350, Train Loss: 3.8852651119232178 Eval Loss: 3.777182936668396\n",
            "Running evaluation at iteration 4360...\n",
            "Iteration 4360, Train Loss: 3.760411024093628 Eval Loss: 4.013522839546203\n",
            "Running evaluation at iteration 4370...\n",
            "Iteration 4370, Train Loss: 4.189733982086182 Eval Loss: 3.9881210803985594\n",
            "Running evaluation at iteration 4380...\n",
            "Iteration 4380, Train Loss: 4.418697357177734 Eval Loss: 3.865244722366333\n",
            "Running evaluation at iteration 4390...\n",
            "Iteration 4390, Train Loss: 3.6713714599609375 Eval Loss: 3.9622865200042723\n",
            "Running evaluation at iteration 4400...\n",
            "Iteration 4400, Train Loss: 4.101773738861084 Eval Loss: 4.057194232940674\n",
            "Running evaluation at iteration 4410...\n",
            "Iteration 4410, Train Loss: 3.8463873863220215 Eval Loss: 3.9218643426895143\n",
            "Running evaluation at iteration 4420...\n",
            "Iteration 4420, Train Loss: 4.009992599487305 Eval Loss: 3.803942394256592\n",
            "Running evaluation at iteration 4430...\n",
            "Iteration 4430, Train Loss: 5.0449700355529785 Eval Loss: 4.037330293655396\n",
            "Running evaluation at iteration 4440...\n",
            "Iteration 4440, Train Loss: 3.7341439723968506 Eval Loss: 3.907293128967285\n",
            "Running evaluation at iteration 4450...\n",
            "Iteration 4450, Train Loss: 3.260946035385132 Eval Loss: 3.91309814453125\n",
            "Running evaluation at iteration 4460...\n",
            "Iteration 4460, Train Loss: 4.559197902679443 Eval Loss: 3.858085370063782\n",
            "Running evaluation at iteration 4470...\n",
            "Iteration 4470, Train Loss: 3.867826223373413 Eval Loss: 3.7425054550170898\n",
            "Running evaluation at iteration 4480...\n",
            "Iteration 4480, Train Loss: 3.9945971965789795 Eval Loss: 4.136798763275147\n",
            "Running evaluation at iteration 4490...\n",
            "Iteration 4490, Train Loss: 3.519181251525879 Eval Loss: 3.7749160528182983\n",
            "Running evaluation at iteration 4500...\n",
            "Iteration 4500, Train Loss: 4.361132621765137 Eval Loss: 3.8093404531478883\n",
            "Running evaluation at iteration 4510...\n",
            "Iteration 4510, Train Loss: 4.396454334259033 Eval Loss: 3.682260608673096\n",
            "Running evaluation at iteration 4520...\n",
            "Iteration 4520, Train Loss: 3.716865301132202 Eval Loss: 3.9049437761306764\n",
            "Running evaluation at iteration 4530...\n",
            "Iteration 4530, Train Loss: 4.3517374992370605 Eval Loss: 3.9368860006332396\n",
            "Running evaluation at iteration 4540...\n",
            "Iteration 4540, Train Loss: 3.97415828704834 Eval Loss: 3.7254587411880493\n",
            "Running evaluation at iteration 4550...\n",
            "Iteration 4550, Train Loss: 4.067322731018066 Eval Loss: 3.871855068206787\n",
            "Running evaluation at iteration 4560...\n",
            "Iteration 4560, Train Loss: 3.844999074935913 Eval Loss: 3.950157141685486\n",
            "Running evaluation at iteration 4570...\n",
            "Iteration 4570, Train Loss: 3.5304765701293945 Eval Loss: 3.967430090904236\n",
            "Running evaluation at iteration 4580...\n",
            "Iteration 4580, Train Loss: 3.870007276535034 Eval Loss: 3.780192232131958\n",
            "Running evaluation at iteration 4590...\n",
            "Iteration 4590, Train Loss: 2.8945610523223877 Eval Loss: 3.699766445159912\n",
            "Running evaluation at iteration 4600...\n",
            "Iteration 4600, Train Loss: 3.8481945991516113 Eval Loss: 3.9283947229385374\n",
            "Running evaluation at iteration 4610...\n",
            "Iteration 4610, Train Loss: 3.7570745944976807 Eval Loss: 3.901123595237732\n",
            "Running evaluation at iteration 4620...\n",
            "Iteration 4620, Train Loss: 3.5825741291046143 Eval Loss: 3.799956202507019\n",
            "Running evaluation at iteration 4630...\n",
            "Iteration 4630, Train Loss: 3.808198928833008 Eval Loss: 3.7759578704833983\n",
            "Running evaluation at iteration 4640...\n",
            "Iteration 4640, Train Loss: 4.251731872558594 Eval Loss: 3.7431829929351808\n",
            "Running evaluation at iteration 4650...\n",
            "Iteration 4650, Train Loss: 3.7937047481536865 Eval Loss: 3.7938550472259522\n",
            "Running evaluation at iteration 4660...\n",
            "Iteration 4660, Train Loss: 4.303825855255127 Eval Loss: 3.9857354164123535\n",
            "Running evaluation at iteration 4670...\n",
            "Iteration 4670, Train Loss: 3.9637997150421143 Eval Loss: 3.8799214363098145\n",
            "Running evaluation at iteration 4680...\n",
            "Iteration 4680, Train Loss: 3.069570302963257 Eval Loss: 3.736362266540527\n",
            "Running evaluation at iteration 4690...\n",
            "Iteration 4690, Train Loss: 3.9467523097991943 Eval Loss: 3.89745147228241\n",
            "Running evaluation at iteration 4700...\n",
            "Iteration 4700, Train Loss: 3.7222354412078857 Eval Loss: 4.00834698677063\n",
            "Running evaluation at iteration 4710...\n",
            "Iteration 4710, Train Loss: 4.202929496765137 Eval Loss: 3.8413111209869384\n",
            "Running evaluation at iteration 4720...\n",
            "Iteration 4720, Train Loss: 3.275041103363037 Eval Loss: 3.8444560527801515\n",
            "Running evaluation at iteration 4730...\n",
            "Iteration 4730, Train Loss: 4.828775882720947 Eval Loss: 3.761608290672302\n",
            "Running evaluation at iteration 4740...\n",
            "Iteration 4740, Train Loss: 3.311692476272583 Eval Loss: 4.011914944648742\n",
            "Running evaluation at iteration 4750...\n",
            "Iteration 4750, Train Loss: 3.955601692199707 Eval Loss: 3.7292827129364015\n",
            "Running evaluation at iteration 4760...\n",
            "Iteration 4760, Train Loss: 4.378974437713623 Eval Loss: 3.666118121147156\n",
            "Running evaluation at iteration 4770...\n",
            "Iteration 4770, Train Loss: 4.519153118133545 Eval Loss: 3.765869307518005\n",
            "Running evaluation at iteration 4780...\n",
            "Iteration 4780, Train Loss: 4.491915702819824 Eval Loss: 3.8082566022872926\n",
            "Running evaluation at iteration 4790...\n",
            "Iteration 4790, Train Loss: 4.223993301391602 Eval Loss: 3.8331485986709595\n",
            "Running evaluation at iteration 4800...\n",
            "Iteration 4800, Train Loss: 3.8802051544189453 Eval Loss: 3.8612937927246094\n",
            "Running evaluation at iteration 4810...\n",
            "Iteration 4810, Train Loss: 3.9961702823638916 Eval Loss: 3.903344440460205\n",
            "Running evaluation at iteration 4820...\n",
            "Iteration 4820, Train Loss: 3.7736644744873047 Eval Loss: 3.884369158744812\n",
            "Running evaluation at iteration 4830...\n",
            "Iteration 4830, Train Loss: 3.669950008392334 Eval Loss: 3.874127173423767\n",
            "Running evaluation at iteration 4840...\n",
            "Iteration 4840, Train Loss: 3.519005060195923 Eval Loss: 3.9299888134002687\n",
            "Running evaluation at iteration 4850...\n",
            "Iteration 4850, Train Loss: 3.343747138977051 Eval Loss: 3.932152819633484\n",
            "Running evaluation at iteration 4860...\n",
            "Iteration 4860, Train Loss: 4.716259002685547 Eval Loss: 3.74128999710083\n",
            "Running evaluation at iteration 4870...\n",
            "Iteration 4870, Train Loss: 3.994661808013916 Eval Loss: 3.728739547729492\n",
            "Running evaluation at iteration 4880...\n",
            "Iteration 4880, Train Loss: 4.172705173492432 Eval Loss: 4.0413289070129395\n",
            "Running evaluation at iteration 4890...\n",
            "Iteration 4890, Train Loss: 3.4430501461029053 Eval Loss: 3.749504899978638\n",
            "Running evaluation at iteration 4900...\n",
            "Iteration 4900, Train Loss: 3.9960904121398926 Eval Loss: 3.828662705421448\n",
            "Running evaluation at iteration 4910...\n",
            "Iteration 4910, Train Loss: 4.19753885269165 Eval Loss: 3.7740943670272826\n",
            "Running evaluation at iteration 4920...\n",
            "Iteration 4920, Train Loss: 3.9999489784240723 Eval Loss: 3.8341090202331545\n",
            "Running evaluation at iteration 4930...\n",
            "Iteration 4930, Train Loss: 3.2506532669067383 Eval Loss: 3.9716493606567385\n",
            "Running evaluation at iteration 4940...\n",
            "Iteration 4940, Train Loss: 3.7600395679473877 Eval Loss: 3.708614706993103\n",
            "Running evaluation at iteration 4950...\n",
            "Iteration 4950, Train Loss: 4.080804824829102 Eval Loss: 4.036648297309876\n",
            "Running evaluation at iteration 4960...\n",
            "Iteration 4960, Train Loss: 3.700256109237671 Eval Loss: 3.741758108139038\n",
            "Running evaluation at iteration 4970...\n",
            "Iteration 4970, Train Loss: 3.7596795558929443 Eval Loss: 3.8606848239898683\n",
            "Running evaluation at iteration 4980...\n",
            "Iteration 4980, Train Loss: 3.8292016983032227 Eval Loss: 3.844367432594299\n",
            "Running evaluation at iteration 4990...\n",
            "Iteration 4990, Train Loss: 3.216444969177246 Eval Loss: 3.843183708190918\n",
            "Running evaluation at iteration 5000...\n",
            "Iteration 5000, Train Loss: 3.513237237930298 Eval Loss: 3.865778613090515\n",
            "Running evaluation at iteration 5010...\n",
            "Iteration 5010, Train Loss: 4.199390888214111 Eval Loss: 3.6209770679473876\n",
            "Running evaluation at iteration 5020...\n",
            "Iteration 5020, Train Loss: 3.7624213695526123 Eval Loss: 4.037904047966004\n",
            "Running evaluation at iteration 5030...\n",
            "Iteration 5030, Train Loss: 3.804051399230957 Eval Loss: 3.96079568862915\n",
            "Running evaluation at iteration 5040...\n",
            "Iteration 5040, Train Loss: 3.7237915992736816 Eval Loss: 3.863410496711731\n",
            "Running evaluation at iteration 5050...\n",
            "Iteration 5050, Train Loss: 5.221208572387695 Eval Loss: 3.8367038011550902\n",
            "Running evaluation at iteration 5060...\n",
            "Iteration 5060, Train Loss: 3.595538377761841 Eval Loss: 3.737702488899231\n",
            "Running evaluation at iteration 5070...\n",
            "Iteration 5070, Train Loss: 3.7443764209747314 Eval Loss: 3.710436201095581\n",
            "Running evaluation at iteration 5080...\n",
            "Iteration 5080, Train Loss: 3.4691388607025146 Eval Loss: 3.797370934486389\n",
            "Running evaluation at iteration 5090...\n",
            "Iteration 5090, Train Loss: 3.3208508491516113 Eval Loss: 4.076425528526306\n",
            "Running evaluation at iteration 5100...\n",
            "Iteration 5100, Train Loss: 4.563025951385498 Eval Loss: 3.8384702444076537\n",
            "Running evaluation at iteration 5110...\n",
            "Iteration 5110, Train Loss: 3.4020559787750244 Eval Loss: 3.8242932319641114\n",
            "Running evaluation at iteration 5120...\n",
            "Iteration 5120, Train Loss: 3.5451037883758545 Eval Loss: 3.630757975578308\n",
            "Running evaluation at iteration 5130...\n",
            "Iteration 5130, Train Loss: 3.9708056449890137 Eval Loss: 3.7903314590454102\n",
            "Running evaluation at iteration 5140...\n",
            "Iteration 5140, Train Loss: 4.081915378570557 Eval Loss: 3.9926301002502442\n",
            "Running evaluation at iteration 5150...\n",
            "Iteration 5150, Train Loss: 3.5767345428466797 Eval Loss: 3.7460700273513794\n",
            "Running evaluation at iteration 5160...\n",
            "Iteration 5160, Train Loss: 3.9165725708007812 Eval Loss: 3.7144615411758424\n",
            "Running evaluation at iteration 5170...\n",
            "Iteration 5170, Train Loss: 3.5277979373931885 Eval Loss: 3.9010182857513427\n",
            "Running evaluation at iteration 5180...\n",
            "Iteration 5180, Train Loss: 3.2598354816436768 Eval Loss: 3.948230743408203\n",
            "Running evaluation at iteration 5190...\n",
            "Iteration 5190, Train Loss: 4.06453275680542 Eval Loss: 3.7155378580093386\n",
            "Running evaluation at iteration 5200...\n",
            "Iteration 5200, Train Loss: 3.7668709754943848 Eval Loss: 3.994475507736206\n",
            "Running evaluation at iteration 5210...\n",
            "Iteration 5210, Train Loss: 3.847381353378296 Eval Loss: 3.8023261070251464\n",
            "Running evaluation at iteration 5220...\n",
            "Iteration 5220, Train Loss: 3.6853723526000977 Eval Loss: 3.82430579662323\n",
            "Running evaluation at iteration 5230...\n",
            "Iteration 5230, Train Loss: 3.6571109294891357 Eval Loss: 3.672495222091675\n",
            "Running evaluation at iteration 5240...\n",
            "Iteration 5240, Train Loss: 3.825929641723633 Eval Loss: 3.8145619869232177\n",
            "Running evaluation at iteration 5250...\n",
            "Iteration 5250, Train Loss: 4.0900444984436035 Eval Loss: 3.7963944911956786\n",
            "Running evaluation at iteration 5260...\n",
            "Iteration 5260, Train Loss: 3.7238171100616455 Eval Loss: 3.9150325298309325\n",
            "Running evaluation at iteration 5270...\n",
            "Iteration 5270, Train Loss: 4.055485248565674 Eval Loss: 3.9858322381973266\n",
            "Running evaluation at iteration 5280...\n",
            "Iteration 5280, Train Loss: 3.9733121395111084 Eval Loss: 3.7660828828811646\n",
            "Running evaluation at iteration 5290...\n",
            "Iteration 5290, Train Loss: 3.9632327556610107 Eval Loss: 3.82263298034668\n",
            "Running evaluation at iteration 5300...\n",
            "Iteration 5300, Train Loss: 3.760394811630249 Eval Loss: 3.6309706211090087\n",
            "Running evaluation at iteration 5310...\n",
            "Iteration 5310, Train Loss: 3.300065517425537 Eval Loss: 3.9412307500839234\n",
            "Running evaluation at iteration 5320...\n",
            "Iteration 5320, Train Loss: 4.528984546661377 Eval Loss: 3.6559083223342896\n",
            "Running evaluation at iteration 5330...\n",
            "Iteration 5330, Train Loss: 3.302537202835083 Eval Loss: 3.5996110439300537\n",
            "Running evaluation at iteration 5340...\n",
            "Iteration 5340, Train Loss: 3.945364475250244 Eval Loss: 3.6603020668029784\n",
            "Running evaluation at iteration 5350...\n",
            "Iteration 5350, Train Loss: 4.260743618011475 Eval Loss: 3.740217113494873\n",
            "Running evaluation at iteration 5360...\n",
            "Iteration 5360, Train Loss: 3.1655516624450684 Eval Loss: 3.8307570219039917\n",
            "Running evaluation at iteration 5370...\n",
            "Iteration 5370, Train Loss: 4.123007297515869 Eval Loss: 3.7830089807510374\n",
            "Running evaluation at iteration 5380...\n",
            "Iteration 5380, Train Loss: 3.9738340377807617 Eval Loss: 3.73765869140625\n",
            "Running evaluation at iteration 5390...\n",
            "Iteration 5390, Train Loss: 4.105648517608643 Eval Loss: 3.712043023109436\n",
            "Running evaluation at iteration 5400...\n",
            "Iteration 5400, Train Loss: 3.4811882972717285 Eval Loss: 3.875329780578613\n",
            "Running evaluation at iteration 5410...\n",
            "Iteration 5410, Train Loss: 4.115394115447998 Eval Loss: 3.997993993759155\n",
            "Running evaluation at iteration 5420...\n",
            "Iteration 5420, Train Loss: 4.216224193572998 Eval Loss: 3.953591537475586\n",
            "Running evaluation at iteration 5430...\n",
            "Iteration 5430, Train Loss: 3.8906264305114746 Eval Loss: 3.922110080718994\n",
            "Running evaluation at iteration 5440...\n",
            "Iteration 5440, Train Loss: 3.6568896770477295 Eval Loss: 3.697754406929016\n",
            "Running evaluation at iteration 5450...\n",
            "Iteration 5450, Train Loss: 4.148881912231445 Eval Loss: 3.7590092420578003\n",
            "Running evaluation at iteration 5460...\n",
            "Iteration 5460, Train Loss: 3.8696041107177734 Eval Loss: 3.6758246660232543\n",
            "Running evaluation at iteration 5470...\n",
            "Iteration 5470, Train Loss: 3.940197467803955 Eval Loss: 3.8319849014282226\n",
            "Running evaluation at iteration 5480...\n",
            "Iteration 5480, Train Loss: 4.048856735229492 Eval Loss: 3.8585025310516357\n",
            "Running evaluation at iteration 5490...\n",
            "Iteration 5490, Train Loss: 3.8411595821380615 Eval Loss: 3.81960768699646\n",
            "Running evaluation at iteration 5500...\n",
            "Iteration 5500, Train Loss: 3.872300386428833 Eval Loss: 3.6540348529815674\n",
            "Running evaluation at iteration 5510...\n",
            "Iteration 5510, Train Loss: 3.493988275527954 Eval Loss: 3.4950849771499635\n",
            "Running evaluation at iteration 5520...\n",
            "Iteration 5520, Train Loss: 3.2775444984436035 Eval Loss: 3.7358309268951415\n",
            "Running evaluation at iteration 5530...\n",
            "Iteration 5530, Train Loss: 4.193415641784668 Eval Loss: 3.579607129096985\n",
            "Running evaluation at iteration 5540...\n",
            "Iteration 5540, Train Loss: 3.5911765098571777 Eval Loss: 3.748409461975098\n",
            "Running evaluation at iteration 5550...\n",
            "Iteration 5550, Train Loss: 3.7443087100982666 Eval Loss: 3.7831543684005737\n",
            "Running evaluation at iteration 5560...\n",
            "Iteration 5560, Train Loss: 2.9778897762298584 Eval Loss: 3.621785354614258\n",
            "Running evaluation at iteration 5570...\n",
            "Iteration 5570, Train Loss: 3.729421615600586 Eval Loss: 3.6716633081436156\n",
            "Running evaluation at iteration 5580...\n",
            "Iteration 5580, Train Loss: 4.6425275802612305 Eval Loss: 3.5119271516799926\n",
            "Running evaluation at iteration 5590...\n",
            "Iteration 5590, Train Loss: 4.011920928955078 Eval Loss: 3.5430503845214845\n",
            "Running evaluation at iteration 5600...\n",
            "Iteration 5600, Train Loss: 3.787416458129883 Eval Loss: 3.6107067584991457\n",
            "Running evaluation at iteration 5610...\n",
            "Iteration 5610, Train Loss: 3.6223297119140625 Eval Loss: 3.943857789039612\n",
            "Running evaluation at iteration 5620...\n",
            "Iteration 5620, Train Loss: 3.3309009075164795 Eval Loss: 3.900548219680786\n",
            "Running evaluation at iteration 5630...\n",
            "Iteration 5630, Train Loss: 4.042343616485596 Eval Loss: 3.69200484752655\n",
            "Running evaluation at iteration 5640...\n",
            "Iteration 5640, Train Loss: 3.7368345260620117 Eval Loss: 3.8591445446014405\n",
            "Running evaluation at iteration 5650...\n",
            "Iteration 5650, Train Loss: 3.4756503105163574 Eval Loss: 3.6838990211486817\n",
            "Running evaluation at iteration 5660...\n",
            "Iteration 5660, Train Loss: 3.7957358360290527 Eval Loss: 3.8370104312896727\n",
            "Running evaluation at iteration 5670...\n",
            "Iteration 5670, Train Loss: 4.091307640075684 Eval Loss: 3.5407612562179565\n",
            "Running evaluation at iteration 5680...\n",
            "Iteration 5680, Train Loss: 3.7643542289733887 Eval Loss: 3.8305079698562623\n",
            "Running evaluation at iteration 5690...\n",
            "Iteration 5690, Train Loss: 4.060204982757568 Eval Loss: 3.612543249130249\n",
            "Running evaluation at iteration 5700...\n",
            "Iteration 5700, Train Loss: 3.333735704421997 Eval Loss: 3.6425631761550905\n",
            "Running evaluation at iteration 5710...\n",
            "Iteration 5710, Train Loss: 3.7097744941711426 Eval Loss: 3.8539071083068848\n",
            "Running evaluation at iteration 5720...\n",
            "Iteration 5720, Train Loss: 3.8459644317626953 Eval Loss: 3.6075145483016966\n",
            "Running evaluation at iteration 5730...\n",
            "Iteration 5730, Train Loss: 3.374396324157715 Eval Loss: 3.7347008228302\n",
            "Running evaluation at iteration 5740...\n",
            "Iteration 5740, Train Loss: 3.534787178039551 Eval Loss: 3.5471437692642214\n",
            "Running evaluation at iteration 5750...\n",
            "Iteration 5750, Train Loss: 3.684907913208008 Eval Loss: 3.844329023361206\n",
            "Running evaluation at iteration 5760...\n",
            "Iteration 5760, Train Loss: 3.5801444053649902 Eval Loss: 3.8051142930984496\n",
            "Running evaluation at iteration 5770...\n",
            "Iteration 5770, Train Loss: 3.3814640045166016 Eval Loss: 3.9029909133911134\n",
            "Running evaluation at iteration 5780...\n",
            "Iteration 5780, Train Loss: 3.3732857704162598 Eval Loss: 3.845008707046509\n",
            "Running evaluation at iteration 5790...\n",
            "Iteration 5790, Train Loss: 3.7196013927459717 Eval Loss: 3.5645983934402468\n",
            "Running evaluation at iteration 5800...\n",
            "Iteration 5800, Train Loss: 3.483365058898926 Eval Loss: 3.797187566757202\n",
            "Running evaluation at iteration 5810...\n",
            "Iteration 5810, Train Loss: 3.866445541381836 Eval Loss: 3.819710040092468\n",
            "Running evaluation at iteration 5820...\n",
            "Iteration 5820, Train Loss: 3.810319185256958 Eval Loss: 3.748968482017517\n",
            "Running evaluation at iteration 5830...\n",
            "Iteration 5830, Train Loss: 4.232719421386719 Eval Loss: 3.835740661621094\n",
            "Running evaluation at iteration 5840...\n",
            "Iteration 5840, Train Loss: 3.693350315093994 Eval Loss: 3.7141139984130858\n",
            "Running evaluation at iteration 5850...\n",
            "Iteration 5850, Train Loss: 3.983701705932617 Eval Loss: 3.701103448867798\n",
            "Running evaluation at iteration 5860...\n",
            "Iteration 5860, Train Loss: 4.093725204467773 Eval Loss: 3.6993189811706544\n",
            "Running evaluation at iteration 5870...\n",
            "Iteration 5870, Train Loss: 3.5434861183166504 Eval Loss: 3.6330023050308227\n",
            "Running evaluation at iteration 5880...\n",
            "Iteration 5880, Train Loss: 3.7720181941986084 Eval Loss: 3.729053497314453\n",
            "Running evaluation at iteration 5890...\n",
            "Iteration 5890, Train Loss: 4.327325820922852 Eval Loss: 3.641530680656433\n",
            "Running evaluation at iteration 5900...\n",
            "Iteration 5900, Train Loss: 3.955918073654175 Eval Loss: 3.82911422252655\n",
            "Running evaluation at iteration 5910...\n",
            "Iteration 5910, Train Loss: 3.9817845821380615 Eval Loss: 3.765025782585144\n",
            "Running evaluation at iteration 5920...\n",
            "Iteration 5920, Train Loss: 3.984225273132324 Eval Loss: 3.620752477645874\n",
            "Running evaluation at iteration 5930...\n",
            "Iteration 5930, Train Loss: 4.23701286315918 Eval Loss: 3.7116999864578246\n",
            "Running evaluation at iteration 5940...\n",
            "Iteration 5940, Train Loss: 3.8489553928375244 Eval Loss: 3.646216702461243\n",
            "Running evaluation at iteration 5950...\n",
            "Iteration 5950, Train Loss: 3.8891477584838867 Eval Loss: 3.8523970365524294\n",
            "Running evaluation at iteration 5960...\n",
            "Iteration 5960, Train Loss: 4.41884708404541 Eval Loss: 3.8486852169036867\n",
            "Running evaluation at iteration 5970...\n",
            "Iteration 5970, Train Loss: 4.064311981201172 Eval Loss: 3.749546933174133\n",
            "Running evaluation at iteration 5980...\n",
            "Iteration 5980, Train Loss: 3.636898994445801 Eval Loss: 3.870033311843872\n",
            "Running evaluation at iteration 5990...\n",
            "Iteration 5990, Train Loss: 3.709864854812622 Eval Loss: 3.5574219703674315\n",
            "Running evaluation at iteration 6000...\n",
            "Iteration 6000, Train Loss: 3.9752440452575684 Eval Loss: 3.8437087059021\n",
            "Running evaluation at iteration 6010...\n",
            "Iteration 6010, Train Loss: 3.2482335567474365 Eval Loss: 3.8175421953201294\n",
            "Running evaluation at iteration 6020...\n",
            "Iteration 6020, Train Loss: 3.5265085697174072 Eval Loss: 3.799745559692383\n",
            "Running evaluation at iteration 6030...\n",
            "Iteration 6030, Train Loss: 3.620631694793701 Eval Loss: 3.6773844718933106\n",
            "Running evaluation at iteration 6040...\n",
            "Iteration 6040, Train Loss: 3.6914548873901367 Eval Loss: 3.563380241394043\n",
            "Running evaluation at iteration 6050...\n",
            "Iteration 6050, Train Loss: 3.7639944553375244 Eval Loss: 3.8650107383728027\n",
            "Running evaluation at iteration 6060...\n",
            "Iteration 6060, Train Loss: 4.340537071228027 Eval Loss: 3.7310033321380613\n",
            "Running evaluation at iteration 6070...\n",
            "Iteration 6070, Train Loss: 3.972499132156372 Eval Loss: 3.708608388900757\n",
            "Running evaluation at iteration 6080...\n",
            "Iteration 6080, Train Loss: 2.916539192199707 Eval Loss: 3.588911604881287\n",
            "Running evaluation at iteration 6090...\n",
            "Iteration 6090, Train Loss: 3.983952522277832 Eval Loss: 3.697915029525757\n",
            "Running evaluation at iteration 6100...\n",
            "Iteration 6100, Train Loss: 3.5569610595703125 Eval Loss: 3.4109475374221803\n",
            "Running evaluation at iteration 6110...\n",
            "Iteration 6110, Train Loss: 3.5856974124908447 Eval Loss: 3.736760210990906\n",
            "Running evaluation at iteration 6120...\n",
            "Iteration 6120, Train Loss: 3.900773048400879 Eval Loss: 3.7083906888961793\n",
            "Running evaluation at iteration 6130...\n",
            "Iteration 6130, Train Loss: 4.413791179656982 Eval Loss: 3.622557282447815\n",
            "Running evaluation at iteration 6140...\n",
            "Iteration 6140, Train Loss: 4.061438083648682 Eval Loss: 3.6741764307022096\n",
            "Running evaluation at iteration 6150...\n",
            "Iteration 6150, Train Loss: 3.9243927001953125 Eval Loss: 3.6159923315048217\n",
            "Running evaluation at iteration 6160...\n",
            "Iteration 6160, Train Loss: 3.7851920127868652 Eval Loss: 3.814499020576477\n",
            "Running evaluation at iteration 6170...\n",
            "Iteration 6170, Train Loss: 3.3651154041290283 Eval Loss: 3.790066194534302\n",
            "Running evaluation at iteration 6180...\n",
            "Iteration 6180, Train Loss: 3.226792097091675 Eval Loss: 3.830217170715332\n",
            "Running evaluation at iteration 6190...\n",
            "Iteration 6190, Train Loss: 3.5341484546661377 Eval Loss: 3.796328973770142\n",
            "Running evaluation at iteration 6200...\n",
            "Iteration 6200, Train Loss: 3.3319790363311768 Eval Loss: 3.910465884208679\n",
            "Running evaluation at iteration 6210...\n",
            "Iteration 6210, Train Loss: 3.7637603282928467 Eval Loss: 3.793342709541321\n",
            "Running evaluation at iteration 6220...\n",
            "Iteration 6220, Train Loss: 3.6522960662841797 Eval Loss: 3.669117045402527\n",
            "Running evaluation at iteration 6230...\n",
            "Iteration 6230, Train Loss: 3.726855754852295 Eval Loss: 3.610787010192871\n",
            "Running evaluation at iteration 6240...\n",
            "Iteration 6240, Train Loss: 3.3268935680389404 Eval Loss: 3.7125433683395386\n",
            "Running evaluation at iteration 6250...\n",
            "Iteration 6250, Train Loss: 3.681464910507202 Eval Loss: 3.7158092975616457\n",
            "Running evaluation at iteration 6260...\n",
            "Iteration 6260, Train Loss: 3.6043262481689453 Eval Loss: 3.5873947620391844\n",
            "Running evaluation at iteration 6270...\n",
            "Iteration 6270, Train Loss: 3.8595528602600098 Eval Loss: 3.713028907775879\n",
            "Running evaluation at iteration 6280...\n",
            "Iteration 6280, Train Loss: 3.3489818572998047 Eval Loss: 3.642050313949585\n",
            "Running evaluation at iteration 6290...\n",
            "Iteration 6290, Train Loss: 3.6741628646850586 Eval Loss: 3.5181819438934325\n",
            "Running evaluation at iteration 6300...\n",
            "Iteration 6300, Train Loss: 4.028866767883301 Eval Loss: 3.621825695037842\n",
            "Running evaluation at iteration 6310...\n",
            "Iteration 6310, Train Loss: 4.078242778778076 Eval Loss: 3.8739552736282348\n",
            "Running evaluation at iteration 6320...\n",
            "Iteration 6320, Train Loss: 3.3992209434509277 Eval Loss: 3.6368688583374023\n",
            "Running evaluation at iteration 6330...\n",
            "Iteration 6330, Train Loss: 3.8025319576263428 Eval Loss: 3.8925261735916137\n",
            "Running evaluation at iteration 6340...\n",
            "Iteration 6340, Train Loss: 4.4561638832092285 Eval Loss: 3.8167062997817993\n",
            "Running evaluation at iteration 6350...\n",
            "Iteration 6350, Train Loss: 3.4433960914611816 Eval Loss: 3.6270437240600586\n",
            "Running evaluation at iteration 6360...\n",
            "Iteration 6360, Train Loss: 3.821622610092163 Eval Loss: 3.493286609649658\n",
            "Running evaluation at iteration 6370...\n",
            "Iteration 6370, Train Loss: 3.793630599975586 Eval Loss: 3.956514000892639\n",
            "Running evaluation at iteration 6380...\n",
            "Iteration 6380, Train Loss: 4.109426021575928 Eval Loss: 3.835503935813904\n",
            "Running evaluation at iteration 6390...\n",
            "Iteration 6390, Train Loss: 3.9206130504608154 Eval Loss: 3.664002561569214\n",
            "Running evaluation at iteration 6400...\n",
            "Iteration 6400, Train Loss: 4.005948066711426 Eval Loss: 3.669019031524658\n",
            "Running evaluation at iteration 6410...\n",
            "Iteration 6410, Train Loss: 3.6176459789276123 Eval Loss: 3.692059111595154\n",
            "Running evaluation at iteration 6420...\n",
            "Iteration 6420, Train Loss: 3.572596788406372 Eval Loss: 3.7511205434799195\n",
            "Running evaluation at iteration 6430...\n",
            "Iteration 6430, Train Loss: 3.9416069984436035 Eval Loss: 3.7995009660720824\n",
            "Running evaluation at iteration 6440...\n",
            "Iteration 6440, Train Loss: 3.22025203704834 Eval Loss: 3.7176810026168825\n",
            "Running evaluation at iteration 6450...\n",
            "Iteration 6450, Train Loss: 4.155073165893555 Eval Loss: 3.6979909896850587\n",
            "Running evaluation at iteration 6460...\n",
            "Iteration 6460, Train Loss: 4.005050182342529 Eval Loss: 3.785894012451172\n",
            "Running evaluation at iteration 6470...\n",
            "Iteration 6470, Train Loss: 4.3729472160339355 Eval Loss: 3.4218622922897337\n",
            "Running evaluation at iteration 6480...\n",
            "Iteration 6480, Train Loss: 4.751262664794922 Eval Loss: 3.7318029403686523\n",
            "Running evaluation at iteration 6490...\n",
            "Iteration 6490, Train Loss: 4.358811855316162 Eval Loss: 3.6715465068817137\n",
            "Running evaluation at iteration 6500...\n",
            "Iteration 6500, Train Loss: 3.6420319080352783 Eval Loss: 3.658082056045532\n",
            "Running evaluation at iteration 6510...\n",
            "Iteration 6510, Train Loss: 3.2173819541931152 Eval Loss: 3.634244680404663\n",
            "Running evaluation at iteration 6520...\n",
            "Iteration 6520, Train Loss: 3.3394248485565186 Eval Loss: 3.6492381572723387\n",
            "Running evaluation at iteration 6530...\n",
            "Iteration 6530, Train Loss: 3.3223626613616943 Eval Loss: 3.8355340242385862\n",
            "Running evaluation at iteration 6540...\n",
            "Iteration 6540, Train Loss: 3.891066312789917 Eval Loss: 3.7436168909072878\n",
            "Running evaluation at iteration 6550...\n",
            "Iteration 6550, Train Loss: 4.00469446182251 Eval Loss: 4.076603031158447\n",
            "Running evaluation at iteration 6560...\n",
            "Iteration 6560, Train Loss: 3.5397229194641113 Eval Loss: 3.7001304626464844\n",
            "Running evaluation at iteration 6570...\n",
            "Iteration 6570, Train Loss: 3.314915657043457 Eval Loss: 3.9149081468582154\n",
            "Running evaluation at iteration 6580...\n",
            "Iteration 6580, Train Loss: 3.9543955326080322 Eval Loss: 3.6955066919326782\n",
            "Running evaluation at iteration 6590...\n",
            "Iteration 6590, Train Loss: 3.777470588684082 Eval Loss: 3.7069415807724\n",
            "Running evaluation at iteration 6600...\n",
            "Iteration 6600, Train Loss: 3.6982104778289795 Eval Loss: 3.7938750743865968\n",
            "Running evaluation at iteration 6610...\n",
            "Iteration 6610, Train Loss: 4.251070976257324 Eval Loss: 3.800925922393799\n",
            "Running evaluation at iteration 6620...\n",
            "Iteration 6620, Train Loss: 3.766528367996216 Eval Loss: 3.6700695037841795\n",
            "Running evaluation at iteration 6630...\n",
            "Iteration 6630, Train Loss: 3.589015483856201 Eval Loss: 3.7315096139907835\n",
            "Running evaluation at iteration 6640...\n",
            "Iteration 6640, Train Loss: 3.547708511352539 Eval Loss: 3.589192295074463\n",
            "Running evaluation at iteration 6650...\n",
            "Iteration 6650, Train Loss: 3.7681570053100586 Eval Loss: 3.6788588047027586\n",
            "Running evaluation at iteration 6660...\n",
            "Iteration 6660, Train Loss: 3.775779962539673 Eval Loss: 3.9623623371124266\n",
            "Running evaluation at iteration 6670...\n",
            "Iteration 6670, Train Loss: 3.3698108196258545 Eval Loss: 3.592160964012146\n",
            "Running evaluation at iteration 6680...\n",
            "Iteration 6680, Train Loss: 3.278543710708618 Eval Loss: 3.462652635574341\n",
            "Running evaluation at iteration 6690...\n",
            "Iteration 6690, Train Loss: 3.674607753753662 Eval Loss: 3.711282229423523\n",
            "Running evaluation at iteration 6700...\n",
            "Iteration 6700, Train Loss: 3.902585983276367 Eval Loss: 3.57222101688385\n",
            "Running evaluation at iteration 6710...\n",
            "Iteration 6710, Train Loss: 3.4394402503967285 Eval Loss: 3.783842158317566\n",
            "Running evaluation at iteration 6720...\n",
            "Iteration 6720, Train Loss: 3.8985936641693115 Eval Loss: 3.4076178312301635\n",
            "Running evaluation at iteration 6730...\n",
            "Iteration 6730, Train Loss: 3.583836317062378 Eval Loss: 3.454439473152161\n",
            "Running evaluation at iteration 6740...\n",
            "Iteration 6740, Train Loss: 4.876987934112549 Eval Loss: 3.7551610469818115\n",
            "Running evaluation at iteration 6750...\n",
            "Iteration 6750, Train Loss: 3.4654722213745117 Eval Loss: 3.6540977716445924\n",
            "Running evaluation at iteration 6760...\n",
            "Iteration 6760, Train Loss: 4.108813285827637 Eval Loss: 3.490354061126709\n",
            "Running evaluation at iteration 6770...\n",
            "Iteration 6770, Train Loss: 4.336986064910889 Eval Loss: 3.7610563039779663\n",
            "Running evaluation at iteration 6780...\n",
            "Iteration 6780, Train Loss: 4.081658840179443 Eval Loss: 3.5607674360275268\n",
            "Running evaluation at iteration 6790...\n",
            "Iteration 6790, Train Loss: 3.428415298461914 Eval Loss: 3.6989758014678955\n",
            "Running evaluation at iteration 6800...\n",
            "Iteration 6800, Train Loss: 3.0829858779907227 Eval Loss: 3.721538019180298\n",
            "Running evaluation at iteration 6810...\n",
            "Iteration 6810, Train Loss: 3.5494885444641113 Eval Loss: 3.590145778656006\n",
            "Running evaluation at iteration 6820...\n",
            "Iteration 6820, Train Loss: 3.458392381668091 Eval Loss: 3.7836368083953857\n",
            "Running evaluation at iteration 6830...\n",
            "Iteration 6830, Train Loss: 4.538780689239502 Eval Loss: 3.6371896743774412\n",
            "Running evaluation at iteration 6840...\n",
            "Iteration 6840, Train Loss: 3.9691033363342285 Eval Loss: 3.569407820701599\n",
            "Running evaluation at iteration 6850...\n",
            "Iteration 6850, Train Loss: 3.8010003566741943 Eval Loss: 3.9031941890716553\n",
            "Running evaluation at iteration 6860...\n",
            "Iteration 6860, Train Loss: 4.326757907867432 Eval Loss: 3.6201996088027952\n",
            "Running evaluation at iteration 6870...\n",
            "Iteration 6870, Train Loss: 3.605153799057007 Eval Loss: 3.3704872846603395\n",
            "Running evaluation at iteration 6880...\n",
            "Iteration 6880, Train Loss: 3.682981491088867 Eval Loss: 3.8810927152633665\n",
            "Running evaluation at iteration 6890...\n",
            "Iteration 6890, Train Loss: 3.478609561920166 Eval Loss: 3.7922295570373534\n",
            "Running evaluation at iteration 6900...\n",
            "Iteration 6900, Train Loss: 4.013790130615234 Eval Loss: 3.8634459733963014\n",
            "Running evaluation at iteration 6910...\n",
            "Iteration 6910, Train Loss: 3.535588026046753 Eval Loss: 3.71215500831604\n",
            "Running evaluation at iteration 6920...\n",
            "Iteration 6920, Train Loss: 3.4891715049743652 Eval Loss: 3.578123593330383\n",
            "Running evaluation at iteration 6930...\n",
            "Iteration 6930, Train Loss: 3.2519073486328125 Eval Loss: 3.7455375671386717\n",
            "Running evaluation at iteration 6940...\n",
            "Iteration 6940, Train Loss: 4.215303421020508 Eval Loss: 3.634191393852234\n",
            "Running evaluation at iteration 6950...\n",
            "Iteration 6950, Train Loss: 3.4396557807922363 Eval Loss: 3.670389986038208\n",
            "Running evaluation at iteration 6960...\n",
            "Iteration 6960, Train Loss: 3.6057164669036865 Eval Loss: 3.6205809354782104\n",
            "Running evaluation at iteration 6970...\n",
            "Iteration 6970, Train Loss: 3.26076078414917 Eval Loss: 3.7782675981521607\n",
            "Running evaluation at iteration 6980...\n",
            "Iteration 6980, Train Loss: 3.86224365234375 Eval Loss: 3.8281259536743164\n",
            "Running evaluation at iteration 6990...\n",
            "Iteration 6990, Train Loss: 4.074540138244629 Eval Loss: 3.7140616893768312\n",
            "Running evaluation at iteration 7000...\n",
            "Iteration 7000, Train Loss: 3.737645149230957 Eval Loss: 3.8887348651885985\n",
            "Running evaluation at iteration 7010...\n",
            "Iteration 7010, Train Loss: 3.2574615478515625 Eval Loss: 3.6393620014190673\n",
            "Running evaluation at iteration 7020...\n",
            "Iteration 7020, Train Loss: 3.600459575653076 Eval Loss: 3.5102946519851685\n",
            "Running evaluation at iteration 7030...\n",
            "Iteration 7030, Train Loss: 3.0882351398468018 Eval Loss: 3.6117889642715455\n",
            "Running evaluation at iteration 7040...\n",
            "Iteration 7040, Train Loss: 3.456218957901001 Eval Loss: 3.790536069869995\n",
            "Running evaluation at iteration 7050...\n",
            "Iteration 7050, Train Loss: 3.992560625076294 Eval Loss: 3.563773488998413\n",
            "Running evaluation at iteration 7060...\n",
            "Iteration 7060, Train Loss: 3.6201114654541016 Eval Loss: 3.616793966293335\n",
            "Running evaluation at iteration 7070...\n",
            "Iteration 7070, Train Loss: 3.4049363136291504 Eval Loss: 3.6535427570343018\n",
            "Running evaluation at iteration 7080...\n",
            "Iteration 7080, Train Loss: 3.9056589603424072 Eval Loss: 3.6548124313354493\n",
            "Running evaluation at iteration 7090...\n",
            "Iteration 7090, Train Loss: 3.6641738414764404 Eval Loss: 3.7294339418411253\n",
            "Running evaluation at iteration 7100...\n",
            "Iteration 7100, Train Loss: 3.803090810775757 Eval Loss: 3.776371216773987\n",
            "Running evaluation at iteration 7110...\n",
            "Iteration 7110, Train Loss: 3.5289576053619385 Eval Loss: 3.712470817565918\n",
            "Running evaluation at iteration 7120...\n",
            "Iteration 7120, Train Loss: 4.566922187805176 Eval Loss: 3.7605998039245607\n",
            "Running evaluation at iteration 7130...\n",
            "Iteration 7130, Train Loss: 3.581319570541382 Eval Loss: 3.8321053266525267\n",
            "Running evaluation at iteration 7140...\n",
            "Iteration 7140, Train Loss: 3.9711718559265137 Eval Loss: 3.6139774560928344\n",
            "Running evaluation at iteration 7150...\n",
            "Iteration 7150, Train Loss: 3.836040735244751 Eval Loss: 3.797580027580261\n",
            "Running evaluation at iteration 7160...\n",
            "Iteration 7160, Train Loss: 3.87431001663208 Eval Loss: 3.782239246368408\n",
            "Running evaluation at iteration 7170...\n",
            "Iteration 7170, Train Loss: 4.120110988616943 Eval Loss: 3.8126713514328\n",
            "Running evaluation at iteration 7180...\n",
            "Iteration 7180, Train Loss: 3.7876136302948 Eval Loss: 3.5938887357711793\n",
            "Running evaluation at iteration 7190...\n",
            "Iteration 7190, Train Loss: 4.08060359954834 Eval Loss: 3.814077115058899\n",
            "Running evaluation at iteration 7200...\n",
            "Iteration 7200, Train Loss: 3.9830260276794434 Eval Loss: 3.609382081031799\n",
            "Running evaluation at iteration 7210...\n",
            "Iteration 7210, Train Loss: 4.08338737487793 Eval Loss: 3.578861117362976\n",
            "Running evaluation at iteration 7220...\n",
            "Iteration 7220, Train Loss: 3.555802345275879 Eval Loss: 3.6722055196762087\n",
            "Running evaluation at iteration 7230...\n",
            "Iteration 7230, Train Loss: 4.5858683586120605 Eval Loss: 3.644207549095154\n",
            "Running evaluation at iteration 7240...\n",
            "Iteration 7240, Train Loss: 3.620919704437256 Eval Loss: 3.566851997375488\n",
            "Running evaluation at iteration 7250...\n",
            "Iteration 7250, Train Loss: 4.117519378662109 Eval Loss: 3.6925217628479006\n",
            "Running evaluation at iteration 7260...\n",
            "Iteration 7260, Train Loss: 3.2940564155578613 Eval Loss: 3.3472833395004273\n",
            "Running evaluation at iteration 7270...\n",
            "Iteration 7270, Train Loss: 3.761711835861206 Eval Loss: 3.605348896980286\n",
            "Running evaluation at iteration 7280...\n",
            "Iteration 7280, Train Loss: 4.125854969024658 Eval Loss: 3.8462018251419066\n",
            "Running evaluation at iteration 7290...\n",
            "Iteration 7290, Train Loss: 3.2297708988189697 Eval Loss: 3.6211671113967894\n",
            "Running evaluation at iteration 7300...\n",
            "Iteration 7300, Train Loss: 3.9425694942474365 Eval Loss: 3.684340500831604\n",
            "Running evaluation at iteration 7310...\n",
            "Iteration 7310, Train Loss: 4.438324451446533 Eval Loss: 3.768033003807068\n",
            "Running evaluation at iteration 7320...\n",
            "Iteration 7320, Train Loss: 3.7521166801452637 Eval Loss: 3.542321538925171\n",
            "Running evaluation at iteration 7330...\n",
            "Iteration 7330, Train Loss: 3.4909021854400635 Eval Loss: 3.4923859357833864\n",
            "Running evaluation at iteration 7340...\n",
            "Iteration 7340, Train Loss: 3.5783379077911377 Eval Loss: 3.4783905506134034\n",
            "Running evaluation at iteration 7350...\n",
            "Iteration 7350, Train Loss: 3.558032751083374 Eval Loss: 3.527604603767395\n",
            "Running evaluation at iteration 7360...\n",
            "Iteration 7360, Train Loss: 4.235511302947998 Eval Loss: 3.6561970710754395\n",
            "Running evaluation at iteration 7370...\n",
            "Iteration 7370, Train Loss: 3.650527238845825 Eval Loss: 3.507512331008911\n",
            "Running evaluation at iteration 7380...\n",
            "Iteration 7380, Train Loss: 3.9103705883026123 Eval Loss: 3.706536555290222\n",
            "Running evaluation at iteration 7390...\n",
            "Iteration 7390, Train Loss: 3.594256639480591 Eval Loss: 3.648761749267578\n",
            "Running evaluation at iteration 7400...\n",
            "Iteration 7400, Train Loss: 3.0453646183013916 Eval Loss: 3.5098931074142454\n",
            "Running evaluation at iteration 7410...\n",
            "Iteration 7410, Train Loss: 3.498103141784668 Eval Loss: 3.46918203830719\n",
            "Running evaluation at iteration 7420...\n",
            "Iteration 7420, Train Loss: 3.461757183074951 Eval Loss: 3.4387789011001586\n",
            "Running evaluation at iteration 7430...\n",
            "Iteration 7430, Train Loss: 4.141589641571045 Eval Loss: 3.463520359992981\n",
            "Running evaluation at iteration 7440...\n",
            "Iteration 7440, Train Loss: 3.2254040241241455 Eval Loss: 3.838569164276123\n",
            "Running evaluation at iteration 7450...\n",
            "Iteration 7450, Train Loss: 3.853347063064575 Eval Loss: 3.7539183139801025\n",
            "Running evaluation at iteration 7460...\n",
            "Iteration 7460, Train Loss: 3.618086338043213 Eval Loss: 3.4733878135681153\n",
            "Running evaluation at iteration 7470...\n",
            "Iteration 7470, Train Loss: 3.530168056488037 Eval Loss: 3.6505640745162964\n",
            "Running evaluation at iteration 7480...\n",
            "Iteration 7480, Train Loss: 4.324295997619629 Eval Loss: 3.649753046035767\n",
            "Running evaluation at iteration 7490...\n",
            "Iteration 7490, Train Loss: 4.10161828994751 Eval Loss: 3.6657055377960206\n",
            "Running evaluation at iteration 7500...\n",
            "Iteration 7500, Train Loss: 3.6739654541015625 Eval Loss: 3.6112428188323973\n",
            "Running evaluation at iteration 7510...\n",
            "Iteration 7510, Train Loss: 3.571666955947876 Eval Loss: 3.6101022005081176\n",
            "Running evaluation at iteration 7520...\n",
            "Iteration 7520, Train Loss: 3.5316314697265625 Eval Loss: 3.9761949300765993\n",
            "Running evaluation at iteration 7530...\n",
            "Iteration 7530, Train Loss: 4.051072120666504 Eval Loss: 3.63109085559845\n",
            "Running evaluation at iteration 7540...\n",
            "Iteration 7540, Train Loss: 3.635613441467285 Eval Loss: 3.508439230918884\n",
            "Running evaluation at iteration 7550...\n",
            "Iteration 7550, Train Loss: 3.8759164810180664 Eval Loss: 3.493015241622925\n",
            "Running evaluation at iteration 7560...\n",
            "Iteration 7560, Train Loss: 3.7694482803344727 Eval Loss: 3.5423983097076417\n",
            "Running evaluation at iteration 7570...\n",
            "Iteration 7570, Train Loss: 2.998253107070923 Eval Loss: 3.5989821910858155\n",
            "Running evaluation at iteration 7580...\n",
            "Iteration 7580, Train Loss: 3.1853203773498535 Eval Loss: 3.5433356285095217\n",
            "Running evaluation at iteration 7590...\n",
            "Iteration 7590, Train Loss: 3.061514377593994 Eval Loss: 3.621545433998108\n",
            "Running evaluation at iteration 7600...\n",
            "Iteration 7600, Train Loss: 3.498427629470825 Eval Loss: 3.745902109146118\n",
            "Running evaluation at iteration 7610...\n",
            "Iteration 7610, Train Loss: 4.508487224578857 Eval Loss: 3.784563684463501\n",
            "Running evaluation at iteration 7620...\n",
            "Iteration 7620, Train Loss: 3.4836626052856445 Eval Loss: 3.655785870552063\n",
            "Running evaluation at iteration 7630...\n",
            "Iteration 7630, Train Loss: 3.531510353088379 Eval Loss: 3.4429057598114015\n",
            "Running evaluation at iteration 7640...\n",
            "Iteration 7640, Train Loss: 3.884098529815674 Eval Loss: 3.549499845504761\n",
            "Running evaluation at iteration 7650...\n",
            "Iteration 7650, Train Loss: 3.7188217639923096 Eval Loss: 3.5999284744262696\n",
            "Running evaluation at iteration 7660...\n",
            "Iteration 7660, Train Loss: 4.085714340209961 Eval Loss: 3.6222444772720337\n",
            "Running evaluation at iteration 7670...\n",
            "Iteration 7670, Train Loss: 3.575380325317383 Eval Loss: 3.7628937005996703\n",
            "Running evaluation at iteration 7680...\n",
            "Iteration 7680, Train Loss: 4.021993637084961 Eval Loss: 3.7433995962142945\n",
            "Running evaluation at iteration 7690...\n",
            "Iteration 7690, Train Loss: 3.591024875640869 Eval Loss: 3.5827672243118287\n",
            "Running evaluation at iteration 7700...\n",
            "Iteration 7700, Train Loss: 4.482712268829346 Eval Loss: 3.6377708435058596\n",
            "Running evaluation at iteration 7710...\n",
            "Iteration 7710, Train Loss: 4.102880954742432 Eval Loss: 3.5271401405334473\n",
            "Running evaluation at iteration 7720...\n",
            "Iteration 7720, Train Loss: 4.407374382019043 Eval Loss: 3.735899233818054\n",
            "Running evaluation at iteration 7730...\n",
            "Iteration 7730, Train Loss: 4.357114315032959 Eval Loss: 3.639380669593811\n",
            "Running evaluation at iteration 7740...\n",
            "Iteration 7740, Train Loss: 3.8847663402557373 Eval Loss: 3.7221781492233275\n",
            "Running evaluation at iteration 7750...\n",
            "Iteration 7750, Train Loss: 3.5230553150177 Eval Loss: 3.4823498487472535\n",
            "Running evaluation at iteration 7760...\n",
            "Iteration 7760, Train Loss: 4.160274505615234 Eval Loss: 3.597297716140747\n",
            "Running evaluation at iteration 7770...\n",
            "Iteration 7770, Train Loss: 3.712315559387207 Eval Loss: 3.5478793144226075\n",
            "Running evaluation at iteration 7780...\n",
            "Iteration 7780, Train Loss: 3.465113639831543 Eval Loss: 3.6758323192596434\n",
            "Running evaluation at iteration 7790...\n",
            "Iteration 7790, Train Loss: 3.3370351791381836 Eval Loss: 3.675699973106384\n",
            "Running evaluation at iteration 7800...\n",
            "Iteration 7800, Train Loss: 3.6421377658843994 Eval Loss: 3.458480787277222\n",
            "Running evaluation at iteration 7810...\n",
            "Iteration 7810, Train Loss: 3.809218406677246 Eval Loss: 3.595275378227234\n",
            "Running evaluation at iteration 7820...\n",
            "Iteration 7820, Train Loss: 3.6236557960510254 Eval Loss: 3.6672497272491453\n",
            "Running evaluation at iteration 7830...\n",
            "Iteration 7830, Train Loss: 3.3230643272399902 Eval Loss: 3.624566125869751\n",
            "Running evaluation at iteration 7840...\n",
            "Iteration 7840, Train Loss: 3.284440040588379 Eval Loss: 3.6829489707946776\n",
            "Running evaluation at iteration 7850...\n",
            "Iteration 7850, Train Loss: 3.298229694366455 Eval Loss: 3.4477808237075807\n",
            "Running evaluation at iteration 7860...\n",
            "Iteration 7860, Train Loss: 3.318915605545044 Eval Loss: 3.5654566049575807\n",
            "Running evaluation at iteration 7870...\n",
            "Iteration 7870, Train Loss: 3.912307024002075 Eval Loss: 3.711369013786316\n",
            "Running evaluation at iteration 7880...\n",
            "Iteration 7880, Train Loss: 3.4099385738372803 Eval Loss: 3.3441015005111696\n",
            "Running evaluation at iteration 7890...\n",
            "Iteration 7890, Train Loss: 4.051717758178711 Eval Loss: 3.62798593044281\n",
            "Running evaluation at iteration 7900...\n",
            "Iteration 7900, Train Loss: 3.680091142654419 Eval Loss: 3.878839921951294\n",
            "Running evaluation at iteration 7910...\n",
            "Iteration 7910, Train Loss: 3.6730523109436035 Eval Loss: 3.7122344017028808\n",
            "Running evaluation at iteration 7920...\n",
            "Iteration 7920, Train Loss: 4.018686771392822 Eval Loss: 3.6331353187561035\n",
            "Running evaluation at iteration 7930...\n",
            "Iteration 7930, Train Loss: 3.5642998218536377 Eval Loss: 3.6774233102798464\n",
            "Running evaluation at iteration 7940...\n",
            "Iteration 7940, Train Loss: 3.4894492626190186 Eval Loss: 3.701671504974365\n",
            "Running evaluation at iteration 7950...\n",
            "Iteration 7950, Train Loss: 3.597946882247925 Eval Loss: 3.4102303743362428\n",
            "Running evaluation at iteration 7960...\n",
            "Iteration 7960, Train Loss: 2.719667673110962 Eval Loss: 3.907861828804016\n",
            "Running evaluation at iteration 7970...\n",
            "Iteration 7970, Train Loss: 3.5995795726776123 Eval Loss: 3.65778706073761\n",
            "Running evaluation at iteration 7980...\n",
            "Iteration 7980, Train Loss: 3.380941867828369 Eval Loss: 3.733252930641174\n",
            "Running evaluation at iteration 7990...\n",
            "Iteration 7990, Train Loss: 3.439180612564087 Eval Loss: 3.7337385177612306\n",
            "Running evaluation at iteration 8000...\n",
            "Iteration 8000, Train Loss: 3.6493654251098633 Eval Loss: 3.618408679962158\n",
            "Running evaluation at iteration 8010...\n",
            "Iteration 8010, Train Loss: 2.3543522357940674 Eval Loss: 3.534575319290161\n",
            "Running evaluation at iteration 8020...\n",
            "Iteration 8020, Train Loss: 3.4378838539123535 Eval Loss: 3.6337385892868044\n",
            "Running evaluation at iteration 8030...\n",
            "Iteration 8030, Train Loss: 3.134169340133667 Eval Loss: 3.6165583610534666\n",
            "Running evaluation at iteration 8040...\n",
            "Iteration 8040, Train Loss: 3.791386365890503 Eval Loss: 3.6834856986999513\n",
            "Running evaluation at iteration 8050...\n",
            "Iteration 8050, Train Loss: 3.673178195953369 Eval Loss: 3.4825355768203736\n",
            "Running evaluation at iteration 8060...\n",
            "Iteration 8060, Train Loss: 3.781262159347534 Eval Loss: 3.5383057594299316\n",
            "Running evaluation at iteration 8070...\n",
            "Iteration 8070, Train Loss: 3.41440486907959 Eval Loss: 3.6782949447631834\n",
            "Running evaluation at iteration 8080...\n",
            "Iteration 8080, Train Loss: 3.9908437728881836 Eval Loss: 3.5295423030853272\n",
            "Running evaluation at iteration 8090...\n",
            "Iteration 8090, Train Loss: 3.8596248626708984 Eval Loss: 3.6726653814315795\n",
            "Running evaluation at iteration 8100...\n",
            "Iteration 8100, Train Loss: 4.001733303070068 Eval Loss: 3.5005132436752318\n",
            "Running evaluation at iteration 8110...\n",
            "Iteration 8110, Train Loss: 3.0001065731048584 Eval Loss: 3.6843972206115723\n",
            "Running evaluation at iteration 8120...\n",
            "Iteration 8120, Train Loss: 4.0560622215271 Eval Loss: 3.643281173706055\n",
            "Running evaluation at iteration 8130...\n",
            "Iteration 8130, Train Loss: 3.9933130741119385 Eval Loss: 3.4804725646972656\n",
            "Running evaluation at iteration 8140...\n",
            "Iteration 8140, Train Loss: 3.419369697570801 Eval Loss: 3.785644006729126\n",
            "Running evaluation at iteration 8150...\n",
            "Iteration 8150, Train Loss: 3.5526723861694336 Eval Loss: 3.631642556190491\n",
            "Running evaluation at iteration 8160...\n",
            "Iteration 8160, Train Loss: 3.5387868881225586 Eval Loss: 3.482476091384888\n",
            "Running evaluation at iteration 8170...\n",
            "Iteration 8170, Train Loss: 4.187964916229248 Eval Loss: 3.5957182168960573\n",
            "Running evaluation at iteration 8180...\n",
            "Iteration 8180, Train Loss: 4.030482292175293 Eval Loss: 3.65239474773407\n",
            "Running evaluation at iteration 8190...\n",
            "Iteration 8190, Train Loss: 3.988035202026367 Eval Loss: 3.672319793701172\n",
            "Running evaluation at iteration 8200...\n",
            "Iteration 8200, Train Loss: 3.2032580375671387 Eval Loss: 3.7629319190979005\n",
            "Running evaluation at iteration 8210...\n",
            "Iteration 8210, Train Loss: 3.474740982055664 Eval Loss: 3.616087031364441\n",
            "Running evaluation at iteration 8220...\n",
            "Iteration 8220, Train Loss: 3.106464147567749 Eval Loss: 3.644584369659424\n",
            "Running evaluation at iteration 8230...\n",
            "Iteration 8230, Train Loss: 3.4631223678588867 Eval Loss: 3.6188999891281126\n",
            "Running evaluation at iteration 8240...\n",
            "Iteration 8240, Train Loss: 3.635981798171997 Eval Loss: 3.5034655332565308\n",
            "Running evaluation at iteration 8250...\n",
            "Iteration 8250, Train Loss: 3.138699531555176 Eval Loss: 3.605269360542297\n",
            "Running evaluation at iteration 8260...\n",
            "Iteration 8260, Train Loss: 3.3877556324005127 Eval Loss: 3.590647339820862\n",
            "Running evaluation at iteration 8270...\n",
            "Iteration 8270, Train Loss: 3.69382905960083 Eval Loss: 3.5144335269927978\n",
            "Running evaluation at iteration 8280...\n",
            "Iteration 8280, Train Loss: 3.8847005367279053 Eval Loss: 3.6266135215759276\n",
            "Running evaluation at iteration 8290...\n",
            "Iteration 8290, Train Loss: 3.5538489818573 Eval Loss: 3.4208954334259034\n",
            "Running evaluation at iteration 8300...\n",
            "Iteration 8300, Train Loss: 4.812773704528809 Eval Loss: 3.541047191619873\n",
            "Running evaluation at iteration 8310...\n",
            "Iteration 8310, Train Loss: 3.562178134918213 Eval Loss: 3.8944148302078245\n",
            "Running evaluation at iteration 8320...\n",
            "Iteration 8320, Train Loss: 4.231644153594971 Eval Loss: 3.576981616020203\n",
            "Running evaluation at iteration 8330...\n",
            "Iteration 8330, Train Loss: 3.2699203491210938 Eval Loss: 3.4819931983947754\n",
            "Running evaluation at iteration 8340...\n",
            "Iteration 8340, Train Loss: 3.9924206733703613 Eval Loss: 3.7109355211257933\n",
            "Running evaluation at iteration 8350...\n",
            "Iteration 8350, Train Loss: 3.6692469120025635 Eval Loss: 3.5866371154785157\n",
            "Running evaluation at iteration 8360...\n",
            "Iteration 8360, Train Loss: 3.672717332839966 Eval Loss: 3.4616336107254027\n",
            "Running evaluation at iteration 8370...\n",
            "Iteration 8370, Train Loss: 3.697850227355957 Eval Loss: 3.503773236274719\n",
            "Running evaluation at iteration 8380...\n",
            "Iteration 8380, Train Loss: 4.052733898162842 Eval Loss: 3.61281213760376\n",
            "Running evaluation at iteration 8390...\n",
            "Iteration 8390, Train Loss: 3.5960118770599365 Eval Loss: 3.6258310317993163\n",
            "Running evaluation at iteration 8400...\n",
            "Iteration 8400, Train Loss: 3.4556493759155273 Eval Loss: 3.5926825523376467\n",
            "Running evaluation at iteration 8410...\n",
            "Iteration 8410, Train Loss: 4.081321716308594 Eval Loss: 3.707639479637146\n",
            "Running evaluation at iteration 8420...\n",
            "Iteration 8420, Train Loss: 4.38838529586792 Eval Loss: 3.7469600200653077\n",
            "Running evaluation at iteration 8430...\n",
            "Iteration 8430, Train Loss: 3.7694215774536133 Eval Loss: 3.4458781719207763\n",
            "Running evaluation at iteration 8440...\n",
            "Iteration 8440, Train Loss: 3.7443201541900635 Eval Loss: 3.6723387479782104\n",
            "Running evaluation at iteration 8450...\n",
            "Iteration 8450, Train Loss: 3.2073941230773926 Eval Loss: 3.5910629510879515\n",
            "Running evaluation at iteration 8460...\n",
            "Iteration 8460, Train Loss: 3.87380313873291 Eval Loss: 3.546286129951477\n",
            "Running evaluation at iteration 8470...\n",
            "Iteration 8470, Train Loss: 3.1760973930358887 Eval Loss: 3.599947381019592\n",
            "Running evaluation at iteration 8480...\n",
            "Iteration 8480, Train Loss: 4.044595241546631 Eval Loss: 3.696020770072937\n",
            "Running evaluation at iteration 8490...\n",
            "Iteration 8490, Train Loss: 3.2536675930023193 Eval Loss: 3.7363712549209596\n",
            "Running evaluation at iteration 8500...\n",
            "Iteration 8500, Train Loss: 3.4717302322387695 Eval Loss: 3.652780270576477\n",
            "Running evaluation at iteration 8510...\n",
            "Iteration 8510, Train Loss: 3.9280333518981934 Eval Loss: 3.5606901168823244\n",
            "Running evaluation at iteration 8520...\n",
            "Iteration 8520, Train Loss: 4.071246147155762 Eval Loss: 3.716799426078796\n",
            "Running evaluation at iteration 8530...\n",
            "Iteration 8530, Train Loss: 4.174127578735352 Eval Loss: 3.405636692047119\n",
            "Running evaluation at iteration 8540...\n",
            "Iteration 8540, Train Loss: 3.5253024101257324 Eval Loss: 3.6550957441329954\n",
            "Running evaluation at iteration 8550...\n",
            "Iteration 8550, Train Loss: 3.7465028762817383 Eval Loss: 3.9020511388778685\n",
            "Running evaluation at iteration 8560...\n",
            "Iteration 8560, Train Loss: 3.6573073863983154 Eval Loss: 3.6749877452850344\n",
            "Running evaluation at iteration 8570...\n",
            "Iteration 8570, Train Loss: 4.096861362457275 Eval Loss: 3.62633855342865\n",
            "Running evaluation at iteration 8580...\n",
            "Iteration 8580, Train Loss: 3.8280348777770996 Eval Loss: 3.71865496635437\n",
            "Running evaluation at iteration 8590...\n",
            "Iteration 8590, Train Loss: 3.32900333404541 Eval Loss: 3.5944309711456297\n",
            "Running evaluation at iteration 8600...\n",
            "Iteration 8600, Train Loss: 3.113495111465454 Eval Loss: 3.7805386781692505\n",
            "Running evaluation at iteration 8610...\n",
            "Iteration 8610, Train Loss: 3.3942089080810547 Eval Loss: 3.7785799741744994\n",
            "Running evaluation at iteration 8620...\n",
            "Iteration 8620, Train Loss: 3.6939499378204346 Eval Loss: 3.5706175804138183\n",
            "Running evaluation at iteration 8630...\n",
            "Iteration 8630, Train Loss: 3.453657865524292 Eval Loss: 3.54546000957489\n",
            "Running evaluation at iteration 8640...\n",
            "Iteration 8640, Train Loss: 3.1736860275268555 Eval Loss: 3.375479531288147\n",
            "Running evaluation at iteration 8650...\n",
            "Iteration 8650, Train Loss: 3.5746500492095947 Eval Loss: 3.4832216262817384\n",
            "Running evaluation at iteration 8660...\n",
            "Iteration 8660, Train Loss: 3.1411426067352295 Eval Loss: 3.4606693506240847\n",
            "Running evaluation at iteration 8670...\n",
            "Iteration 8670, Train Loss: 3.5898489952087402 Eval Loss: 3.7969202518463137\n",
            "Running evaluation at iteration 8680...\n",
            "Iteration 8680, Train Loss: 4.097592353820801 Eval Loss: 3.4957078456878663\n",
            "Running evaluation at iteration 8690...\n",
            "Iteration 8690, Train Loss: 3.5782859325408936 Eval Loss: 3.781282901763916\n",
            "Running evaluation at iteration 8700...\n",
            "Iteration 8700, Train Loss: 4.179787635803223 Eval Loss: 3.5305912256240846\n",
            "Running evaluation at iteration 8710...\n",
            "Iteration 8710, Train Loss: 3.970968008041382 Eval Loss: 3.620319294929504\n",
            "Running evaluation at iteration 8720...\n",
            "Iteration 8720, Train Loss: 4.278515815734863 Eval Loss: 3.6891488552093508\n",
            "Running evaluation at iteration 8730...\n",
            "Iteration 8730, Train Loss: 3.5666043758392334 Eval Loss: 3.589288020133972\n",
            "Running evaluation at iteration 8740...\n",
            "Iteration 8740, Train Loss: 3.5894298553466797 Eval Loss: 3.426347541809082\n",
            "Running evaluation at iteration 8750...\n",
            "Iteration 8750, Train Loss: 3.499959707260132 Eval Loss: 3.608770513534546\n",
            "Running evaluation at iteration 8760...\n",
            "Iteration 8760, Train Loss: 3.815728187561035 Eval Loss: 3.5367300271987916\n",
            "Running evaluation at iteration 8770...\n",
            "Iteration 8770, Train Loss: 3.8945789337158203 Eval Loss: 3.5273905992507935\n",
            "Running evaluation at iteration 8780...\n",
            "Iteration 8780, Train Loss: 4.154300689697266 Eval Loss: 3.574593663215637\n",
            "Running evaluation at iteration 8790...\n",
            "Iteration 8790, Train Loss: 3.018815279006958 Eval Loss: 3.6068961381912232\n",
            "Running evaluation at iteration 8800...\n",
            "Iteration 8800, Train Loss: 3.1537222862243652 Eval Loss: 3.664239025115967\n",
            "Running evaluation at iteration 8810...\n",
            "Iteration 8810, Train Loss: 3.3021903038024902 Eval Loss: 3.4464736461639403\n",
            "Running evaluation at iteration 8820...\n",
            "Iteration 8820, Train Loss: 4.124876022338867 Eval Loss: 3.5290050506591797\n",
            "Running evaluation at iteration 8830...\n",
            "Iteration 8830, Train Loss: 3.8337278366088867 Eval Loss: 3.549013781547546\n",
            "Running evaluation at iteration 8840...\n",
            "Iteration 8840, Train Loss: 3.3440842628479004 Eval Loss: 3.6077481269836427\n",
            "Running evaluation at iteration 8850...\n",
            "Iteration 8850, Train Loss: 4.309225082397461 Eval Loss: 3.4174967288970945\n",
            "Running evaluation at iteration 8860...\n",
            "Iteration 8860, Train Loss: 3.269822597503662 Eval Loss: 3.558796787261963\n",
            "Running evaluation at iteration 8870...\n",
            "Iteration 8870, Train Loss: 3.3718457221984863 Eval Loss: 3.9412776708602903\n",
            "Running evaluation at iteration 8880...\n",
            "Iteration 8880, Train Loss: 3.472738265991211 Eval Loss: 3.4062850952148436\n",
            "Running evaluation at iteration 8890...\n",
            "Iteration 8890, Train Loss: 3.8052704334259033 Eval Loss: 3.477554988861084\n",
            "Running evaluation at iteration 8900...\n",
            "Iteration 8900, Train Loss: 3.2645938396453857 Eval Loss: 3.671850252151489\n",
            "Running evaluation at iteration 8910...\n",
            "Iteration 8910, Train Loss: 3.194026470184326 Eval Loss: 3.519842743873596\n",
            "Running evaluation at iteration 8920...\n",
            "Iteration 8920, Train Loss: 3.298253059387207 Eval Loss: 3.3935451984405516\n",
            "Running evaluation at iteration 8930...\n",
            "Iteration 8930, Train Loss: 3.398038864135742 Eval Loss: 3.6350382566452026\n",
            "Running evaluation at iteration 8940...\n",
            "Iteration 8940, Train Loss: 3.7924468517303467 Eval Loss: 3.575294828414917\n",
            "Running evaluation at iteration 8950...\n",
            "Iteration 8950, Train Loss: 3.444913864135742 Eval Loss: 3.5839905023574827\n",
            "Running evaluation at iteration 8960...\n",
            "Iteration 8960, Train Loss: 4.738051414489746 Eval Loss: 3.8095550537109375\n",
            "Running evaluation at iteration 8970...\n",
            "Iteration 8970, Train Loss: 3.9954843521118164 Eval Loss: 3.683002758026123\n",
            "Running evaluation at iteration 8980...\n",
            "Iteration 8980, Train Loss: 3.726858615875244 Eval Loss: 3.687288594245911\n",
            "Running evaluation at iteration 8990...\n",
            "Iteration 8990, Train Loss: 3.0671579837799072 Eval Loss: 3.4039698123931883\n",
            "Running evaluation at iteration 9000...\n",
            "Iteration 9000, Train Loss: 3.9531075954437256 Eval Loss: 3.5399585247039793\n",
            "Running evaluation at iteration 9010...\n",
            "Iteration 9010, Train Loss: 3.9684762954711914 Eval Loss: 3.6164114713668822\n",
            "Running evaluation at iteration 9020...\n",
            "Iteration 9020, Train Loss: 3.8411617279052734 Eval Loss: 3.6323572635650634\n",
            "Running evaluation at iteration 9030...\n",
            "Iteration 9030, Train Loss: 3.8508832454681396 Eval Loss: 3.6334691047668457\n",
            "Running evaluation at iteration 9040...\n",
            "Iteration 9040, Train Loss: 3.5148797035217285 Eval Loss: 3.589123249053955\n",
            "Running evaluation at iteration 9050...\n",
            "Iteration 9050, Train Loss: 4.231179714202881 Eval Loss: 3.8222848653793333\n",
            "Running evaluation at iteration 9060...\n",
            "Iteration 9060, Train Loss: 3.522274971008301 Eval Loss: 3.4655192375183104\n",
            "Running evaluation at iteration 9070...\n",
            "Iteration 9070, Train Loss: 3.993772029876709 Eval Loss: 3.7116705894470217\n",
            "Running evaluation at iteration 9080...\n",
            "Iteration 9080, Train Loss: 3.750100612640381 Eval Loss: 3.3581040620803835\n",
            "Running evaluation at iteration 9090...\n",
            "Iteration 9090, Train Loss: 3.889345169067383 Eval Loss: 3.5795387506484984\n",
            "Running evaluation at iteration 9100...\n",
            "Iteration 9100, Train Loss: 4.058107376098633 Eval Loss: 3.4178571939468383\n",
            "Running evaluation at iteration 9110...\n",
            "Iteration 9110, Train Loss: 2.998922109603882 Eval Loss: 3.594744086265564\n",
            "Running evaluation at iteration 9120...\n",
            "Iteration 9120, Train Loss: 2.9805634021759033 Eval Loss: 3.629542660713196\n",
            "Running evaluation at iteration 9130...\n",
            "Iteration 9130, Train Loss: 3.317547559738159 Eval Loss: 3.771361780166626\n",
            "Running evaluation at iteration 9140...\n",
            "Iteration 9140, Train Loss: 3.705125331878662 Eval Loss: 3.3682525396347045\n",
            "Running evaluation at iteration 9150...\n",
            "Iteration 9150, Train Loss: 2.9748995304107666 Eval Loss: 3.370336151123047\n",
            "Running evaluation at iteration 9160...\n",
            "Iteration 9160, Train Loss: 3.4924798011779785 Eval Loss: 3.5870038509368896\n",
            "Running evaluation at iteration 9170...\n",
            "Iteration 9170, Train Loss: 3.3708786964416504 Eval Loss: 3.5881609439849855\n",
            "Running evaluation at iteration 9180...\n",
            "Iteration 9180, Train Loss: 3.175229787826538 Eval Loss: 3.5771132946014403\n",
            "Running evaluation at iteration 9190...\n",
            "Iteration 9190, Train Loss: 3.5884995460510254 Eval Loss: 3.3354005336761476\n",
            "Running evaluation at iteration 9200...\n",
            "Iteration 9200, Train Loss: 4.001929759979248 Eval Loss: 3.567553234100342\n",
            "Running evaluation at iteration 9210...\n",
            "Iteration 9210, Train Loss: 4.153140068054199 Eval Loss: 3.7294196844100953\n",
            "Running evaluation at iteration 9220...\n",
            "Iteration 9220, Train Loss: 3.7368974685668945 Eval Loss: 3.5151157140731812\n",
            "Running evaluation at iteration 9230...\n",
            "Iteration 9230, Train Loss: 4.5208868980407715 Eval Loss: 3.621701788902283\n",
            "Running evaluation at iteration 9240...\n",
            "Iteration 9240, Train Loss: 4.032406806945801 Eval Loss: 3.667016339302063\n",
            "Running evaluation at iteration 9250...\n",
            "Iteration 9250, Train Loss: 3.7800867557525635 Eval Loss: 3.6828731536865233\n",
            "Running evaluation at iteration 9260...\n",
            "Iteration 9260, Train Loss: 3.9128644466400146 Eval Loss: 3.6601723432540894\n",
            "Running evaluation at iteration 9270...\n",
            "Iteration 9270, Train Loss: 3.7846736907958984 Eval Loss: 3.5109614372253417\n",
            "Running evaluation at iteration 9280...\n",
            "Iteration 9280, Train Loss: 3.699857473373413 Eval Loss: 3.5944410085678102\n",
            "Running evaluation at iteration 9290...\n",
            "Iteration 9290, Train Loss: 4.237449645996094 Eval Loss: 3.448212003707886\n",
            "Running evaluation at iteration 9300...\n",
            "Iteration 9300, Train Loss: 3.508913278579712 Eval Loss: 3.619784784317017\n",
            "Running evaluation at iteration 9310...\n",
            "Iteration 9310, Train Loss: 3.8581607341766357 Eval Loss: 3.496436095237732\n",
            "Running evaluation at iteration 9320...\n",
            "Iteration 9320, Train Loss: 3.3549437522888184 Eval Loss: 3.548998737335205\n",
            "Running evaluation at iteration 9330...\n",
            "Iteration 9330, Train Loss: 3.527538776397705 Eval Loss: 3.3807315826416016\n",
            "Running evaluation at iteration 9340...\n",
            "Iteration 9340, Train Loss: 3.947267770767212 Eval Loss: 3.567117714881897\n",
            "Running evaluation at iteration 9350...\n",
            "Iteration 9350, Train Loss: 3.4550344944000244 Eval Loss: 3.535686492919922\n",
            "Running evaluation at iteration 9360...\n",
            "Iteration 9360, Train Loss: 3.989919424057007 Eval Loss: 3.4645238161087035\n",
            "Running evaluation at iteration 9370...\n",
            "Iteration 9370, Train Loss: 3.6776745319366455 Eval Loss: 3.5903354406356813\n",
            "Running evaluation at iteration 9380...\n",
            "Iteration 9380, Train Loss: 3.277702569961548 Eval Loss: 3.4913249731063845\n",
            "Running evaluation at iteration 9390...\n",
            "Iteration 9390, Train Loss: 3.707200050354004 Eval Loss: 3.5595863580703737\n",
            "Running evaluation at iteration 9400...\n",
            "Iteration 9400, Train Loss: 3.694551467895508 Eval Loss: 3.6154793977737425\n",
            "Running evaluation at iteration 9410...\n",
            "Iteration 9410, Train Loss: 3.9554224014282227 Eval Loss: 3.628794002532959\n",
            "Running evaluation at iteration 9420...\n",
            "Iteration 9420, Train Loss: 3.9595258235931396 Eval Loss: 3.6087069272994996\n",
            "Running evaluation at iteration 9430...\n",
            "Iteration 9430, Train Loss: 4.042201042175293 Eval Loss: 3.3710782051086428\n",
            "Running evaluation at iteration 9440...\n",
            "Iteration 9440, Train Loss: 3.4969894886016846 Eval Loss: 3.4636767864227296\n",
            "Running evaluation at iteration 9450...\n",
            "Iteration 9450, Train Loss: 3.863447904586792 Eval Loss: 3.354759120941162\n",
            "Running evaluation at iteration 9460...\n",
            "Iteration 9460, Train Loss: 3.8908274173736572 Eval Loss: 3.5462698459625246\n",
            "Running evaluation at iteration 9470...\n",
            "Iteration 9470, Train Loss: 3.9124834537506104 Eval Loss: 3.4538156509399416\n",
            "Running evaluation at iteration 9480...\n",
            "Iteration 9480, Train Loss: 3.5872278213500977 Eval Loss: 3.569053316116333\n",
            "Running evaluation at iteration 9490...\n",
            "Iteration 9490, Train Loss: 3.514772653579712 Eval Loss: 3.394485425949097\n",
            "Running evaluation at iteration 9500...\n",
            "Iteration 9500, Train Loss: 3.4016101360321045 Eval Loss: 3.3660320043563843\n",
            "Running evaluation at iteration 9510...\n",
            "Iteration 9510, Train Loss: 3.8186728954315186 Eval Loss: 3.2204145193099976\n",
            "Running evaluation at iteration 9520...\n",
            "Iteration 9520, Train Loss: 3.6118712425231934 Eval Loss: 3.703491449356079\n",
            "Running evaluation at iteration 9530...\n",
            "Iteration 9530, Train Loss: 3.6258373260498047 Eval Loss: 3.505088543891907\n",
            "Running evaluation at iteration 9540...\n",
            "Iteration 9540, Train Loss: 3.7602245807647705 Eval Loss: 3.70408411026001\n",
            "Running evaluation at iteration 9550...\n",
            "Iteration 9550, Train Loss: 3.3353922367095947 Eval Loss: 3.482977271080017\n",
            "Running evaluation at iteration 9560...\n",
            "Iteration 9560, Train Loss: 4.074499130249023 Eval Loss: 3.6625497341156006\n",
            "Running evaluation at iteration 9570...\n",
            "Iteration 9570, Train Loss: 3.5421969890594482 Eval Loss: 3.442111015319824\n",
            "Running evaluation at iteration 9580...\n",
            "Iteration 9580, Train Loss: 3.4010281562805176 Eval Loss: 3.5633835315704347\n",
            "Running evaluation at iteration 9590...\n",
            "Iteration 9590, Train Loss: 3.0961248874664307 Eval Loss: 3.742971992492676\n",
            "Running evaluation at iteration 9600...\n",
            "Iteration 9600, Train Loss: 3.7818479537963867 Eval Loss: 3.4284869194030763\n",
            "Running evaluation at iteration 9610...\n",
            "Iteration 9610, Train Loss: 3.694896936416626 Eval Loss: 3.5947437286376953\n",
            "Running evaluation at iteration 9620...\n",
            "Iteration 9620, Train Loss: 3.4684665203094482 Eval Loss: 3.511495327949524\n",
            "Running evaluation at iteration 9630...\n",
            "Iteration 9630, Train Loss: 3.167121171951294 Eval Loss: 3.5278581380844116\n",
            "Running evaluation at iteration 9640...\n",
            "Iteration 9640, Train Loss: 3.68769907951355 Eval Loss: 3.412938451766968\n",
            "Running evaluation at iteration 9650...\n",
            "Iteration 9650, Train Loss: 2.48715877532959 Eval Loss: 3.657580852508545\n",
            "Running evaluation at iteration 9660...\n",
            "Iteration 9660, Train Loss: 3.5887691974639893 Eval Loss: 3.4626299142837524\n",
            "Running evaluation at iteration 9670...\n",
            "Iteration 9670, Train Loss: 3.682055950164795 Eval Loss: 3.389896583557129\n",
            "Running evaluation at iteration 9680...\n",
            "Iteration 9680, Train Loss: 3.3517019748687744 Eval Loss: 3.3550692796707153\n",
            "Running evaluation at iteration 9690...\n",
            "Iteration 9690, Train Loss: 4.080892562866211 Eval Loss: 3.6688372135162353\n",
            "Running evaluation at iteration 9700...\n",
            "Iteration 9700, Train Loss: 3.962998628616333 Eval Loss: 3.5306275367736815\n",
            "Running evaluation at iteration 9710...\n",
            "Iteration 9710, Train Loss: 2.984736442565918 Eval Loss: 3.478728413581848\n",
            "Running evaluation at iteration 9720...\n",
            "Iteration 9720, Train Loss: 3.7095043659210205 Eval Loss: 3.527944254875183\n",
            "Running evaluation at iteration 9730...\n",
            "Iteration 9730, Train Loss: 3.8446741104125977 Eval Loss: 3.344283127784729\n",
            "Running evaluation at iteration 9740...\n",
            "Iteration 9740, Train Loss: 3.2128796577453613 Eval Loss: 3.7096025705337525\n",
            "Running evaluation at iteration 9750...\n",
            "Iteration 9750, Train Loss: 3.9396297931671143 Eval Loss: 3.5435088872909546\n",
            "Running evaluation at iteration 9760...\n",
            "Iteration 9760, Train Loss: 4.236081123352051 Eval Loss: 3.905649733543396\n",
            "Running evaluation at iteration 9770...\n",
            "Iteration 9770, Train Loss: 3.9559013843536377 Eval Loss: 3.4643157005310057\n",
            "Running evaluation at iteration 9780...\n",
            "Iteration 9780, Train Loss: 3.5463716983795166 Eval Loss: 3.503655219078064\n",
            "Running evaluation at iteration 9790...\n",
            "Iteration 9790, Train Loss: 3.3353633880615234 Eval Loss: 3.512034773826599\n",
            "Running evaluation at iteration 9800...\n",
            "Iteration 9800, Train Loss: 3.3623135089874268 Eval Loss: 3.5436509370803835\n",
            "Running evaluation at iteration 9810...\n",
            "Iteration 9810, Train Loss: 3.449578285217285 Eval Loss: 3.601421046257019\n",
            "Running evaluation at iteration 9820...\n",
            "Iteration 9820, Train Loss: 3.4941439628601074 Eval Loss: 3.4411181449890136\n",
            "Running evaluation at iteration 9830...\n",
            "Iteration 9830, Train Loss: 3.643566846847534 Eval Loss: 3.6211350440979\n",
            "Running evaluation at iteration 9840...\n",
            "Iteration 9840, Train Loss: 3.459890365600586 Eval Loss: 3.4662421226501463\n",
            "Running evaluation at iteration 9850...\n",
            "Iteration 9850, Train Loss: 3.654372453689575 Eval Loss: 3.5603690147399902\n",
            "Running evaluation at iteration 9860...\n",
            "Iteration 9860, Train Loss: 3.8510630130767822 Eval Loss: 3.4829832792282103\n",
            "Running evaluation at iteration 9870...\n",
            "Iteration 9870, Train Loss: 3.3122336864471436 Eval Loss: 3.815164637565613\n",
            "Running evaluation at iteration 9880...\n",
            "Iteration 9880, Train Loss: 4.101615905761719 Eval Loss: 3.6106773376464845\n",
            "Running evaluation at iteration 9890...\n",
            "Iteration 9890, Train Loss: 3.4446706771850586 Eval Loss: 3.8007394790649416\n",
            "Running evaluation at iteration 9900...\n",
            "Iteration 9900, Train Loss: 3.4613125324249268 Eval Loss: 3.5107375144958497\n",
            "Running evaluation at iteration 9910...\n",
            "Iteration 9910, Train Loss: 3.5420644283294678 Eval Loss: 3.4584627389907836\n",
            "Running evaluation at iteration 9920...\n",
            "Iteration 9920, Train Loss: 3.122542381286621 Eval Loss: 3.49948673248291\n",
            "Running evaluation at iteration 9930...\n",
            "Iteration 9930, Train Loss: 3.340341091156006 Eval Loss: 3.3744935989379883\n",
            "Running evaluation at iteration 9940...\n",
            "Iteration 9940, Train Loss: 3.736114740371704 Eval Loss: 3.4005714893341064\n",
            "Running evaluation at iteration 9950...\n",
            "Iteration 9950, Train Loss: 4.435467720031738 Eval Loss: 3.6433013916015624\n",
            "Running evaluation at iteration 9960...\n",
            "Iteration 9960, Train Loss: 3.716498374938965 Eval Loss: 3.4254756212234496\n",
            "Running evaluation at iteration 9970...\n",
            "Iteration 9970, Train Loss: 3.6730360984802246 Eval Loss: 3.5806101083755495\n",
            "Running evaluation at iteration 9980...\n",
            "Iteration 9980, Train Loss: 3.916080951690674 Eval Loss: 3.493188667297363\n",
            "Running evaluation at iteration 9990...\n",
            "Iteration 9990, Train Loss: 4.025958061218262 Eval Loss: 3.6601001977920533\n",
            "Running evaluation at iteration 10000...\n",
            "Iteration 10000, Train Loss: 3.296133518218994 Eval Loss: 3.64429931640625\n",
            "Running evaluation at iteration 10010...\n",
            "Iteration 10010, Train Loss: 3.822930335998535 Eval Loss: 3.6193445920944214\n",
            "Running evaluation at iteration 10020...\n",
            "Iteration 10020, Train Loss: 4.308575630187988 Eval Loss: 3.4978066444396974\n",
            "Running evaluation at iteration 10030...\n",
            "Iteration 10030, Train Loss: 3.662372350692749 Eval Loss: 3.5706150054931642\n",
            "Running evaluation at iteration 10040...\n",
            "Iteration 10040, Train Loss: 3.793383836746216 Eval Loss: 3.5202115058898924\n",
            "Running evaluation at iteration 10050...\n",
            "Iteration 10050, Train Loss: 3.1748464107513428 Eval Loss: 3.3349420547485353\n",
            "Running evaluation at iteration 10060...\n",
            "Iteration 10060, Train Loss: 3.7326772212982178 Eval Loss: 3.489458513259888\n",
            "Running evaluation at iteration 10070...\n",
            "Iteration 10070, Train Loss: 3.3823068141937256 Eval Loss: 3.3741349935531617\n",
            "Running evaluation at iteration 10080...\n",
            "Iteration 10080, Train Loss: 3.600787878036499 Eval Loss: 3.586811375617981\n",
            "Running evaluation at iteration 10090...\n",
            "Iteration 10090, Train Loss: 3.3120620250701904 Eval Loss: 3.366680145263672\n",
            "Running evaluation at iteration 10100...\n",
            "Iteration 10100, Train Loss: 3.5479226112365723 Eval Loss: 3.4541016101837156\n",
            "Running evaluation at iteration 10110...\n",
            "Iteration 10110, Train Loss: 3.619771718978882 Eval Loss: 3.5460672616958617\n",
            "Running evaluation at iteration 10120...\n",
            "Iteration 10120, Train Loss: 3.6099042892456055 Eval Loss: 3.4045093297958373\n",
            "Running evaluation at iteration 10130...\n",
            "Iteration 10130, Train Loss: 3.424617290496826 Eval Loss: 3.7470771074295044\n",
            "Running evaluation at iteration 10140...\n",
            "Iteration 10140, Train Loss: 2.9834532737731934 Eval Loss: 3.514461636543274\n",
            "Running evaluation at iteration 10150...\n",
            "Iteration 10150, Train Loss: 3.6163649559020996 Eval Loss: 3.5029876470565795\n",
            "Running evaluation at iteration 10160...\n",
            "Iteration 10160, Train Loss: 3.839010715484619 Eval Loss: 3.6163934230804444\n",
            "Running evaluation at iteration 10170...\n",
            "Iteration 10170, Train Loss: 4.011184215545654 Eval Loss: 3.4238295555114746\n",
            "Running evaluation at iteration 10180...\n",
            "Iteration 10180, Train Loss: 4.071387767791748 Eval Loss: 3.599188709259033\n",
            "Running evaluation at iteration 10190...\n",
            "Iteration 10190, Train Loss: 3.5972347259521484 Eval Loss: 3.5367182731628417\n",
            "Running evaluation at iteration 10200...\n",
            "Iteration 10200, Train Loss: 2.9402120113372803 Eval Loss: 3.617687940597534\n",
            "Running evaluation at iteration 10210...\n",
            "Iteration 10210, Train Loss: 3.7197179794311523 Eval Loss: 3.518981909751892\n",
            "Running evaluation at iteration 10220...\n",
            "Iteration 10220, Train Loss: 3.9651389122009277 Eval Loss: 3.471852016448975\n",
            "Running evaluation at iteration 10230...\n",
            "Iteration 10230, Train Loss: 4.069847583770752 Eval Loss: 3.5970683097839355\n",
            "Running evaluation at iteration 10240...\n",
            "Iteration 10240, Train Loss: 4.071141242980957 Eval Loss: 3.461264157295227\n",
            "Running evaluation at iteration 10250...\n",
            "Iteration 10250, Train Loss: 3.6944849491119385 Eval Loss: 3.504702568054199\n",
            "Running evaluation at iteration 10260...\n",
            "Iteration 10260, Train Loss: 3.770481586456299 Eval Loss: 3.4582046031951905\n",
            "Running evaluation at iteration 10270...\n",
            "Iteration 10270, Train Loss: 3.407388687133789 Eval Loss: 3.395614981651306\n",
            "Running evaluation at iteration 10280...\n",
            "Iteration 10280, Train Loss: 3.509798288345337 Eval Loss: 3.548637580871582\n",
            "Running evaluation at iteration 10290...\n",
            "Iteration 10290, Train Loss: 4.06660270690918 Eval Loss: 3.5367018938064576\n",
            "Running evaluation at iteration 10300...\n",
            "Iteration 10300, Train Loss: 3.4049973487854004 Eval Loss: 3.704443883895874\n",
            "Running evaluation at iteration 10310...\n",
            "Iteration 10310, Train Loss: 3.3759963512420654 Eval Loss: 3.7035722494125367\n",
            "Running evaluation at iteration 10320...\n",
            "Iteration 10320, Train Loss: 3.4670250415802 Eval Loss: 3.3229443788528443\n",
            "Running evaluation at iteration 10330...\n",
            "Iteration 10330, Train Loss: 3.541853666305542 Eval Loss: 3.498687744140625\n",
            "Running evaluation at iteration 10340...\n",
            "Iteration 10340, Train Loss: 3.7483932971954346 Eval Loss: 3.5816461563110353\n",
            "Running evaluation at iteration 10350...\n",
            "Iteration 10350, Train Loss: 3.401541233062744 Eval Loss: 3.5454955101013184\n",
            "Running evaluation at iteration 10360...\n",
            "Iteration 10360, Train Loss: 3.4669246673583984 Eval Loss: 3.641071152687073\n",
            "Running evaluation at iteration 10370...\n",
            "Iteration 10370, Train Loss: 4.119558811187744 Eval Loss: 3.5244005918502808\n",
            "Running evaluation at iteration 10380...\n",
            "Iteration 10380, Train Loss: 3.3416967391967773 Eval Loss: 3.5859126329421995\n",
            "Running evaluation at iteration 10390...\n",
            "Iteration 10390, Train Loss: 3.4127731323242188 Eval Loss: 3.3903624296188353\n",
            "Running evaluation at iteration 10400...\n",
            "Iteration 10400, Train Loss: 3.2068872451782227 Eval Loss: 3.708242106437683\n",
            "Running evaluation at iteration 10410...\n",
            "Iteration 10410, Train Loss: 3.7351789474487305 Eval Loss: 3.5176839590072633\n",
            "Running evaluation at iteration 10420...\n",
            "Iteration 10420, Train Loss: 3.821441411972046 Eval Loss: 3.650679111480713\n",
            "Running evaluation at iteration 10430...\n",
            "Iteration 10430, Train Loss: 3.609578847885132 Eval Loss: 3.3914849281311037\n",
            "Running evaluation at iteration 10440...\n",
            "Iteration 10440, Train Loss: 4.662449836730957 Eval Loss: 3.6858741521835325\n",
            "Running evaluation at iteration 10450...\n",
            "Iteration 10450, Train Loss: 3.3268072605133057 Eval Loss: 3.468586254119873\n",
            "Running evaluation at iteration 10460...\n",
            "Iteration 10460, Train Loss: 3.662484645843506 Eval Loss: 3.4351911306381226\n",
            "Running evaluation at iteration 10470...\n",
            "Iteration 10470, Train Loss: 3.8395330905914307 Eval Loss: 3.5775821924209597\n",
            "Running evaluation at iteration 10480...\n",
            "Iteration 10480, Train Loss: 3.9788930416107178 Eval Loss: 3.674922513961792\n",
            "Running evaluation at iteration 10490...\n",
            "Iteration 10490, Train Loss: 3.7425549030303955 Eval Loss: 3.447910284996033\n",
            "Running evaluation at iteration 10500...\n",
            "Iteration 10500, Train Loss: 3.592694044113159 Eval Loss: 3.6258382320404055\n",
            "Running evaluation at iteration 10510...\n",
            "Iteration 10510, Train Loss: 3.494168281555176 Eval Loss: 3.5797135829925537\n",
            "Running evaluation at iteration 10520...\n",
            "Iteration 10520, Train Loss: 3.48226261138916 Eval Loss: 3.336394476890564\n",
            "Running evaluation at iteration 10530...\n",
            "Iteration 10530, Train Loss: 4.166477203369141 Eval Loss: 3.636616015434265\n",
            "Running evaluation at iteration 10540...\n",
            "Iteration 10540, Train Loss: 3.3892204761505127 Eval Loss: 3.6338451147079467\n",
            "Running evaluation at iteration 10550...\n",
            "Iteration 10550, Train Loss: 3.3654487133026123 Eval Loss: 3.4830405950546264\n",
            "Running evaluation at iteration 10560...\n",
            "Iteration 10560, Train Loss: 4.209068775177002 Eval Loss: 3.4180018663406373\n",
            "Running evaluation at iteration 10570...\n",
            "Iteration 10570, Train Loss: 3.736759662628174 Eval Loss: 3.553078556060791\n",
            "Running evaluation at iteration 10580...\n",
            "Iteration 10580, Train Loss: 3.0097103118896484 Eval Loss: 3.5528731107711793\n",
            "Running evaluation at iteration 10590...\n",
            "Iteration 10590, Train Loss: 4.346818923950195 Eval Loss: 3.5915494918823243\n",
            "Running evaluation at iteration 10600...\n",
            "Iteration 10600, Train Loss: 3.923457622528076 Eval Loss: 3.479382872581482\n",
            "Running evaluation at iteration 10610...\n",
            "Iteration 10610, Train Loss: 2.9820687770843506 Eval Loss: 3.5222792625427246\n",
            "Running evaluation at iteration 10620...\n",
            "Iteration 10620, Train Loss: 3.706866979598999 Eval Loss: 3.578740859031677\n",
            "Running evaluation at iteration 10630...\n",
            "Iteration 10630, Train Loss: 3.1033248901367188 Eval Loss: 3.4040114879608154\n",
            "Running evaluation at iteration 10640...\n",
            "Iteration 10640, Train Loss: 3.560638666152954 Eval Loss: 3.5493905782699584\n",
            "Running evaluation at iteration 10650...\n",
            "Iteration 10650, Train Loss: 3.2675020694732666 Eval Loss: 3.452852964401245\n",
            "Running evaluation at iteration 10660...\n",
            "Iteration 10660, Train Loss: 3.9614951610565186 Eval Loss: 3.312092924118042\n",
            "Running evaluation at iteration 10670...\n",
            "Iteration 10670, Train Loss: 3.6428115367889404 Eval Loss: 3.5174795627593993\n",
            "Running evaluation at iteration 10680...\n",
            "Iteration 10680, Train Loss: 3.337947368621826 Eval Loss: 3.593041205406189\n",
            "Running evaluation at iteration 10690...\n",
            "Iteration 10690, Train Loss: 3.3475286960601807 Eval Loss: 3.555742311477661\n",
            "Running evaluation at iteration 10700...\n",
            "Iteration 10700, Train Loss: 3.7557220458984375 Eval Loss: 3.479369807243347\n",
            "Running evaluation at iteration 10710...\n",
            "Iteration 10710, Train Loss: 3.7276501655578613 Eval Loss: 3.3895876169204713\n",
            "Running evaluation at iteration 10720...\n",
            "Iteration 10720, Train Loss: 3.971997022628784 Eval Loss: 3.4246312618255614\n",
            "Running evaluation at iteration 10730...\n",
            "Iteration 10730, Train Loss: 3.6813275814056396 Eval Loss: 3.6221179485321047\n",
            "Running evaluation at iteration 10740...\n",
            "Iteration 10740, Train Loss: 3.6152281761169434 Eval Loss: 3.3487825870513914\n",
            "Running evaluation at iteration 10750...\n",
            "Iteration 10750, Train Loss: 4.027349948883057 Eval Loss: 3.5654414176940916\n",
            "Running evaluation at iteration 10760...\n",
            "Iteration 10760, Train Loss: 4.112302780151367 Eval Loss: 3.513684105873108\n",
            "Running evaluation at iteration 10770...\n",
            "Iteration 10770, Train Loss: 3.3591649532318115 Eval Loss: 3.605718398094177\n",
            "Running evaluation at iteration 10780...\n",
            "Iteration 10780, Train Loss: 3.1279733180999756 Eval Loss: 3.5933260917663574\n",
            "Running evaluation at iteration 10790...\n",
            "Iteration 10790, Train Loss: 4.166860103607178 Eval Loss: 3.5820608615875242\n",
            "Running evaluation at iteration 10800...\n",
            "Iteration 10800, Train Loss: 2.8642919063568115 Eval Loss: 3.646379828453064\n",
            "Running evaluation at iteration 10810...\n",
            "Iteration 10810, Train Loss: 3.415959119796753 Eval Loss: 3.218746876716614\n",
            "Running evaluation at iteration 10820...\n",
            "Iteration 10820, Train Loss: 4.07638692855835 Eval Loss: 3.388643980026245\n",
            "Running evaluation at iteration 10830...\n",
            "Iteration 10830, Train Loss: 3.6586642265319824 Eval Loss: 3.512135457992554\n",
            "Running evaluation at iteration 10840...\n",
            "Iteration 10840, Train Loss: 3.7963082790374756 Eval Loss: 3.499529266357422\n",
            "Running evaluation at iteration 10850...\n",
            "Iteration 10850, Train Loss: 3.4649016857147217 Eval Loss: 3.420837092399597\n",
            "Running evaluation at iteration 10860...\n",
            "Iteration 10860, Train Loss: 3.292517900466919 Eval Loss: 3.255982494354248\n",
            "Running evaluation at iteration 10870...\n",
            "Iteration 10870, Train Loss: 2.898751735687256 Eval Loss: 3.4948769569396974\n",
            "Running evaluation at iteration 10880...\n",
            "Iteration 10880, Train Loss: 3.3042783737182617 Eval Loss: 3.7105839014053346\n",
            "Running evaluation at iteration 10890...\n",
            "Iteration 10890, Train Loss: 3.251863718032837 Eval Loss: 3.463629055023193\n",
            "Running evaluation at iteration 10900...\n",
            "Iteration 10900, Train Loss: 4.151900768280029 Eval Loss: 3.466713809967041\n",
            "Running evaluation at iteration 10910...\n",
            "Iteration 10910, Train Loss: 3.622117280960083 Eval Loss: 3.5109667062759398\n",
            "Running evaluation at iteration 10920...\n",
            "Iteration 10920, Train Loss: 4.166233539581299 Eval Loss: 3.462617063522339\n",
            "Running evaluation at iteration 10930...\n",
            "Iteration 10930, Train Loss: 2.8668737411499023 Eval Loss: 3.4048616170883177\n",
            "Running evaluation at iteration 10940...\n",
            "Iteration 10940, Train Loss: 3.1392815113067627 Eval Loss: 3.582777905464172\n",
            "Running evaluation at iteration 10950...\n",
            "Iteration 10950, Train Loss: 3.520777702331543 Eval Loss: 3.5284049272537232\n",
            "Running evaluation at iteration 10960...\n",
            "Iteration 10960, Train Loss: 3.1716480255126953 Eval Loss: 3.609025168418884\n",
            "Running evaluation at iteration 10970...\n",
            "Iteration 10970, Train Loss: 3.72507381439209 Eval Loss: 3.4238744974136353\n",
            "Running evaluation at iteration 10980...\n",
            "Iteration 10980, Train Loss: 2.757366418838501 Eval Loss: 3.47515013217926\n",
            "Running evaluation at iteration 10990...\n",
            "Iteration 10990, Train Loss: 3.3323967456817627 Eval Loss: 3.548894929885864\n",
            "Running evaluation at iteration 11000...\n",
            "Iteration 11000, Train Loss: 3.398117780685425 Eval Loss: 3.364560866355896\n",
            "Running evaluation at iteration 11010...\n",
            "Iteration 11010, Train Loss: 3.4164011478424072 Eval Loss: 3.5080501556396486\n",
            "Running evaluation at iteration 11020...\n",
            "Iteration 11020, Train Loss: 3.6934027671813965 Eval Loss: 3.6050586462020875\n",
            "Running evaluation at iteration 11030...\n",
            "Iteration 11030, Train Loss: 3.1293675899505615 Eval Loss: 3.5658643007278443\n",
            "Running evaluation at iteration 11040...\n",
            "Iteration 11040, Train Loss: 4.06418514251709 Eval Loss: 3.5332376956939697\n",
            "Running evaluation at iteration 11050...\n",
            "Iteration 11050, Train Loss: 3.1538443565368652 Eval Loss: 3.4265056371688845\n",
            "Running evaluation at iteration 11060...\n",
            "Iteration 11060, Train Loss: 3.4962592124938965 Eval Loss: 3.6442596673965455\n",
            "Running evaluation at iteration 11070...\n",
            "Iteration 11070, Train Loss: 3.630467414855957 Eval Loss: 3.450725793838501\n",
            "Running evaluation at iteration 11080...\n",
            "Iteration 11080, Train Loss: 3.3100216388702393 Eval Loss: 3.678377866744995\n",
            "Running evaluation at iteration 11090...\n",
            "Iteration 11090, Train Loss: 4.1403350830078125 Eval Loss: 3.437076282501221\n",
            "Running evaluation at iteration 11100...\n",
            "Iteration 11100, Train Loss: 3.8647348880767822 Eval Loss: 3.4755954504013062\n",
            "Running evaluation at iteration 11110...\n",
            "Iteration 11110, Train Loss: 3.207066297531128 Eval Loss: 3.436271142959595\n",
            "Running evaluation at iteration 11120...\n",
            "Iteration 11120, Train Loss: 3.4829742908477783 Eval Loss: 3.4154984951019287\n",
            "Running evaluation at iteration 11130...\n",
            "Iteration 11130, Train Loss: 3.585685968399048 Eval Loss: 3.5041965007781983\n",
            "Running evaluation at iteration 11140...\n",
            "Iteration 11140, Train Loss: 3.802931547164917 Eval Loss: 3.6806286334991456\n",
            "Running evaluation at iteration 11150...\n",
            "Iteration 11150, Train Loss: 4.131338596343994 Eval Loss: 3.5972269773483276\n",
            "Running evaluation at iteration 11160...\n",
            "Iteration 11160, Train Loss: 3.066699504852295 Eval Loss: 3.3134668350219725\n",
            "Running evaluation at iteration 11170...\n",
            "Iteration 11170, Train Loss: 2.9883272647857666 Eval Loss: 3.5641449689865112\n",
            "Running evaluation at iteration 11180...\n",
            "Iteration 11180, Train Loss: 2.9877583980560303 Eval Loss: 3.5152323246002197\n",
            "Running evaluation at iteration 11190...\n",
            "Iteration 11190, Train Loss: 3.4644935131073 Eval Loss: 3.357140564918518\n",
            "Running evaluation at iteration 11200...\n",
            "Iteration 11200, Train Loss: 3.3304271697998047 Eval Loss: 3.400987434387207\n",
            "Running evaluation at iteration 11210...\n",
            "Iteration 11210, Train Loss: 3.217533826828003 Eval Loss: 3.482889485359192\n",
            "Running evaluation at iteration 11220...\n",
            "Iteration 11220, Train Loss: 3.9443962574005127 Eval Loss: 3.6282352685928343\n",
            "Running evaluation at iteration 11230...\n",
            "Iteration 11230, Train Loss: 3.789604902267456 Eval Loss: 3.491424012184143\n",
            "Running evaluation at iteration 11240...\n",
            "Iteration 11240, Train Loss: 3.4928719997406006 Eval Loss: 3.427936053276062\n",
            "Running evaluation at iteration 11250...\n",
            "Iteration 11250, Train Loss: 2.482248544692993 Eval Loss: 3.2997804641723634\n",
            "Running evaluation at iteration 11260...\n",
            "Iteration 11260, Train Loss: 3.9488420486450195 Eval Loss: 3.347754454612732\n",
            "Running evaluation at iteration 11270...\n",
            "Iteration 11270, Train Loss: 4.150172233581543 Eval Loss: 3.6010563373565674\n",
            "Running evaluation at iteration 11280...\n",
            "Iteration 11280, Train Loss: 3.0364327430725098 Eval Loss: 3.346604037284851\n",
            "Running evaluation at iteration 11290...\n",
            "Iteration 11290, Train Loss: 3.447896718978882 Eval Loss: 3.661661434173584\n",
            "Running evaluation at iteration 11300...\n",
            "Iteration 11300, Train Loss: 3.5536887645721436 Eval Loss: 3.4791401863098144\n",
            "Running evaluation at iteration 11310...\n",
            "Iteration 11310, Train Loss: 4.081976890563965 Eval Loss: 3.724537467956543\n",
            "Running evaluation at iteration 11320...\n",
            "Iteration 11320, Train Loss: 3.014735460281372 Eval Loss: 3.5494463443756104\n",
            "Running evaluation at iteration 11330...\n",
            "Iteration 11330, Train Loss: 3.3016104698181152 Eval Loss: 3.378249907493591\n",
            "Running evaluation at iteration 11340...\n",
            "Iteration 11340, Train Loss: 3.5006935596466064 Eval Loss: 3.4479367256164553\n",
            "Running evaluation at iteration 11350...\n",
            "Iteration 11350, Train Loss: 4.0465497970581055 Eval Loss: 3.4520988702774047\n",
            "Running evaluation at iteration 11360...\n",
            "Iteration 11360, Train Loss: 3.7835962772369385 Eval Loss: 3.3712371349334718\n",
            "Running evaluation at iteration 11370...\n",
            "Iteration 11370, Train Loss: 3.149608850479126 Eval Loss: 3.644729971885681\n",
            "Running evaluation at iteration 11380...\n",
            "Iteration 11380, Train Loss: 3.5777130126953125 Eval Loss: 3.6226203203201295\n",
            "Running evaluation at iteration 11390...\n",
            "Iteration 11390, Train Loss: 3.4988749027252197 Eval Loss: 3.587295937538147\n",
            "Running evaluation at iteration 11400...\n",
            "Iteration 11400, Train Loss: 3.9291656017303467 Eval Loss: 3.330739140510559\n",
            "Running evaluation at iteration 11410...\n",
            "Iteration 11410, Train Loss: 3.504930019378662 Eval Loss: 3.4239110469818117\n",
            "Running evaluation at iteration 11420...\n",
            "Iteration 11420, Train Loss: 3.5316216945648193 Eval Loss: 3.432814884185791\n",
            "Running evaluation at iteration 11430...\n",
            "Iteration 11430, Train Loss: 4.242053508758545 Eval Loss: 3.550025773048401\n",
            "Running evaluation at iteration 11440...\n",
            "Iteration 11440, Train Loss: 3.3819046020507812 Eval Loss: 3.520495867729187\n",
            "Running evaluation at iteration 11450...\n",
            "Iteration 11450, Train Loss: 3.6188809871673584 Eval Loss: 3.5046900272369386\n",
            "Running evaluation at iteration 11460...\n",
            "Iteration 11460, Train Loss: 3.7772631645202637 Eval Loss: 3.529776620864868\n",
            "Running evaluation at iteration 11470...\n",
            "Iteration 11470, Train Loss: 3.3848683834075928 Eval Loss: 3.4815387964248656\n",
            "Running evaluation at iteration 11480...\n",
            "Iteration 11480, Train Loss: 3.5678954124450684 Eval Loss: 3.489781212806702\n",
            "Running evaluation at iteration 11490...\n",
            "Iteration 11490, Train Loss: 3.3162906169891357 Eval Loss: 3.3138492107391357\n",
            "Running evaluation at iteration 11500...\n",
            "Iteration 11500, Train Loss: 3.307596445083618 Eval Loss: 3.5244889736175535\n",
            "Running evaluation at iteration 11510...\n",
            "Iteration 11510, Train Loss: 3.6875269412994385 Eval Loss: 3.558891248703003\n",
            "Running evaluation at iteration 11520...\n",
            "Iteration 11520, Train Loss: 3.2650110721588135 Eval Loss: 3.4287338495254516\n",
            "Running evaluation at iteration 11530...\n",
            "Iteration 11530, Train Loss: 3.453261137008667 Eval Loss: 3.4006627559661866\n",
            "Running evaluation at iteration 11540...\n",
            "Iteration 11540, Train Loss: 3.5709176063537598 Eval Loss: 3.5631884336471558\n",
            "Running evaluation at iteration 11550...\n",
            "Iteration 11550, Train Loss: 3.5405800342559814 Eval Loss: 3.6618171453475954\n",
            "Running evaluation at iteration 11560...\n",
            "Iteration 11560, Train Loss: 4.357524871826172 Eval Loss: 3.587054800987244\n",
            "Running evaluation at iteration 11570...\n",
            "Iteration 11570, Train Loss: 3.4317378997802734 Eval Loss: 3.277481508255005\n",
            "Running evaluation at iteration 11580...\n",
            "Iteration 11580, Train Loss: 3.6244354248046875 Eval Loss: 3.54008629322052\n",
            "Running evaluation at iteration 11590...\n",
            "Iteration 11590, Train Loss: 3.5724539756774902 Eval Loss: 3.31891450881958\n",
            "Running evaluation at iteration 11600...\n",
            "Iteration 11600, Train Loss: 3.599424362182617 Eval Loss: 3.4462958335876466\n",
            "Running evaluation at iteration 11610...\n",
            "Iteration 11610, Train Loss: 3.7805089950561523 Eval Loss: 3.6370503664016725\n",
            "Running evaluation at iteration 11620...\n",
            "Iteration 11620, Train Loss: 3.9089038372039795 Eval Loss: 3.47673020362854\n",
            "Running evaluation at iteration 11630...\n",
            "Iteration 11630, Train Loss: 3.576507806777954 Eval Loss: 3.5724966526031494\n",
            "Running evaluation at iteration 11640...\n",
            "Iteration 11640, Train Loss: 3.6240851879119873 Eval Loss: 3.345784878730774\n",
            "Running evaluation at iteration 11650...\n",
            "Iteration 11650, Train Loss: 4.132687568664551 Eval Loss: 3.752199816703796\n",
            "Running evaluation at iteration 11660...\n",
            "Iteration 11660, Train Loss: 3.515817165374756 Eval Loss: 3.706445741653442\n",
            "Running evaluation at iteration 11670...\n",
            "Iteration 11670, Train Loss: 3.7583000659942627 Eval Loss: 3.320697236061096\n",
            "Running evaluation at iteration 11680...\n",
            "Iteration 11680, Train Loss: 3.7275617122650146 Eval Loss: 3.465492010116577\n",
            "Running evaluation at iteration 11690...\n",
            "Iteration 11690, Train Loss: 3.127079486846924 Eval Loss: 3.3379988193511965\n",
            "Running evaluation at iteration 11700...\n",
            "Iteration 11700, Train Loss: 3.3123133182525635 Eval Loss: 3.6768502235412597\n",
            "Running evaluation at iteration 11710...\n",
            "Iteration 11710, Train Loss: 3.927584648132324 Eval Loss: 3.5898966550827027\n",
            "Running evaluation at iteration 11720...\n",
            "Iteration 11720, Train Loss: 4.160738468170166 Eval Loss: 3.810137939453125\n",
            "Running evaluation at iteration 11730...\n",
            "Iteration 11730, Train Loss: 3.877880811691284 Eval Loss: 3.5759885787963865\n",
            "Running evaluation at iteration 11740...\n",
            "Iteration 11740, Train Loss: 3.5741262435913086 Eval Loss: 3.370980215072632\n",
            "Running evaluation at iteration 11750...\n",
            "Iteration 11750, Train Loss: 3.2482590675354004 Eval Loss: 3.379982852935791\n",
            "Running evaluation at iteration 11760...\n",
            "Iteration 11760, Train Loss: 3.976497173309326 Eval Loss: 3.479482078552246\n",
            "Running evaluation at iteration 11770...\n",
            "Iteration 11770, Train Loss: 3.610527276992798 Eval Loss: 3.433263874053955\n",
            "Running evaluation at iteration 11780...\n",
            "Iteration 11780, Train Loss: 3.103355646133423 Eval Loss: 3.380999708175659\n",
            "Running evaluation at iteration 11790...\n",
            "Iteration 11790, Train Loss: 3.072465181350708 Eval Loss: 3.423981523513794\n",
            "Running evaluation at iteration 11800...\n",
            "Iteration 11800, Train Loss: 3.7835693359375 Eval Loss: 3.363536262512207\n",
            "Running evaluation at iteration 11810...\n",
            "Iteration 11810, Train Loss: 3.191044569015503 Eval Loss: 3.598594903945923\n",
            "Running evaluation at iteration 11820...\n",
            "Iteration 11820, Train Loss: 3.4825398921966553 Eval Loss: 3.55639443397522\n",
            "Running evaluation at iteration 11830...\n",
            "Iteration 11830, Train Loss: 4.25430154800415 Eval Loss: 3.50266547203064\n",
            "Running evaluation at iteration 11840...\n",
            "Iteration 11840, Train Loss: 4.468511581420898 Eval Loss: 3.377383995056152\n",
            "Running evaluation at iteration 11850...\n",
            "Iteration 11850, Train Loss: 3.6553292274475098 Eval Loss: 3.524616074562073\n",
            "Running evaluation at iteration 11860...\n",
            "Iteration 11860, Train Loss: 2.8277599811553955 Eval Loss: 3.433809185028076\n",
            "Running evaluation at iteration 11870...\n",
            "Iteration 11870, Train Loss: 3.5013046264648438 Eval Loss: 3.5731998682022095\n",
            "Running evaluation at iteration 11880...\n",
            "Iteration 11880, Train Loss: 3.3483428955078125 Eval Loss: 3.2154282331466675\n",
            "Running evaluation at iteration 11890...\n",
            "Iteration 11890, Train Loss: 3.035501718521118 Eval Loss: 3.4225975036621095\n",
            "Running evaluation at iteration 11900...\n",
            "Iteration 11900, Train Loss: 3.1185989379882812 Eval Loss: 3.4793023824691773\n",
            "Running evaluation at iteration 11910...\n",
            "Iteration 11910, Train Loss: 3.3945746421813965 Eval Loss: 3.520878791809082\n",
            "Running evaluation at iteration 11920...\n",
            "Iteration 11920, Train Loss: 3.307218551635742 Eval Loss: 3.493583822250366\n",
            "Running evaluation at iteration 11930...\n",
            "Iteration 11930, Train Loss: 4.542718410491943 Eval Loss: 3.544422912597656\n",
            "Running evaluation at iteration 11940...\n",
            "Iteration 11940, Train Loss: 2.9573731422424316 Eval Loss: 3.428581643104553\n",
            "Running evaluation at iteration 11950...\n",
            "Iteration 11950, Train Loss: 3.4757590293884277 Eval Loss: 3.3016745805740357\n",
            "Running evaluation at iteration 11960...\n",
            "Iteration 11960, Train Loss: 3.4411697387695312 Eval Loss: 3.168357181549072\n",
            "Running evaluation at iteration 11970...\n",
            "Iteration 11970, Train Loss: 3.9917566776275635 Eval Loss: 3.4582786560058594\n",
            "Running evaluation at iteration 11980...\n",
            "Iteration 11980, Train Loss: 3.5404791831970215 Eval Loss: 3.581271433830261\n",
            "Running evaluation at iteration 11990...\n",
            "Iteration 11990, Train Loss: 3.237825870513916 Eval Loss: 3.416035008430481\n",
            "Running evaluation at iteration 12000...\n",
            "Iteration 12000, Train Loss: 3.7202625274658203 Eval Loss: 3.4476242065429688\n",
            "Running evaluation at iteration 12010...\n",
            "Iteration 12010, Train Loss: 3.2276439666748047 Eval Loss: 3.4310739994049073\n",
            "Running evaluation at iteration 12020...\n",
            "Iteration 12020, Train Loss: 3.5097384452819824 Eval Loss: 3.4969570875167846\n",
            "Running evaluation at iteration 12030...\n",
            "Iteration 12030, Train Loss: 3.514468193054199 Eval Loss: 3.4179781675338745\n",
            "Running evaluation at iteration 12040...\n",
            "Iteration 12040, Train Loss: 3.7604503631591797 Eval Loss: 3.642557454109192\n",
            "Running evaluation at iteration 12050...\n",
            "Iteration 12050, Train Loss: 3.939028024673462 Eval Loss: 3.6332229375839233\n",
            "Running evaluation at iteration 12060...\n",
            "Iteration 12060, Train Loss: 3.8029465675354004 Eval Loss: 3.4656315803527833\n",
            "Running evaluation at iteration 12070...\n",
            "Iteration 12070, Train Loss: 3.5543792247772217 Eval Loss: 3.3453678369522093\n",
            "Running evaluation at iteration 12080...\n",
            "Iteration 12080, Train Loss: 4.198309421539307 Eval Loss: 3.5205697298049925\n",
            "Running evaluation at iteration 12090...\n",
            "Iteration 12090, Train Loss: 3.926942825317383 Eval Loss: 3.3656340837478638\n",
            "Running evaluation at iteration 12100...\n",
            "Iteration 12100, Train Loss: 3.8666493892669678 Eval Loss: 3.428512191772461\n",
            "Running evaluation at iteration 12110...\n",
            "Iteration 12110, Train Loss: 2.9054830074310303 Eval Loss: 3.667637276649475\n",
            "Running evaluation at iteration 12120...\n",
            "Iteration 12120, Train Loss: 3.2645418643951416 Eval Loss: 3.528736186027527\n",
            "Running evaluation at iteration 12130...\n",
            "Iteration 12130, Train Loss: 3.764570951461792 Eval Loss: 3.536288547515869\n",
            "Running evaluation at iteration 12140...\n",
            "Iteration 12140, Train Loss: 3.7667694091796875 Eval Loss: 3.5050775527954103\n",
            "Running evaluation at iteration 12150...\n",
            "Iteration 12150, Train Loss: 3.2000198364257812 Eval Loss: 3.4921315193176268\n",
            "Running evaluation at iteration 12160...\n",
            "Iteration 12160, Train Loss: 2.423849582672119 Eval Loss: 3.427344870567322\n",
            "Running evaluation at iteration 12170...\n",
            "Iteration 12170, Train Loss: 4.011019706726074 Eval Loss: 3.330558705329895\n",
            "Running evaluation at iteration 12180...\n",
            "Iteration 12180, Train Loss: 3.4798309803009033 Eval Loss: 3.6103275299072264\n",
            "Running evaluation at iteration 12190...\n",
            "Iteration 12190, Train Loss: 3.4316582679748535 Eval Loss: 3.29214289188385\n",
            "Running evaluation at iteration 12200...\n",
            "Iteration 12200, Train Loss: 3.2270302772521973 Eval Loss: 3.6658541202545165\n",
            "Running evaluation at iteration 12210...\n",
            "Iteration 12210, Train Loss: 3.8009958267211914 Eval Loss: 3.211374068260193\n",
            "Running evaluation at iteration 12220...\n",
            "Iteration 12220, Train Loss: 4.136679649353027 Eval Loss: 3.403383708000183\n",
            "Running evaluation at iteration 12230...\n",
            "Iteration 12230, Train Loss: 3.3989429473876953 Eval Loss: 3.507587933540344\n",
            "Running evaluation at iteration 12240...\n",
            "Iteration 12240, Train Loss: 3.281825065612793 Eval Loss: 3.2830129861831665\n",
            "Running evaluation at iteration 12250...\n",
            "Iteration 12250, Train Loss: 3.6692538261413574 Eval Loss: 3.4321944952011108\n",
            "Running evaluation at iteration 12260...\n",
            "Iteration 12260, Train Loss: 4.274306774139404 Eval Loss: 3.4429650783538817\n",
            "Running evaluation at iteration 12270...\n",
            "Iteration 12270, Train Loss: 3.6593613624572754 Eval Loss: 3.4502925872802734\n",
            "Running evaluation at iteration 12280...\n",
            "Iteration 12280, Train Loss: 4.199246406555176 Eval Loss: 3.527192497253418\n",
            "Running evaluation at iteration 12290...\n",
            "Iteration 12290, Train Loss: 3.9074883460998535 Eval Loss: 3.5417436838150023\n",
            "Running evaluation at iteration 12300...\n",
            "Iteration 12300, Train Loss: 3.4691121578216553 Eval Loss: 3.4912205219268797\n",
            "Running evaluation at iteration 12310...\n",
            "Iteration 12310, Train Loss: 3.698270559310913 Eval Loss: 3.3587827920913695\n",
            "Running evaluation at iteration 12320...\n",
            "Iteration 12320, Train Loss: 2.8231613636016846 Eval Loss: 3.42735276222229\n",
            "Running evaluation at iteration 12330...\n",
            "Iteration 12330, Train Loss: 3.7407426834106445 Eval Loss: 3.4368359804153443\n",
            "Running evaluation at iteration 12340...\n",
            "Iteration 12340, Train Loss: 3.584380865097046 Eval Loss: 3.2270986080169677\n",
            "Running evaluation at iteration 12350...\n",
            "Iteration 12350, Train Loss: 3.633042335510254 Eval Loss: 3.450082039833069\n",
            "Running evaluation at iteration 12360...\n",
            "Iteration 12360, Train Loss: 3.1664843559265137 Eval Loss: 3.540786790847778\n",
            "Running evaluation at iteration 12370...\n",
            "Iteration 12370, Train Loss: 3.300731897354126 Eval Loss: 3.3724384784698485\n",
            "Running evaluation at iteration 12380...\n",
            "Iteration 12380, Train Loss: 3.5979175567626953 Eval Loss: 3.416680335998535\n",
            "Running evaluation at iteration 12390...\n",
            "Iteration 12390, Train Loss: 3.519920587539673 Eval Loss: 3.6321655035018923\n",
            "Running evaluation at iteration 12400...\n",
            "Iteration 12400, Train Loss: 3.3231449127197266 Eval Loss: 3.6420960426330566\n",
            "Running evaluation at iteration 12410...\n",
            "Iteration 12410, Train Loss: 3.419783115386963 Eval Loss: 3.5158926248550415\n",
            "Running evaluation at iteration 12420...\n",
            "Iteration 12420, Train Loss: 3.2905588150024414 Eval Loss: 3.480956721305847\n",
            "Running evaluation at iteration 12430...\n",
            "Iteration 12430, Train Loss: 3.231904983520508 Eval Loss: 3.5973484039306642\n",
            "Running evaluation at iteration 12440...\n",
            "Iteration 12440, Train Loss: 3.286543607711792 Eval Loss: 3.4726763010025024\n",
            "Running evaluation at iteration 12450...\n",
            "Iteration 12450, Train Loss: 3.3432445526123047 Eval Loss: 3.563935708999634\n",
            "Running evaluation at iteration 12460...\n",
            "Iteration 12460, Train Loss: 3.308922052383423 Eval Loss: 3.432111144065857\n",
            "Running evaluation at iteration 12470...\n",
            "Iteration 12470, Train Loss: 3.013596534729004 Eval Loss: 3.5156610012054443\n",
            "Running evaluation at iteration 12480...\n",
            "Iteration 12480, Train Loss: 3.474430799484253 Eval Loss: 3.2898648738861085\n",
            "Running evaluation at iteration 12490...\n",
            "Iteration 12490, Train Loss: 3.0982372760772705 Eval Loss: 3.5192275524139403\n",
            "Running evaluation at iteration 12500...\n",
            "Iteration 12500, Train Loss: 3.647993803024292 Eval Loss: 3.5198150396347048\n",
            "Running evaluation at iteration 12510...\n",
            "Iteration 12510, Train Loss: 3.649773359298706 Eval Loss: 3.3693179368972777\n",
            "Running evaluation at iteration 12520...\n",
            "Iteration 12520, Train Loss: 4.036285400390625 Eval Loss: 3.4664501190185546\n",
            "Running evaluation at iteration 12530...\n",
            "Iteration 12530, Train Loss: 3.38193941116333 Eval Loss: 3.414252519607544\n",
            "Running evaluation at iteration 12540...\n",
            "Iteration 12540, Train Loss: 4.065396308898926 Eval Loss: 3.6418610334396364\n",
            "Running evaluation at iteration 12550...\n",
            "Iteration 12550, Train Loss: 3.7070472240448 Eval Loss: 3.4544025659561157\n",
            "Running evaluation at iteration 12560...\n",
            "Iteration 12560, Train Loss: 3.3303027153015137 Eval Loss: 3.5195398330688477\n",
            "Running evaluation at iteration 12570...\n",
            "Iteration 12570, Train Loss: 3.6700382232666016 Eval Loss: 3.5124075651168822\n",
            "Running evaluation at iteration 12580...\n",
            "Iteration 12580, Train Loss: 3.399263381958008 Eval Loss: 3.4367282152175904\n",
            "Running evaluation at iteration 12590...\n",
            "Iteration 12590, Train Loss: 3.0013163089752197 Eval Loss: 3.205117106437683\n",
            "Running evaluation at iteration 12600...\n",
            "Iteration 12600, Train Loss: 3.3798153400421143 Eval Loss: 3.4419476985931396\n",
            "Running evaluation at iteration 12610...\n",
            "Iteration 12610, Train Loss: 4.181591033935547 Eval Loss: 3.535089540481567\n",
            "Running evaluation at iteration 12620...\n",
            "Iteration 12620, Train Loss: 3.4157509803771973 Eval Loss: 3.556253123283386\n",
            "Running evaluation at iteration 12630...\n",
            "Iteration 12630, Train Loss: 2.955267906188965 Eval Loss: 3.17322838306427\n",
            "Running evaluation at iteration 12640...\n",
            "Iteration 12640, Train Loss: 3.38779878616333 Eval Loss: 3.4780437469482424\n",
            "Running evaluation at iteration 12650...\n",
            "Iteration 12650, Train Loss: 4.179328441619873 Eval Loss: 3.394024443626404\n",
            "Running evaluation at iteration 12660...\n",
            "Iteration 12660, Train Loss: 4.057156085968018 Eval Loss: 3.6232944250106813\n",
            "Running evaluation at iteration 12670...\n",
            "Iteration 12670, Train Loss: 3.6229565143585205 Eval Loss: 3.263623356819153\n",
            "Running evaluation at iteration 12680...\n",
            "Iteration 12680, Train Loss: 4.1121320724487305 Eval Loss: 3.3962600231170654\n",
            "Running evaluation at iteration 12690...\n",
            "Iteration 12690, Train Loss: 4.092934608459473 Eval Loss: 3.44028377532959\n",
            "Running evaluation at iteration 12700...\n",
            "Iteration 12700, Train Loss: 3.8216848373413086 Eval Loss: 3.2862576246261597\n",
            "Running evaluation at iteration 12710...\n",
            "Iteration 12710, Train Loss: 3.698850154876709 Eval Loss: 3.2992568969726563\n",
            "Running evaluation at iteration 12720...\n",
            "Iteration 12720, Train Loss: 3.3195879459381104 Eval Loss: 3.3306719779968263\n",
            "Running evaluation at iteration 12730...\n",
            "Iteration 12730, Train Loss: 3.778674364089966 Eval Loss: 3.3319505214691163\n",
            "Running evaluation at iteration 12740...\n",
            "Iteration 12740, Train Loss: 3.557293653488159 Eval Loss: 3.4506855249404906\n",
            "Running evaluation at iteration 12750...\n",
            "Iteration 12750, Train Loss: 3.466841459274292 Eval Loss: 3.3756387710571287\n",
            "Running evaluation at iteration 12760...\n",
            "Iteration 12760, Train Loss: 3.089460849761963 Eval Loss: 3.4571316957473757\n",
            "Running evaluation at iteration 12770...\n",
            "Iteration 12770, Train Loss: 3.9384241104125977 Eval Loss: 3.5558504819869996\n",
            "Running evaluation at iteration 12780...\n",
            "Iteration 12780, Train Loss: 3.266725778579712 Eval Loss: 3.285433626174927\n",
            "Running evaluation at iteration 12790...\n",
            "Iteration 12790, Train Loss: 3.9557461738586426 Eval Loss: 3.447612833976746\n",
            "Running evaluation at iteration 12800...\n",
            "Iteration 12800, Train Loss: 3.2546944618225098 Eval Loss: 3.4835246562957765\n",
            "Running evaluation at iteration 12810...\n",
            "Iteration 12810, Train Loss: 3.319967746734619 Eval Loss: 3.5312541723251343\n",
            "Running evaluation at iteration 12820...\n",
            "Iteration 12820, Train Loss: 3.541956901550293 Eval Loss: 3.510541868209839\n",
            "Running evaluation at iteration 12830...\n",
            "Iteration 12830, Train Loss: 3.2852261066436768 Eval Loss: 3.418605852127075\n",
            "Running evaluation at iteration 12840...\n",
            "Iteration 12840, Train Loss: 3.3743648529052734 Eval Loss: 3.3140331745147704\n",
            "Running evaluation at iteration 12850...\n",
            "Iteration 12850, Train Loss: 3.561596632003784 Eval Loss: 3.5325074195861816\n",
            "Running evaluation at iteration 12860...\n",
            "Iteration 12860, Train Loss: 3.587308883666992 Eval Loss: 3.7479034423828126\n",
            "Running evaluation at iteration 12870...\n",
            "Iteration 12870, Train Loss: 3.669534206390381 Eval Loss: 3.4583829641342163\n",
            "Running evaluation at iteration 12880...\n",
            "Iteration 12880, Train Loss: 3.691920757293701 Eval Loss: 3.3607593774795532\n",
            "Running evaluation at iteration 12890...\n",
            "Iteration 12890, Train Loss: 3.824718952178955 Eval Loss: 3.6474629163742067\n",
            "Running evaluation at iteration 12900...\n",
            "Iteration 12900, Train Loss: 4.010088920593262 Eval Loss: 3.5436674833297728\n",
            "Running evaluation at iteration 12910...\n",
            "Iteration 12910, Train Loss: 3.5296053886413574 Eval Loss: 3.5422590255737303\n",
            "Running evaluation at iteration 12920...\n",
            "Iteration 12920, Train Loss: 3.63322114944458 Eval Loss: 3.6422128677368164\n",
            "Running evaluation at iteration 12930...\n",
            "Iteration 12930, Train Loss: 3.5513782501220703 Eval Loss: 3.4632481813430784\n",
            "Running evaluation at iteration 12940...\n",
            "Iteration 12940, Train Loss: 3.0155889987945557 Eval Loss: 3.4413338899612427\n",
            "Running evaluation at iteration 12950...\n",
            "Iteration 12950, Train Loss: 4.442905902862549 Eval Loss: 3.484224271774292\n",
            "Running evaluation at iteration 12960...\n",
            "Iteration 12960, Train Loss: 3.1598012447357178 Eval Loss: 3.1798285484313964\n",
            "Running evaluation at iteration 12970...\n",
            "Iteration 12970, Train Loss: 4.317737579345703 Eval Loss: 3.7035568714141847\n",
            "Running evaluation at iteration 12980...\n",
            "Iteration 12980, Train Loss: 3.1379404067993164 Eval Loss: 3.6049968719482424\n",
            "Running evaluation at iteration 12990...\n",
            "Iteration 12990, Train Loss: 3.4366161823272705 Eval Loss: 3.5251025438308714\n",
            "Running evaluation at iteration 13000...\n",
            "Iteration 13000, Train Loss: 3.262902021408081 Eval Loss: 3.394465208053589\n",
            "Running evaluation at iteration 13010...\n",
            "Iteration 13010, Train Loss: 3.5248258113861084 Eval Loss: 3.5816163063049316\n",
            "Running evaluation at iteration 13020...\n",
            "Iteration 13020, Train Loss: 3.164191484451294 Eval Loss: 3.4622215032577515\n",
            "Running evaluation at iteration 13030...\n",
            "Iteration 13030, Train Loss: 3.996570348739624 Eval Loss: 3.6595008850097654\n",
            "Running evaluation at iteration 13040...\n",
            "Iteration 13040, Train Loss: 3.1933257579803467 Eval Loss: 3.4574438095092774\n",
            "Running evaluation at iteration 13050...\n",
            "Iteration 13050, Train Loss: 3.296079158782959 Eval Loss: 3.5255868196487428\n",
            "Running evaluation at iteration 13060...\n",
            "Iteration 13060, Train Loss: 3.0386340618133545 Eval Loss: 3.3635401010513304\n",
            "Running evaluation at iteration 13070...\n",
            "Iteration 13070, Train Loss: 3.421444892883301 Eval Loss: 3.352244162559509\n",
            "Running evaluation at iteration 13080...\n",
            "Iteration 13080, Train Loss: 3.1053638458251953 Eval Loss: 3.4813018798828126\n",
            "Running evaluation at iteration 13090...\n",
            "Iteration 13090, Train Loss: 3.868044853210449 Eval Loss: 3.409189295768738\n",
            "Running evaluation at iteration 13100...\n",
            "Iteration 13100, Train Loss: 3.797659397125244 Eval Loss: 3.5986343145370485\n",
            "Running evaluation at iteration 13110...\n",
            "Iteration 13110, Train Loss: 3.640742778778076 Eval Loss: 3.2752713918685914\n",
            "Running evaluation at iteration 13120...\n",
            "Iteration 13120, Train Loss: 3.750978469848633 Eval Loss: 3.6770251035690307\n",
            "Running evaluation at iteration 13130...\n",
            "Iteration 13130, Train Loss: 3.976639747619629 Eval Loss: 3.5944154262542725\n",
            "Running evaluation at iteration 13140...\n",
            "Iteration 13140, Train Loss: 3.8115172386169434 Eval Loss: 3.467087507247925\n",
            "Running evaluation at iteration 13150...\n",
            "Iteration 13150, Train Loss: 3.520548105239868 Eval Loss: 3.503205156326294\n",
            "Running evaluation at iteration 13160...\n",
            "Iteration 13160, Train Loss: 3.4580931663513184 Eval Loss: 3.4635302305221556\n",
            "Running evaluation at iteration 13170...\n",
            "Iteration 13170, Train Loss: 3.268007755279541 Eval Loss: 3.266235041618347\n",
            "Running evaluation at iteration 13180...\n",
            "Iteration 13180, Train Loss: 3.781980037689209 Eval Loss: 3.613935351371765\n",
            "Running evaluation at iteration 13190...\n",
            "Iteration 13190, Train Loss: 3.100280523300171 Eval Loss: 3.3022220373153686\n",
            "Running evaluation at iteration 13200...\n",
            "Iteration 13200, Train Loss: 3.828037977218628 Eval Loss: 3.351845359802246\n",
            "Running evaluation at iteration 13210...\n",
            "Iteration 13210, Train Loss: 3.525109052658081 Eval Loss: 3.4583856105804442\n",
            "Running evaluation at iteration 13220...\n",
            "Iteration 13220, Train Loss: 3.551853656768799 Eval Loss: 3.584800148010254\n",
            "Running evaluation at iteration 13230...\n",
            "Iteration 13230, Train Loss: 2.5821945667266846 Eval Loss: 3.2661648273468016\n",
            "Running evaluation at iteration 13240...\n",
            "Iteration 13240, Train Loss: 3.3429791927337646 Eval Loss: 3.5989177942276003\n",
            "Running evaluation at iteration 13250...\n",
            "Iteration 13250, Train Loss: 3.4302279949188232 Eval Loss: 3.412436866760254\n",
            "Running evaluation at iteration 13260...\n",
            "Iteration 13260, Train Loss: 3.842339515686035 Eval Loss: 3.363697361946106\n",
            "Running evaluation at iteration 13270...\n",
            "Iteration 13270, Train Loss: 4.235852241516113 Eval Loss: 3.495869755744934\n",
            "Running evaluation at iteration 13280...\n",
            "Iteration 13280, Train Loss: 3.7518224716186523 Eval Loss: 3.415993809700012\n",
            "Running evaluation at iteration 13290...\n",
            "Iteration 13290, Train Loss: 3.352550745010376 Eval Loss: 3.4173484802246095\n",
            "Running evaluation at iteration 13300...\n",
            "Iteration 13300, Train Loss: 3.546525001525879 Eval Loss: 3.2865514278411867\n",
            "Running evaluation at iteration 13310...\n",
            "Iteration 13310, Train Loss: 3.467064619064331 Eval Loss: 3.225693130493164\n",
            "Running evaluation at iteration 13320...\n",
            "Iteration 13320, Train Loss: 3.250201940536499 Eval Loss: 3.447464871406555\n",
            "Running evaluation at iteration 13330...\n",
            "Iteration 13330, Train Loss: 3.814669609069824 Eval Loss: 3.4182199478149413\n",
            "Running evaluation at iteration 13340...\n",
            "Iteration 13340, Train Loss: 4.056527614593506 Eval Loss: 3.444084620475769\n",
            "Running evaluation at iteration 13350...\n",
            "Iteration 13350, Train Loss: 2.657651662826538 Eval Loss: 3.62598032951355\n",
            "Running evaluation at iteration 13360...\n",
            "Iteration 13360, Train Loss: 3.5772716999053955 Eval Loss: 3.43570876121521\n",
            "Running evaluation at iteration 13370...\n",
            "Iteration 13370, Train Loss: 3.859673500061035 Eval Loss: 3.435564112663269\n",
            "Running evaluation at iteration 13380...\n",
            "Iteration 13380, Train Loss: 2.8938422203063965 Eval Loss: 3.248632478713989\n",
            "Running evaluation at iteration 13390...\n",
            "Iteration 13390, Train Loss: 3.3902578353881836 Eval Loss: 3.6438714027404786\n",
            "Running evaluation at iteration 13400...\n",
            "Iteration 13400, Train Loss: 3.9446887969970703 Eval Loss: 3.2979788780212402\n",
            "Running evaluation at iteration 13410...\n",
            "Iteration 13410, Train Loss: 3.496967077255249 Eval Loss: 3.4370798587799074\n",
            "Running evaluation at iteration 13420...\n",
            "Iteration 13420, Train Loss: 2.7473065853118896 Eval Loss: 3.3465021371841432\n",
            "Running evaluation at iteration 13430...\n",
            "Iteration 13430, Train Loss: 3.450378656387329 Eval Loss: 3.4416194677352907\n",
            "Running evaluation at iteration 13440...\n",
            "Iteration 13440, Train Loss: 3.795910596847534 Eval Loss: 3.455473804473877\n",
            "Running evaluation at iteration 13450...\n",
            "Iteration 13450, Train Loss: 3.4560630321502686 Eval Loss: 3.3459190130233765\n",
            "Running evaluation at iteration 13460...\n",
            "Iteration 13460, Train Loss: 3.1218254566192627 Eval Loss: 3.6274094581604004\n",
            "Running evaluation at iteration 13470...\n",
            "Iteration 13470, Train Loss: 4.305514812469482 Eval Loss: 3.3370004415512087\n",
            "Running evaluation at iteration 13480...\n",
            "Iteration 13480, Train Loss: 3.4796762466430664 Eval Loss: 3.3344677686691284\n",
            "Running evaluation at iteration 13490...\n",
            "Iteration 13490, Train Loss: 3.2183303833007812 Eval Loss: 3.347129988670349\n",
            "Running evaluation at iteration 13500...\n",
            "Iteration 13500, Train Loss: 3.6925418376922607 Eval Loss: 3.535481405258179\n",
            "Running evaluation at iteration 13510...\n",
            "Iteration 13510, Train Loss: 3.2958879470825195 Eval Loss: 3.327279496192932\n",
            "Running evaluation at iteration 13520...\n",
            "Iteration 13520, Train Loss: 3.9638311862945557 Eval Loss: 3.3690181255340574\n",
            "Running evaluation at iteration 13530...\n",
            "Iteration 13530, Train Loss: 3.364302396774292 Eval Loss: 3.2186598062515257\n",
            "Running evaluation at iteration 13540...\n",
            "Iteration 13540, Train Loss: 2.8293609619140625 Eval Loss: 3.4594082832336426\n",
            "Running evaluation at iteration 13550...\n",
            "Iteration 13550, Train Loss: 4.078105449676514 Eval Loss: 3.2927480936050415\n",
            "Running evaluation at iteration 13560...\n",
            "Iteration 13560, Train Loss: 3.5645720958709717 Eval Loss: 3.6452820539474486\n",
            "Running evaluation at iteration 13570...\n",
            "Iteration 13570, Train Loss: 3.9645092487335205 Eval Loss: 3.526337504386902\n",
            "Running evaluation at iteration 13580...\n",
            "Iteration 13580, Train Loss: 4.2409586906433105 Eval Loss: 3.400406074523926\n",
            "Running evaluation at iteration 13590...\n",
            "Iteration 13590, Train Loss: 3.646561861038208 Eval Loss: 3.514484930038452\n",
            "Running evaluation at iteration 13600...\n",
            "Iteration 13600, Train Loss: 3.3086118698120117 Eval Loss: 3.4272261381149294\n",
            "Running evaluation at iteration 13610...\n",
            "Iteration 13610, Train Loss: 3.4361441135406494 Eval Loss: 3.603846001625061\n",
            "Running evaluation at iteration 13620...\n",
            "Iteration 13620, Train Loss: 3.451040029525757 Eval Loss: 3.541054105758667\n",
            "Running evaluation at iteration 13630...\n",
            "Iteration 13630, Train Loss: 3.6497931480407715 Eval Loss: 3.5208627223968505\n",
            "Running evaluation at iteration 13640...\n",
            "Iteration 13640, Train Loss: 4.331457614898682 Eval Loss: 3.3353786706924438\n",
            "Running evaluation at iteration 13650...\n",
            "Iteration 13650, Train Loss: 3.9047157764434814 Eval Loss: 3.3640140771865843\n",
            "Running evaluation at iteration 13660...\n",
            "Iteration 13660, Train Loss: 3.245450735092163 Eval Loss: 3.724063205718994\n",
            "Running evaluation at iteration 13670...\n",
            "Iteration 13670, Train Loss: 2.9838757514953613 Eval Loss: 3.407764744758606\n",
            "Running evaluation at iteration 13680...\n",
            "Iteration 13680, Train Loss: 3.6113386154174805 Eval Loss: 3.5454702854156492\n",
            "Running evaluation at iteration 13690...\n",
            "Iteration 13690, Train Loss: 3.588712215423584 Eval Loss: 3.3659525156021117\n",
            "Running evaluation at iteration 13700...\n",
            "Iteration 13700, Train Loss: 2.4854211807250977 Eval Loss: 3.4085186958312987\n",
            "Running evaluation at iteration 13710...\n",
            "Iteration 13710, Train Loss: 3.9203927516937256 Eval Loss: 3.5430607080459593\n",
            "Running evaluation at iteration 13720...\n",
            "Iteration 13720, Train Loss: 3.093106269836426 Eval Loss: 3.4777293682098387\n",
            "Running evaluation at iteration 13730...\n",
            "Iteration 13730, Train Loss: 3.327566146850586 Eval Loss: 3.524724912643433\n",
            "Running evaluation at iteration 13740...\n",
            "Iteration 13740, Train Loss: 3.1602911949157715 Eval Loss: 3.511491537094116\n",
            "Running evaluation at iteration 13750...\n",
            "Iteration 13750, Train Loss: 3.8527867794036865 Eval Loss: 3.27282235622406\n",
            "Running evaluation at iteration 13760...\n",
            "Iteration 13760, Train Loss: 3.3500311374664307 Eval Loss: 3.4111732959747316\n",
            "Running evaluation at iteration 13770...\n",
            "Iteration 13770, Train Loss: 3.295964002609253 Eval Loss: 3.4552425861358644\n",
            "Running evaluation at iteration 13780...\n",
            "Iteration 13780, Train Loss: 3.6067755222320557 Eval Loss: 3.4533249616622923\n",
            "Running evaluation at iteration 13790...\n",
            "Iteration 13790, Train Loss: 4.522393703460693 Eval Loss: 3.317715382575989\n",
            "Running evaluation at iteration 13800...\n",
            "Iteration 13800, Train Loss: 3.2172470092773438 Eval Loss: 3.3541237115859985\n",
            "Running evaluation at iteration 13810...\n",
            "Iteration 13810, Train Loss: 3.548232078552246 Eval Loss: 3.3938321828842164\n",
            "Running evaluation at iteration 13820...\n",
            "Iteration 13820, Train Loss: 3.1157596111297607 Eval Loss: 3.3629205942153932\n",
            "Running evaluation at iteration 13830...\n",
            "Iteration 13830, Train Loss: 3.718684673309326 Eval Loss: 3.5893244981765746\n",
            "Running evaluation at iteration 13840...\n",
            "Iteration 13840, Train Loss: 3.838238000869751 Eval Loss: 3.5395564317703245\n",
            "Running evaluation at iteration 13850...\n",
            "Iteration 13850, Train Loss: 3.16810941696167 Eval Loss: 3.3125630140304567\n",
            "Running evaluation at iteration 13860...\n",
            "Iteration 13860, Train Loss: 3.2731707096099854 Eval Loss: 3.5961339712142943\n",
            "Running evaluation at iteration 13870...\n",
            "Iteration 13870, Train Loss: 3.6816768646240234 Eval Loss: 3.560835361480713\n",
            "Running evaluation at iteration 13880...\n",
            "Iteration 13880, Train Loss: 3.4780783653259277 Eval Loss: 3.2991814374923707\n",
            "Running evaluation at iteration 13890...\n",
            "Iteration 13890, Train Loss: 3.8233020305633545 Eval Loss: 3.4441503286361694\n",
            "Running evaluation at iteration 13900...\n",
            "Iteration 13900, Train Loss: 3.0858805179595947 Eval Loss: 3.561349654197693\n",
            "Running evaluation at iteration 13910...\n",
            "Iteration 13910, Train Loss: 3.8299460411071777 Eval Loss: 3.3274701833724976\n",
            "Running evaluation at iteration 13920...\n",
            "Iteration 13920, Train Loss: 3.558530330657959 Eval Loss: 3.3513887882232667\n",
            "Running evaluation at iteration 13930...\n",
            "Iteration 13930, Train Loss: 3.628652334213257 Eval Loss: 3.4540902614593505\n",
            "Running evaluation at iteration 13940...\n",
            "Iteration 13940, Train Loss: 3.3527069091796875 Eval Loss: 3.5659942626953125\n",
            "Running evaluation at iteration 13950...\n",
            "Iteration 13950, Train Loss: 3.540095329284668 Eval Loss: 3.435124969482422\n",
            "Running evaluation at iteration 13960...\n",
            "Iteration 13960, Train Loss: 3.4538450241088867 Eval Loss: 3.4167030334472654\n",
            "Running evaluation at iteration 13970...\n",
            "Iteration 13970, Train Loss: 3.203000545501709 Eval Loss: 3.7029392957687377\n",
            "Running evaluation at iteration 13980...\n",
            "Iteration 13980, Train Loss: 3.641526699066162 Eval Loss: 3.3290118932724\n",
            "Running evaluation at iteration 13990...\n",
            "Iteration 13990, Train Loss: 3.3932442665100098 Eval Loss: 3.597743058204651\n",
            "Running evaluation at iteration 14000...\n",
            "Iteration 14000, Train Loss: 4.070235729217529 Eval Loss: 3.4840056896209717\n",
            "Running evaluation at iteration 14010...\n",
            "Iteration 14010, Train Loss: 3.507220983505249 Eval Loss: 3.3615018129348755\n",
            "Running evaluation at iteration 14020...\n",
            "Iteration 14020, Train Loss: 3.2027108669281006 Eval Loss: 3.303968405723572\n",
            "Running evaluation at iteration 14030...\n",
            "Iteration 14030, Train Loss: 3.766470432281494 Eval Loss: 3.4090686321258543\n",
            "Running evaluation at iteration 14040...\n",
            "Iteration 14040, Train Loss: 3.875032663345337 Eval Loss: 3.2748475790023805\n",
            "Running evaluation at iteration 14050...\n",
            "Iteration 14050, Train Loss: 3.4142398834228516 Eval Loss: 3.38674635887146\n",
            "Running evaluation at iteration 14060...\n",
            "Iteration 14060, Train Loss: 3.943113327026367 Eval Loss: 3.2846296548843386\n",
            "Running evaluation at iteration 14070...\n",
            "Iteration 14070, Train Loss: 4.06436824798584 Eval Loss: 3.4583622455596923\n",
            "Running evaluation at iteration 14080...\n",
            "Iteration 14080, Train Loss: 3.584820032119751 Eval Loss: 3.38546679019928\n",
            "Running evaluation at iteration 14090...\n",
            "Iteration 14090, Train Loss: 3.3602240085601807 Eval Loss: 3.340991711616516\n",
            "Running evaluation at iteration 14100...\n",
            "Iteration 14100, Train Loss: 3.5965161323547363 Eval Loss: 3.4265487432479858\n",
            "Running evaluation at iteration 14110...\n",
            "Iteration 14110, Train Loss: 4.203701019287109 Eval Loss: 3.462450385093689\n",
            "Running evaluation at iteration 14120...\n",
            "Iteration 14120, Train Loss: 3.249069929122925 Eval Loss: 3.3747880697250365\n",
            "Running evaluation at iteration 14130...\n",
            "Iteration 14130, Train Loss: 3.805525541305542 Eval Loss: 3.4704308032989504\n",
            "Running evaluation at iteration 14140...\n",
            "Iteration 14140, Train Loss: 3.2766823768615723 Eval Loss: 3.5766605138778687\n",
            "Running evaluation at iteration 14150...\n",
            "Iteration 14150, Train Loss: 4.200709342956543 Eval Loss: 3.2889193058013917\n",
            "Running evaluation at iteration 14160...\n",
            "Iteration 14160, Train Loss: 2.9596779346466064 Eval Loss: 3.433005452156067\n",
            "Running evaluation at iteration 14170...\n",
            "Iteration 14170, Train Loss: 4.0135087966918945 Eval Loss: 3.6456863641738892\n",
            "Running evaluation at iteration 14180...\n",
            "Iteration 14180, Train Loss: 4.107817649841309 Eval Loss: 3.418493461608887\n",
            "Running evaluation at iteration 14190...\n",
            "Iteration 14190, Train Loss: 3.580763816833496 Eval Loss: 3.2230775356292725\n",
            "Running evaluation at iteration 14200...\n",
            "Iteration 14200, Train Loss: 2.7769439220428467 Eval Loss: 3.302415132522583\n",
            "Running evaluation at iteration 14210...\n",
            "Iteration 14210, Train Loss: 3.8395256996154785 Eval Loss: 3.339237833023071\n",
            "Running evaluation at iteration 14220...\n",
            "Iteration 14220, Train Loss: 3.518744707107544 Eval Loss: 3.3619040727615355\n",
            "Running evaluation at iteration 14230...\n",
            "Iteration 14230, Train Loss: 3.61250638961792 Eval Loss: 3.5792238473892213\n",
            "Running evaluation at iteration 14240...\n",
            "Iteration 14240, Train Loss: 3.4155240058898926 Eval Loss: 3.3010605812072753\n",
            "Running evaluation at iteration 14250...\n",
            "Iteration 14250, Train Loss: 3.402291774749756 Eval Loss: 3.30590443611145\n",
            "Running evaluation at iteration 14260...\n",
            "Iteration 14260, Train Loss: 3.279898166656494 Eval Loss: 3.3674545526504516\n",
            "Running evaluation at iteration 14270...\n",
            "Iteration 14270, Train Loss: 3.407668352127075 Eval Loss: 3.5001997709274293\n",
            "Running evaluation at iteration 14280...\n",
            "Iteration 14280, Train Loss: 3.4616622924804688 Eval Loss: 3.4853924751281737\n",
            "Running evaluation at iteration 14290...\n",
            "Iteration 14290, Train Loss: 3.9364562034606934 Eval Loss: 3.2172662734985353\n",
            "Running evaluation at iteration 14300...\n",
            "Iteration 14300, Train Loss: 3.693716526031494 Eval Loss: 3.2250252962112427\n",
            "Running evaluation at iteration 14310...\n",
            "Iteration 14310, Train Loss: 3.0161237716674805 Eval Loss: 3.6520161390304566\n",
            "Running evaluation at iteration 14320...\n",
            "Iteration 14320, Train Loss: 3.762082815170288 Eval Loss: 3.331251859664917\n",
            "Running evaluation at iteration 14330...\n",
            "Iteration 14330, Train Loss: 3.5436642169952393 Eval Loss: 3.498225784301758\n",
            "Running evaluation at iteration 14340...\n",
            "Iteration 14340, Train Loss: 3.6127769947052 Eval Loss: 3.412898802757263\n",
            "Running evaluation at iteration 14350...\n",
            "Iteration 14350, Train Loss: 3.146657943725586 Eval Loss: 3.45906457901001\n",
            "Running evaluation at iteration 14360...\n",
            "Iteration 14360, Train Loss: 3.880239486694336 Eval Loss: 3.5197617053985595\n",
            "Running evaluation at iteration 14370...\n",
            "Iteration 14370, Train Loss: 4.046876430511475 Eval Loss: 3.3708356618881226\n",
            "Running evaluation at iteration 14380...\n",
            "Iteration 14380, Train Loss: 3.5015618801116943 Eval Loss: 3.431522011756897\n",
            "Running evaluation at iteration 14390...\n",
            "Iteration 14390, Train Loss: 3.8158369064331055 Eval Loss: 3.616572046279907\n",
            "Running evaluation at iteration 14400...\n",
            "Iteration 14400, Train Loss: 3.9757344722747803 Eval Loss: 3.440652346611023\n",
            "Running evaluation at iteration 14410...\n",
            "Iteration 14410, Train Loss: 3.321892499923706 Eval Loss: 3.417549419403076\n",
            "Running evaluation at iteration 14420...\n",
            "Iteration 14420, Train Loss: 3.392275810241699 Eval Loss: 3.2227728605270385\n",
            "Running evaluation at iteration 14430...\n",
            "Iteration 14430, Train Loss: 3.273336410522461 Eval Loss: 3.2675952196121214\n",
            "Running evaluation at iteration 14440...\n",
            "Iteration 14440, Train Loss: 3.782958984375 Eval Loss: 3.2628764867782594\n",
            "Running evaluation at iteration 14450...\n",
            "Iteration 14450, Train Loss: 3.362056016921997 Eval Loss: 3.471855306625366\n",
            "Running evaluation at iteration 14460...\n",
            "Iteration 14460, Train Loss: 3.68044376373291 Eval Loss: 3.561485528945923\n",
            "Running evaluation at iteration 14470...\n",
            "Iteration 14470, Train Loss: 3.3078551292419434 Eval Loss: 3.5292961835861205\n",
            "Running evaluation at iteration 14480...\n",
            "Iteration 14480, Train Loss: 3.2892959117889404 Eval Loss: 3.5115134716033936\n",
            "Running evaluation at iteration 14490...\n",
            "Iteration 14490, Train Loss: 3.3426969051361084 Eval Loss: 3.2948116302490233\n",
            "Running evaluation at iteration 14500...\n",
            "Iteration 14500, Train Loss: 2.839071273803711 Eval Loss: 3.3643349409103394\n",
            "Running evaluation at iteration 14510...\n",
            "Iteration 14510, Train Loss: 3.2499148845672607 Eval Loss: 3.2711336851119994\n",
            "Running evaluation at iteration 14520...\n",
            "Iteration 14520, Train Loss: 3.677673578262329 Eval Loss: 3.4744898080825806\n",
            "Running evaluation at iteration 14530...\n",
            "Iteration 14530, Train Loss: 3.9994397163391113 Eval Loss: 3.404254698753357\n",
            "Running evaluation at iteration 14540...\n",
            "Iteration 14540, Train Loss: 3.4701733589172363 Eval Loss: 3.4988332986831665\n",
            "Running evaluation at iteration 14550...\n",
            "Iteration 14550, Train Loss: 3.3539841175079346 Eval Loss: 3.4184277057647705\n",
            "Running evaluation at iteration 14560...\n",
            "Iteration 14560, Train Loss: 3.0466156005859375 Eval Loss: 3.3280133247375487\n",
            "Running evaluation at iteration 14570...\n",
            "Iteration 14570, Train Loss: 3.760695457458496 Eval Loss: 3.375339961051941\n",
            "Running evaluation at iteration 14580...\n",
            "Iteration 14580, Train Loss: 3.4259610176086426 Eval Loss: 3.5297133922576904\n",
            "Running evaluation at iteration 14590...\n",
            "Iteration 14590, Train Loss: 3.7390265464782715 Eval Loss: 3.3079959392547607\n",
            "Running evaluation at iteration 14600...\n",
            "Iteration 14600, Train Loss: 3.6339964866638184 Eval Loss: 3.579377031326294\n",
            "Running evaluation at iteration 14610...\n",
            "Iteration 14610, Train Loss: 3.3976898193359375 Eval Loss: 3.587549591064453\n",
            "Running evaluation at iteration 14620...\n",
            "Iteration 14620, Train Loss: 3.660184621810913 Eval Loss: 3.361380434036255\n",
            "Running evaluation at iteration 14630...\n",
            "Iteration 14630, Train Loss: 2.9655532836914062 Eval Loss: 3.323371076583862\n",
            "Running evaluation at iteration 14640...\n",
            "Iteration 14640, Train Loss: 2.4956793785095215 Eval Loss: 3.5718137979507447\n",
            "Running evaluation at iteration 14650...\n",
            "Iteration 14650, Train Loss: 3.2322678565979004 Eval Loss: 3.3704265117645265\n",
            "Running evaluation at iteration 14660...\n",
            "Iteration 14660, Train Loss: 3.579441547393799 Eval Loss: 3.2045969724655152\n",
            "Running evaluation at iteration 14670...\n",
            "Iteration 14670, Train Loss: 3.6890616416931152 Eval Loss: 3.3768007516860963\n",
            "Running evaluation at iteration 14680...\n",
            "Iteration 14680, Train Loss: 3.8822405338287354 Eval Loss: 3.6178875207901\n",
            "Running evaluation at iteration 14690...\n",
            "Iteration 14690, Train Loss: 3.614469289779663 Eval Loss: 3.523203706741333\n",
            "Running evaluation at iteration 14700...\n",
            "Iteration 14700, Train Loss: 3.7332165241241455 Eval Loss: 3.482949709892273\n",
            "Running evaluation at iteration 14710...\n",
            "Iteration 14710, Train Loss: 3.6076183319091797 Eval Loss: 3.3730793237686156\n",
            "Running evaluation at iteration 14720...\n",
            "Iteration 14720, Train Loss: 3.888936996459961 Eval Loss: 3.153165912628174\n",
            "Running evaluation at iteration 14730...\n",
            "Iteration 14730, Train Loss: 4.170310974121094 Eval Loss: 3.456646227836609\n",
            "Running evaluation at iteration 14740...\n",
            "Iteration 14740, Train Loss: 3.594029188156128 Eval Loss: 3.468057107925415\n",
            "Running evaluation at iteration 14750...\n",
            "Iteration 14750, Train Loss: 3.731734037399292 Eval Loss: 3.3844202041625975\n",
            "Running evaluation at iteration 14760...\n",
            "Iteration 14760, Train Loss: 3.956352472305298 Eval Loss: 3.488416600227356\n",
            "Running evaluation at iteration 14770...\n",
            "Iteration 14770, Train Loss: 3.125493049621582 Eval Loss: 3.3469124317169188\n",
            "Running evaluation at iteration 14780...\n",
            "Iteration 14780, Train Loss: 3.8947341442108154 Eval Loss: 3.4658074378967285\n",
            "Running evaluation at iteration 14790...\n",
            "Iteration 14790, Train Loss: 3.5871288776397705 Eval Loss: 3.4847716093063354\n",
            "Running evaluation at iteration 14800...\n",
            "Iteration 14800, Train Loss: 3.582138776779175 Eval Loss: 3.254996180534363\n",
            "Running evaluation at iteration 14810...\n",
            "Iteration 14810, Train Loss: 3.565256118774414 Eval Loss: 3.4160478353500365\n",
            "Running evaluation at iteration 14820...\n",
            "Iteration 14820, Train Loss: 3.988859176635742 Eval Loss: 3.4193979263305665\n",
            "Running evaluation at iteration 14830...\n",
            "Iteration 14830, Train Loss: 3.799090623855591 Eval Loss: 3.4376805305480955\n",
            "Running evaluation at iteration 14840...\n",
            "Iteration 14840, Train Loss: 3.7846596240997314 Eval Loss: 3.392126441001892\n",
            "Running evaluation at iteration 14850...\n",
            "Iteration 14850, Train Loss: 3.4453563690185547 Eval Loss: 3.417688417434692\n",
            "Running evaluation at iteration 14860...\n",
            "Iteration 14860, Train Loss: 3.206528902053833 Eval Loss: 3.6594106435775755\n",
            "Running evaluation at iteration 14870...\n",
            "Iteration 14870, Train Loss: 3.981787919998169 Eval Loss: 3.3388182401657103\n",
            "Running evaluation at iteration 14880...\n",
            "Iteration 14880, Train Loss: 3.6575767993927 Eval Loss: 3.4058417797088625\n",
            "Running evaluation at iteration 14890...\n",
            "Iteration 14890, Train Loss: 3.6957547664642334 Eval Loss: 3.429598093032837\n",
            "Running evaluation at iteration 14900...\n",
            "Iteration 14900, Train Loss: 3.946244239807129 Eval Loss: 3.355426239967346\n",
            "Running evaluation at iteration 14910...\n",
            "Iteration 14910, Train Loss: 3.2957777976989746 Eval Loss: 3.354464364051819\n",
            "Running evaluation at iteration 14920...\n",
            "Iteration 14920, Train Loss: 3.3878509998321533 Eval Loss: 3.233353614807129\n",
            "Running evaluation at iteration 14930...\n",
            "Iteration 14930, Train Loss: 3.3777315616607666 Eval Loss: 3.4185842514038085\n",
            "Running evaluation at iteration 14940...\n",
            "Iteration 14940, Train Loss: 3.2356929779052734 Eval Loss: 3.376754069328308\n",
            "Running evaluation at iteration 14950...\n",
            "Iteration 14950, Train Loss: 3.403625249862671 Eval Loss: 3.139908695220947\n",
            "Running evaluation at iteration 14960...\n",
            "Iteration 14960, Train Loss: 3.1897284984588623 Eval Loss: 3.2608652353286742\n",
            "Running evaluation at iteration 14970...\n",
            "Iteration 14970, Train Loss: 3.804562568664551 Eval Loss: 3.478246259689331\n",
            "Running evaluation at iteration 14980...\n",
            "Iteration 14980, Train Loss: 3.6403074264526367 Eval Loss: 3.529179310798645\n",
            "Running evaluation at iteration 14990...\n",
            "Iteration 14990, Train Loss: 4.01159143447876 Eval Loss: 3.235791063308716\n",
            "Running evaluation at iteration 15000...\n",
            "Iteration 15000, Train Loss: 3.6100962162017822 Eval Loss: 3.4402608394622805\n",
            "Running evaluation at iteration 15010...\n",
            "Iteration 15010, Train Loss: 3.6023082733154297 Eval Loss: 3.4256126403808596\n",
            "Running evaluation at iteration 15020...\n",
            "Iteration 15020, Train Loss: 3.8530733585357666 Eval Loss: 3.4521006107330323\n",
            "Running evaluation at iteration 15030...\n",
            "Iteration 15030, Train Loss: 4.096438884735107 Eval Loss: 3.451339507102966\n",
            "Running evaluation at iteration 15040...\n",
            "Iteration 15040, Train Loss: 3.825425386428833 Eval Loss: 3.279975748062134\n",
            "Running evaluation at iteration 15050...\n",
            "Iteration 15050, Train Loss: 3.05018949508667 Eval Loss: 3.2660456895828247\n",
            "Running evaluation at iteration 15060...\n",
            "Iteration 15060, Train Loss: 3.5708203315734863 Eval Loss: 3.583285355567932\n",
            "Running evaluation at iteration 15070...\n",
            "Iteration 15070, Train Loss: 3.9054062366485596 Eval Loss: 3.3353416442871096\n",
            "Running evaluation at iteration 15080...\n",
            "Iteration 15080, Train Loss: 3.831576347351074 Eval Loss: 3.2619133234024047\n",
            "Running evaluation at iteration 15090...\n",
            "Iteration 15090, Train Loss: 3.561497688293457 Eval Loss: 3.2917493104934694\n",
            "Running evaluation at iteration 15100...\n",
            "Iteration 15100, Train Loss: 3.0485827922821045 Eval Loss: 3.292326641082764\n",
            "Running evaluation at iteration 15110...\n",
            "Iteration 15110, Train Loss: 2.749236822128296 Eval Loss: 3.580307221412659\n",
            "Running evaluation at iteration 15120...\n",
            "Iteration 15120, Train Loss: 3.5068564414978027 Eval Loss: 3.072472906112671\n",
            "Running evaluation at iteration 15130...\n",
            "Iteration 15130, Train Loss: 3.630089044570923 Eval Loss: 3.3538219213485716\n",
            "Running evaluation at iteration 15140...\n",
            "Iteration 15140, Train Loss: 3.4392499923706055 Eval Loss: 3.4235750675201415\n",
            "Running evaluation at iteration 15150...\n",
            "Iteration 15150, Train Loss: 3.418869733810425 Eval Loss: 3.527099776268005\n",
            "Running evaluation at iteration 15160...\n",
            "Iteration 15160, Train Loss: 3.0054922103881836 Eval Loss: 3.451331639289856\n",
            "Running evaluation at iteration 15170...\n",
            "Iteration 15170, Train Loss: 3.384509563446045 Eval Loss: 3.3970964193344115\n",
            "Running evaluation at iteration 15180...\n",
            "Iteration 15180, Train Loss: 3.9532437324523926 Eval Loss: 3.390415382385254\n",
            "Running evaluation at iteration 15190...\n",
            "Iteration 15190, Train Loss: 3.625931978225708 Eval Loss: 3.391321086883545\n",
            "Running evaluation at iteration 15200...\n",
            "Iteration 15200, Train Loss: 4.045433044433594 Eval Loss: 3.561145853996277\n",
            "Running evaluation at iteration 15210...\n",
            "Iteration 15210, Train Loss: 3.665916681289673 Eval Loss: 3.6542353868484496\n",
            "Running evaluation at iteration 15220...\n",
            "Iteration 15220, Train Loss: 3.4911375045776367 Eval Loss: 3.4479227304458617\n",
            "Running evaluation at iteration 15230...\n",
            "Iteration 15230, Train Loss: 3.0220441818237305 Eval Loss: 3.355755114555359\n",
            "Running evaluation at iteration 15240...\n",
            "Iteration 15240, Train Loss: 3.346921920776367 Eval Loss: 3.282432532310486\n",
            "Running evaluation at iteration 15250...\n",
            "Iteration 15250, Train Loss: 2.9364655017852783 Eval Loss: 3.554728388786316\n",
            "Running evaluation at iteration 15260...\n",
            "Iteration 15260, Train Loss: 3.0136358737945557 Eval Loss: 3.365410590171814\n",
            "Running evaluation at iteration 15270...\n",
            "Iteration 15270, Train Loss: 3.4960358142852783 Eval Loss: 3.479334235191345\n",
            "Running evaluation at iteration 15280...\n",
            "Iteration 15280, Train Loss: 3.325895309448242 Eval Loss: 3.3184625625610353\n",
            "Running evaluation at iteration 15290...\n",
            "Iteration 15290, Train Loss: 3.0545437335968018 Eval Loss: 3.413707399368286\n",
            "Running evaluation at iteration 15300...\n",
            "Iteration 15300, Train Loss: 3.600881338119507 Eval Loss: 3.5018351316452025\n",
            "Running evaluation at iteration 15310...\n",
            "Iteration 15310, Train Loss: 3.483900785446167 Eval Loss: 3.332258629798889\n",
            "Running evaluation at iteration 15320...\n",
            "Iteration 15320, Train Loss: 3.1871798038482666 Eval Loss: 3.349310040473938\n",
            "Running evaluation at iteration 15330...\n",
            "Iteration 15330, Train Loss: 3.8343923091888428 Eval Loss: 3.503695321083069\n",
            "Running evaluation at iteration 15340...\n",
            "Iteration 15340, Train Loss: 3.639132022857666 Eval Loss: 3.363167071342468\n",
            "Running evaluation at iteration 15350...\n",
            "Iteration 15350, Train Loss: 3.5894479751586914 Eval Loss: 3.4019088983535766\n",
            "Running evaluation at iteration 15360...\n",
            "Iteration 15360, Train Loss: 3.590792179107666 Eval Loss: 3.4821127891540526\n",
            "Running evaluation at iteration 15370...\n",
            "Iteration 15370, Train Loss: 3.4960618019104004 Eval Loss: 3.374841094017029\n",
            "Running evaluation at iteration 15380...\n",
            "Iteration 15380, Train Loss: 3.577507972717285 Eval Loss: 3.2784642934799195\n",
            "Running evaluation at iteration 15390...\n",
            "Iteration 15390, Train Loss: 3.116414785385132 Eval Loss: 3.5587870836257935\n",
            "Running evaluation at iteration 15400...\n",
            "Iteration 15400, Train Loss: 3.410446882247925 Eval Loss: 3.559663200378418\n",
            "Running evaluation at iteration 15410...\n",
            "Iteration 15410, Train Loss: 2.92073917388916 Eval Loss: 3.4666014194488524\n",
            "Running evaluation at iteration 15420...\n",
            "Iteration 15420, Train Loss: 3.522831916809082 Eval Loss: 3.401505637168884\n",
            "Running evaluation at iteration 15430...\n",
            "Iteration 15430, Train Loss: 4.090789318084717 Eval Loss: 3.6516357183456423\n",
            "Running evaluation at iteration 15440...\n",
            "Iteration 15440, Train Loss: 3.6479897499084473 Eval Loss: 3.2701125860214235\n",
            "Running evaluation at iteration 15450...\n",
            "Iteration 15450, Train Loss: 3.3892388343811035 Eval Loss: 3.1756758213043215\n",
            "Running evaluation at iteration 15460...\n",
            "Iteration 15460, Train Loss: 3.116398572921753 Eval Loss: 3.416365957260132\n",
            "Running evaluation at iteration 15470...\n",
            "Iteration 15470, Train Loss: 3.5908398628234863 Eval Loss: 3.3241424322128297\n",
            "Running evaluation at iteration 15480...\n",
            "Iteration 15480, Train Loss: 3.5411736965179443 Eval Loss: 3.3875812292099\n",
            "Running evaluation at iteration 15490...\n",
            "Iteration 15490, Train Loss: 3.4617128372192383 Eval Loss: 3.576237988471985\n",
            "Running evaluation at iteration 15500...\n",
            "Iteration 15500, Train Loss: 3.474768877029419 Eval Loss: 3.4803099632263184\n",
            "Running evaluation at iteration 15510...\n",
            "Iteration 15510, Train Loss: 3.82304310798645 Eval Loss: 3.5466702461242674\n",
            "Running evaluation at iteration 15520...\n",
            "Iteration 15520, Train Loss: 3.5225415229797363 Eval Loss: 3.357218360900879\n",
            "Running evaluation at iteration 15530...\n",
            "Iteration 15530, Train Loss: 3.713423490524292 Eval Loss: 3.369316744804382\n",
            "Running evaluation at iteration 15540...\n",
            "Iteration 15540, Train Loss: 2.8406014442443848 Eval Loss: 3.411013960838318\n",
            "Running evaluation at iteration 15550...\n",
            "Iteration 15550, Train Loss: 3.443432092666626 Eval Loss: 3.4368388414382935\n",
            "Running evaluation at iteration 15560...\n",
            "Iteration 15560, Train Loss: 3.5177838802337646 Eval Loss: 3.5012942790985107\n",
            "Running evaluation at iteration 15570...\n",
            "Iteration 15570, Train Loss: 3.8752951622009277 Eval Loss: 3.251069355010986\n",
            "Running evaluation at iteration 15580...\n",
            "Iteration 15580, Train Loss: 2.8571791648864746 Eval Loss: 3.4233448982238768\n",
            "Running evaluation at iteration 15590...\n",
            "Iteration 15590, Train Loss: 4.002465724945068 Eval Loss: 3.3090106010437013\n",
            "Running evaluation at iteration 15600...\n",
            "Iteration 15600, Train Loss: 3.368821620941162 Eval Loss: 3.1917760610580443\n",
            "Running evaluation at iteration 15610...\n",
            "Iteration 15610, Train Loss: 3.7654078006744385 Eval Loss: 3.300613832473755\n",
            "Running evaluation at iteration 15620...\n",
            "Iteration 15620, Train Loss: 3.6776463985443115 Eval Loss: 3.354595255851746\n",
            "Running evaluation at iteration 15630...\n",
            "Iteration 15630, Train Loss: 3.7210214138031006 Eval Loss: 3.348657011985779\n",
            "Running evaluation at iteration 15640...\n",
            "Iteration 15640, Train Loss: 2.795208215713501 Eval Loss: 3.6365546226501464\n",
            "Running evaluation at iteration 15650...\n",
            "Iteration 15650, Train Loss: 3.5242462158203125 Eval Loss: 3.437251353263855\n",
            "Running evaluation at iteration 15660...\n",
            "Iteration 15660, Train Loss: 4.169909954071045 Eval Loss: 3.5331884384155274\n",
            "Running evaluation at iteration 15670...\n",
            "Iteration 15670, Train Loss: 3.283184289932251 Eval Loss: 3.4289618730545044\n",
            "Running evaluation at iteration 15680...\n",
            "Iteration 15680, Train Loss: 3.605665683746338 Eval Loss: 3.4994956970214846\n",
            "Running evaluation at iteration 15690...\n",
            "Iteration 15690, Train Loss: 3.9900319576263428 Eval Loss: 3.59202401638031\n",
            "Running evaluation at iteration 15700...\n",
            "Iteration 15700, Train Loss: 3.7494592666625977 Eval Loss: 3.3189202547073364\n",
            "Running evaluation at iteration 15710...\n",
            "Iteration 15710, Train Loss: 3.5768628120422363 Eval Loss: 3.502902698516846\n",
            "Running evaluation at iteration 15720...\n",
            "Iteration 15720, Train Loss: 4.029985427856445 Eval Loss: 3.6053658723831177\n",
            "Running evaluation at iteration 15730...\n",
            "Iteration 15730, Train Loss: 3.181321382522583 Eval Loss: 3.4259525537490845\n",
            "Running evaluation at iteration 15740...\n",
            "Iteration 15740, Train Loss: 3.227710247039795 Eval Loss: 3.2794398069381714\n",
            "Running evaluation at iteration 15750...\n",
            "Iteration 15750, Train Loss: 2.2825896739959717 Eval Loss: 3.08906512260437\n",
            "Running evaluation at iteration 15760...\n",
            "Iteration 15760, Train Loss: 2.2238149642944336 Eval Loss: 3.562815856933594\n",
            "Running evaluation at iteration 15770...\n",
            "Iteration 15770, Train Loss: 3.3418877124786377 Eval Loss: 3.4864848375320436\n",
            "Running evaluation at iteration 15780...\n",
            "Iteration 15780, Train Loss: 3.7756457328796387 Eval Loss: 3.519463562965393\n",
            "Running evaluation at iteration 15790...\n",
            "Iteration 15790, Train Loss: 3.8628504276275635 Eval Loss: 3.4128623723983766\n",
            "Running evaluation at iteration 15800...\n",
            "Iteration 15800, Train Loss: 3.954465866088867 Eval Loss: 3.3138015508651733\n",
            "Running evaluation at iteration 15810...\n",
            "Iteration 15810, Train Loss: 3.5230257511138916 Eval Loss: 3.449345111846924\n",
            "Running evaluation at iteration 15820...\n",
            "Iteration 15820, Train Loss: 2.8620755672454834 Eval Loss: 3.205787420272827\n",
            "Running evaluation at iteration 15830...\n",
            "Iteration 15830, Train Loss: 3.7136032581329346 Eval Loss: 3.4476402521133425\n",
            "Running evaluation at iteration 15840...\n",
            "Iteration 15840, Train Loss: 3.7295262813568115 Eval Loss: 3.385153603553772\n",
            "Running evaluation at iteration 15850...\n",
            "Iteration 15850, Train Loss: 3.6576180458068848 Eval Loss: 3.4391082763671874\n",
            "Running evaluation at iteration 15860...\n",
            "Iteration 15860, Train Loss: 3.504117012023926 Eval Loss: 3.3982534646987914\n",
            "Running evaluation at iteration 15870...\n",
            "Iteration 15870, Train Loss: 2.2616758346557617 Eval Loss: 3.3251903295516967\n",
            "Running evaluation at iteration 15880...\n",
            "Iteration 15880, Train Loss: 3.6322851181030273 Eval Loss: 3.452613949775696\n",
            "Running evaluation at iteration 15890...\n",
            "Iteration 15890, Train Loss: 3.043544054031372 Eval Loss: 3.5305363655090334\n",
            "Running evaluation at iteration 15900...\n",
            "Iteration 15900, Train Loss: 3.846991539001465 Eval Loss: 3.5483513116836547\n",
            "Running evaluation at iteration 15910...\n",
            "Iteration 15910, Train Loss: 3.7841248512268066 Eval Loss: 3.27244918346405\n",
            "Running evaluation at iteration 15920...\n",
            "Iteration 15920, Train Loss: 3.716648817062378 Eval Loss: 3.5247292280197144\n",
            "Running evaluation at iteration 15930...\n",
            "Iteration 15930, Train Loss: 3.832427740097046 Eval Loss: 3.1815908432006834\n",
            "Running evaluation at iteration 15940...\n",
            "Iteration 15940, Train Loss: 4.2257609367370605 Eval Loss: 3.384371590614319\n",
            "Running evaluation at iteration 15950...\n",
            "Iteration 15950, Train Loss: 4.078919887542725 Eval Loss: 3.3108123779296874\n",
            "Running evaluation at iteration 15960...\n",
            "Iteration 15960, Train Loss: 2.8472893238067627 Eval Loss: 3.515843057632446\n",
            "Running evaluation at iteration 15970...\n",
            "Iteration 15970, Train Loss: 3.5336782932281494 Eval Loss: 3.388098931312561\n",
            "Running evaluation at iteration 15980...\n",
            "Iteration 15980, Train Loss: 3.632286787033081 Eval Loss: 3.2910593509674073\n",
            "Running evaluation at iteration 15990...\n",
            "Iteration 15990, Train Loss: 3.1533737182617188 Eval Loss: 3.408902716636658\n",
            "Running evaluation at iteration 16000...\n",
            "Iteration 16000, Train Loss: 3.387441396713257 Eval Loss: 3.450110363960266\n",
            "Running evaluation at iteration 16010...\n",
            "Iteration 16010, Train Loss: 2.575138807296753 Eval Loss: 3.4468079566955567\n",
            "Running evaluation at iteration 16020...\n",
            "Iteration 16020, Train Loss: 3.9245312213897705 Eval Loss: 3.388299655914307\n",
            "Running evaluation at iteration 16030...\n",
            "Iteration 16030, Train Loss: 3.3997790813446045 Eval Loss: 3.1252562046051025\n",
            "Running evaluation at iteration 16040...\n",
            "Iteration 16040, Train Loss: 2.86663818359375 Eval Loss: 3.5828413724899293\n",
            "Running evaluation at iteration 16050...\n",
            "Iteration 16050, Train Loss: 3.535494327545166 Eval Loss: 3.5598056077957154\n",
            "Running evaluation at iteration 16060...\n",
            "Iteration 16060, Train Loss: 3.4213762283325195 Eval Loss: 3.386773109436035\n",
            "Running evaluation at iteration 16070...\n",
            "Iteration 16070, Train Loss: 3.235023260116577 Eval Loss: 3.4349807262420655\n",
            "Running evaluation at iteration 16080...\n",
            "Iteration 16080, Train Loss: 3.2600042819976807 Eval Loss: 3.584828805923462\n",
            "Running evaluation at iteration 16090...\n",
            "Iteration 16090, Train Loss: 3.6821205615997314 Eval Loss: 3.4216126441955566\n",
            "Running evaluation at iteration 16100...\n",
            "Iteration 16100, Train Loss: 3.343168020248413 Eval Loss: 3.298387145996094\n",
            "Running evaluation at iteration 16110...\n",
            "Iteration 16110, Train Loss: 3.211639404296875 Eval Loss: 3.4818090438842773\n",
            "Running evaluation at iteration 16120...\n",
            "Iteration 16120, Train Loss: 2.8515820503234863 Eval Loss: 3.239515781402588\n",
            "Running evaluation at iteration 16130...\n",
            "Iteration 16130, Train Loss: 3.1473183631896973 Eval Loss: 3.3375611782073973\n",
            "Running evaluation at iteration 16140...\n",
            "Iteration 16140, Train Loss: 3.387625217437744 Eval Loss: 3.2902594327926638\n",
            "Running evaluation at iteration 16150...\n",
            "Iteration 16150, Train Loss: 3.3858466148376465 Eval Loss: 3.244745707511902\n",
            "Running evaluation at iteration 16160...\n",
            "Iteration 16160, Train Loss: 3.2840425968170166 Eval Loss: 3.586805319786072\n",
            "Running evaluation at iteration 16170...\n",
            "Iteration 16170, Train Loss: 3.3255434036254883 Eval Loss: 3.330117344856262\n",
            "Running evaluation at iteration 16180...\n",
            "Iteration 16180, Train Loss: 3.6712865829467773 Eval Loss: 3.3170555591583253\n",
            "Running evaluation at iteration 16190...\n",
            "Iteration 16190, Train Loss: 3.657536029815674 Eval Loss: 3.3352169513702394\n",
            "Running evaluation at iteration 16200...\n",
            "Iteration 16200, Train Loss: 3.3250808715820312 Eval Loss: 3.491298484802246\n",
            "Running evaluation at iteration 16210...\n",
            "Iteration 16210, Train Loss: 3.4834606647491455 Eval Loss: 3.393945240974426\n",
            "Running evaluation at iteration 16220...\n",
            "Iteration 16220, Train Loss: 3.725344657897949 Eval Loss: 3.206576371192932\n",
            "Running evaluation at iteration 16230...\n",
            "Iteration 16230, Train Loss: 3.122734785079956 Eval Loss: 3.334290599822998\n",
            "Running evaluation at iteration 16240...\n",
            "Iteration 16240, Train Loss: 3.914581060409546 Eval Loss: 3.553735589981079\n",
            "Running evaluation at iteration 16250...\n",
            "Iteration 16250, Train Loss: 2.9524331092834473 Eval Loss: 3.3400044679641723\n",
            "Running evaluation at iteration 16260...\n",
            "Iteration 16260, Train Loss: 3.8960516452789307 Eval Loss: 3.3826846361160277\n",
            "Running evaluation at iteration 16270...\n",
            "Iteration 16270, Train Loss: 3.372767686843872 Eval Loss: 3.4157268524169924\n",
            "Running evaluation at iteration 16280...\n",
            "Iteration 16280, Train Loss: 3.9660284519195557 Eval Loss: 3.4874791860580445\n",
            "Running evaluation at iteration 16290...\n",
            "Iteration 16290, Train Loss: 3.6619184017181396 Eval Loss: 3.0753240823745727\n",
            "Running evaluation at iteration 16300...\n",
            "Iteration 16300, Train Loss: 3.8195910453796387 Eval Loss: 3.441066288948059\n",
            "Running evaluation at iteration 16310...\n",
            "Iteration 16310, Train Loss: 3.095775842666626 Eval Loss: 3.298830008506775\n",
            "Running evaluation at iteration 16320...\n",
            "Iteration 16320, Train Loss: 3.2349843978881836 Eval Loss: 3.537916326522827\n",
            "Running evaluation at iteration 16330...\n",
            "Iteration 16330, Train Loss: 3.827873468399048 Eval Loss: 3.536954665184021\n",
            "Running evaluation at iteration 16340...\n",
            "Iteration 16340, Train Loss: 3.5970211029052734 Eval Loss: 3.2271598100662233\n",
            "Running evaluation at iteration 16350...\n",
            "Iteration 16350, Train Loss: 2.663278818130493 Eval Loss: 3.409295916557312\n",
            "Running evaluation at iteration 16360...\n",
            "Iteration 16360, Train Loss: 3.8795595169067383 Eval Loss: 3.3040223836898805\n",
            "Running evaluation at iteration 16370...\n",
            "Iteration 16370, Train Loss: 3.604653835296631 Eval Loss: 3.5079586029052736\n",
            "Running evaluation at iteration 16380...\n",
            "Iteration 16380, Train Loss: 3.3928911685943604 Eval Loss: 3.198833727836609\n",
            "Running evaluation at iteration 16390...\n",
            "Iteration 16390, Train Loss: 3.714454412460327 Eval Loss: 3.354305458068848\n",
            "Running evaluation at iteration 16400...\n",
            "Iteration 16400, Train Loss: 3.3677432537078857 Eval Loss: 3.2978028297424316\n",
            "Running evaluation at iteration 16410...\n",
            "Iteration 16410, Train Loss: 3.4105823040008545 Eval Loss: 3.2430826663970946\n",
            "Running evaluation at iteration 16420...\n",
            "Iteration 16420, Train Loss: 3.8262529373168945 Eval Loss: 3.4055556297302245\n",
            "Running evaluation at iteration 16430...\n",
            "Iteration 16430, Train Loss: 3.6371517181396484 Eval Loss: 3.328070783615112\n",
            "Running evaluation at iteration 16440...\n",
            "Iteration 16440, Train Loss: 2.855656147003174 Eval Loss: 3.3556185007095336\n",
            "Running evaluation at iteration 16450...\n",
            "Iteration 16450, Train Loss: 3.647420883178711 Eval Loss: 3.3342021226882936\n",
            "Running evaluation at iteration 16460...\n",
            "Iteration 16460, Train Loss: 3.0672249794006348 Eval Loss: 3.4018948793411257\n",
            "Running evaluation at iteration 16470...\n",
            "Iteration 16470, Train Loss: 2.9654150009155273 Eval Loss: 3.217443060874939\n",
            "Running evaluation at iteration 16480...\n",
            "Iteration 16480, Train Loss: 2.5919394493103027 Eval Loss: 3.365284824371338\n",
            "Running evaluation at iteration 16490...\n",
            "Iteration 16490, Train Loss: 3.7613565921783447 Eval Loss: 3.3093844175338747\n",
            "Running evaluation at iteration 16500...\n",
            "Iteration 16500, Train Loss: 3.4553310871124268 Eval Loss: 3.3681941509246824\n",
            "Running evaluation at iteration 16510...\n",
            "Iteration 16510, Train Loss: 4.1400604248046875 Eval Loss: 3.298138427734375\n",
            "Running evaluation at iteration 16520...\n",
            "Iteration 16520, Train Loss: 3.217548131942749 Eval Loss: 3.1742664575576782\n",
            "Running evaluation at iteration 16530...\n",
            "Iteration 16530, Train Loss: 3.3329219818115234 Eval Loss: 3.4476627349853515\n",
            "Running evaluation at iteration 16540...\n",
            "Iteration 16540, Train Loss: 4.0340070724487305 Eval Loss: 3.709744930267334\n",
            "Running evaluation at iteration 16550...\n",
            "Iteration 16550, Train Loss: 3.609856605529785 Eval Loss: 3.367781639099121\n",
            "Running evaluation at iteration 16560...\n",
            "Iteration 16560, Train Loss: 4.051620006561279 Eval Loss: 3.4135921716690065\n",
            "Running evaluation at iteration 16570...\n",
            "Iteration 16570, Train Loss: 3.5430285930633545 Eval Loss: 3.2803141117095946\n",
            "Running evaluation at iteration 16580...\n",
            "Iteration 16580, Train Loss: 2.7233493328094482 Eval Loss: 3.3993998527526856\n",
            "Running evaluation at iteration 16590...\n",
            "Iteration 16590, Train Loss: 3.85568904876709 Eval Loss: 3.4975172758102415\n",
            "Running evaluation at iteration 16600...\n",
            "Iteration 16600, Train Loss: 2.6737308502197266 Eval Loss: 3.5210909128189085\n",
            "Running evaluation at iteration 16610...\n",
            "Iteration 16610, Train Loss: 2.9423372745513916 Eval Loss: 3.4756352186203\n",
            "Running evaluation at iteration 16620...\n",
            "Iteration 16620, Train Loss: 4.049601078033447 Eval Loss: 3.354128861427307\n",
            "Running evaluation at iteration 16630...\n",
            "Iteration 16630, Train Loss: 2.8412094116210938 Eval Loss: 3.301691937446594\n",
            "Running evaluation at iteration 16640...\n",
            "Iteration 16640, Train Loss: 3.09883451461792 Eval Loss: 3.499615955352783\n",
            "Running evaluation at iteration 16650...\n",
            "Iteration 16650, Train Loss: 3.219484567642212 Eval Loss: 3.387336564064026\n",
            "Running evaluation at iteration 16660...\n",
            "Iteration 16660, Train Loss: 3.4988677501678467 Eval Loss: 3.4955554723739626\n",
            "Running evaluation at iteration 16670...\n",
            "Iteration 16670, Train Loss: 3.869455337524414 Eval Loss: 3.4727322101593017\n",
            "Running evaluation at iteration 16680...\n",
            "Iteration 16680, Train Loss: 3.680213689804077 Eval Loss: 3.520491337776184\n",
            "Running evaluation at iteration 16690...\n",
            "Iteration 16690, Train Loss: 3.6364529132843018 Eval Loss: 3.181395149230957\n",
            "Running evaluation at iteration 16700...\n",
            "Iteration 16700, Train Loss: 4.0207929611206055 Eval Loss: 3.362460207939148\n",
            "Running evaluation at iteration 16710...\n",
            "Iteration 16710, Train Loss: 3.615779161453247 Eval Loss: 3.4631817817687987\n",
            "Running evaluation at iteration 16720...\n",
            "Iteration 16720, Train Loss: 3.724318265914917 Eval Loss: 3.3511301994323732\n",
            "Running evaluation at iteration 16730...\n",
            "Iteration 16730, Train Loss: 3.10349702835083 Eval Loss: 3.375331449508667\n",
            "Running evaluation at iteration 16740...\n",
            "Iteration 16740, Train Loss: 3.5265321731567383 Eval Loss: 3.4924554109573362\n",
            "Running evaluation at iteration 16750...\n",
            "Iteration 16750, Train Loss: 3.614315986633301 Eval Loss: 3.3898349046707152\n",
            "Running evaluation at iteration 16760...\n",
            "Iteration 16760, Train Loss: 3.309783935546875 Eval Loss: 3.3349663972854615\n",
            "Running evaluation at iteration 16770...\n",
            "Iteration 16770, Train Loss: 3.0549356937408447 Eval Loss: 3.3360209703445434\n",
            "Running evaluation at iteration 16780...\n",
            "Iteration 16780, Train Loss: 3.585073471069336 Eval Loss: 3.368414616584778\n",
            "Running evaluation at iteration 16790...\n",
            "Iteration 16790, Train Loss: 3.667551040649414 Eval Loss: 3.61917564868927\n",
            "Running evaluation at iteration 16800...\n",
            "Iteration 16800, Train Loss: 3.217597007751465 Eval Loss: 3.508180332183838\n",
            "Running evaluation at iteration 16810...\n",
            "Iteration 16810, Train Loss: 3.275423288345337 Eval Loss: 3.1410791873931885\n",
            "Running evaluation at iteration 16820...\n",
            "Iteration 16820, Train Loss: 4.288715362548828 Eval Loss: 3.4260455846786497\n",
            "Running evaluation at iteration 16830...\n",
            "Iteration 16830, Train Loss: 3.6705987453460693 Eval Loss: 3.2548083543777464\n",
            "Running evaluation at iteration 16840...\n",
            "Iteration 16840, Train Loss: 3.088834285736084 Eval Loss: 3.3175179243087767\n",
            "Running evaluation at iteration 16850...\n",
            "Iteration 16850, Train Loss: 3.200540781021118 Eval Loss: 3.3479055166244507\n",
            "Running evaluation at iteration 16860...\n",
            "Iteration 16860, Train Loss: 3.0803492069244385 Eval Loss: 3.2832608222961426\n",
            "Running evaluation at iteration 16870...\n",
            "Iteration 16870, Train Loss: 3.8722827434539795 Eval Loss: 3.308156442642212\n",
            "Running evaluation at iteration 16880...\n",
            "Iteration 16880, Train Loss: 2.7694602012634277 Eval Loss: 3.5593992948532103\n",
            "Running evaluation at iteration 16890...\n",
            "Iteration 16890, Train Loss: 3.027867078781128 Eval Loss: 3.4391386985778807\n",
            "Running evaluation at iteration 16900...\n",
            "Iteration 16900, Train Loss: 3.5225534439086914 Eval Loss: 3.5986258029937743\n",
            "Running evaluation at iteration 16910...\n",
            "Iteration 16910, Train Loss: 3.510059118270874 Eval Loss: 3.475806641578674\n",
            "Running evaluation at iteration 16920...\n",
            "Iteration 16920, Train Loss: 3.5426547527313232 Eval Loss: 3.403592920303345\n",
            "Running evaluation at iteration 16930...\n",
            "Iteration 16930, Train Loss: 3.3366622924804688 Eval Loss: 3.3621319055557253\n",
            "Running evaluation at iteration 16940...\n",
            "Iteration 16940, Train Loss: 2.8609793186187744 Eval Loss: 3.399048089981079\n",
            "Running evaluation at iteration 16950...\n",
            "Iteration 16950, Train Loss: 3.344864845275879 Eval Loss: 3.4153725147247314\n",
            "Running evaluation at iteration 16960...\n",
            "Iteration 16960, Train Loss: 3.304168701171875 Eval Loss: 3.2793821811676027\n",
            "Running evaluation at iteration 16970...\n",
            "Iteration 16970, Train Loss: 3.6580893993377686 Eval Loss: 3.3926019191741945\n",
            "Running evaluation at iteration 16980...\n",
            "Iteration 16980, Train Loss: 3.354685068130493 Eval Loss: 3.401238965988159\n",
            "Running evaluation at iteration 16990...\n",
            "Iteration 16990, Train Loss: 3.4779746532440186 Eval Loss: 3.2486062049865723\n",
            "Running evaluation at iteration 17000...\n",
            "Iteration 17000, Train Loss: 3.5302820205688477 Eval Loss: 3.438071584701538\n",
            "Running evaluation at iteration 17010...\n",
            "Iteration 17010, Train Loss: 3.4323668479919434 Eval Loss: 3.3329651594161986\n",
            "Running evaluation at iteration 17020...\n",
            "Iteration 17020, Train Loss: 3.761080265045166 Eval Loss: 3.484030318260193\n",
            "Running evaluation at iteration 17030...\n",
            "Iteration 17030, Train Loss: 3.642611503601074 Eval Loss: 3.349197196960449\n",
            "Running evaluation at iteration 17040...\n",
            "Iteration 17040, Train Loss: 4.545831680297852 Eval Loss: 3.4179271697998046\n",
            "Running evaluation at iteration 17050...\n",
            "Iteration 17050, Train Loss: 2.9179084300994873 Eval Loss: 3.2325257062911987\n",
            "Running evaluation at iteration 17060...\n",
            "Iteration 17060, Train Loss: 3.144049882888794 Eval Loss: 3.3189980745315553\n",
            "Running evaluation at iteration 17070...\n",
            "Iteration 17070, Train Loss: 3.0192372798919678 Eval Loss: 3.328429365158081\n",
            "Running evaluation at iteration 17080...\n",
            "Iteration 17080, Train Loss: 3.23779296875 Eval Loss: 3.3549368381500244\n",
            "Running evaluation at iteration 17090...\n",
            "Iteration 17090, Train Loss: 3.9840712547302246 Eval Loss: 3.3632619619369506\n",
            "Running evaluation at iteration 17100...\n",
            "Iteration 17100, Train Loss: 3.5380606651306152 Eval Loss: 3.355758023262024\n",
            "Running evaluation at iteration 17110...\n",
            "Iteration 17110, Train Loss: 3.452376127243042 Eval Loss: 3.399600601196289\n",
            "Running evaluation at iteration 17120...\n",
            "Iteration 17120, Train Loss: 3.190178871154785 Eval Loss: 3.4609315395355225\n",
            "Running evaluation at iteration 17130...\n",
            "Iteration 17130, Train Loss: 3.202749252319336 Eval Loss: 3.287063217163086\n",
            "Running evaluation at iteration 17140...\n",
            "Iteration 17140, Train Loss: 3.3722870349884033 Eval Loss: 3.3011714696884153\n",
            "Running evaluation at iteration 17150...\n",
            "Iteration 17150, Train Loss: 4.478057384490967 Eval Loss: 3.4064889669418337\n",
            "Running evaluation at iteration 17160...\n",
            "Iteration 17160, Train Loss: 3.4921340942382812 Eval Loss: 3.373153877258301\n",
            "Running evaluation at iteration 17170...\n",
            "Iteration 17170, Train Loss: 3.7606983184814453 Eval Loss: 3.278129053115845\n",
            "Running evaluation at iteration 17180...\n",
            "Iteration 17180, Train Loss: 2.732783317565918 Eval Loss: 3.4090226888656616\n",
            "Running evaluation at iteration 17190...\n",
            "Iteration 17190, Train Loss: 3.8196091651916504 Eval Loss: 3.384621429443359\n",
            "Running evaluation at iteration 17200...\n",
            "Iteration 17200, Train Loss: 3.4658021926879883 Eval Loss: 3.276643681526184\n",
            "Running evaluation at iteration 17210...\n",
            "Iteration 17210, Train Loss: 2.9365780353546143 Eval Loss: 3.423051905632019\n",
            "Running evaluation at iteration 17220...\n",
            "Iteration 17220, Train Loss: 3.2879798412323 Eval Loss: 3.4396680116653444\n",
            "Running evaluation at iteration 17230...\n",
            "Iteration 17230, Train Loss: 3.5892162322998047 Eval Loss: 3.4933281183242797\n",
            "Running evaluation at iteration 17240...\n",
            "Iteration 17240, Train Loss: 2.767543077468872 Eval Loss: 3.353088116645813\n",
            "Running evaluation at iteration 17250...\n",
            "Iteration 17250, Train Loss: 2.7187201976776123 Eval Loss: 3.125235605239868\n",
            "Running evaluation at iteration 17260...\n",
            "Iteration 17260, Train Loss: 3.1490111351013184 Eval Loss: 3.4497796297073364\n",
            "Running evaluation at iteration 17270...\n",
            "Iteration 17270, Train Loss: 3.0842206478118896 Eval Loss: 3.2600205898284913\n",
            "Running evaluation at iteration 17280...\n",
            "Iteration 17280, Train Loss: 3.326145648956299 Eval Loss: 3.0837646484375\n",
            "Running evaluation at iteration 17290...\n",
            "Iteration 17290, Train Loss: 3.769742727279663 Eval Loss: 3.315623927116394\n",
            "Running evaluation at iteration 17300...\n",
            "Iteration 17300, Train Loss: 3.2179794311523438 Eval Loss: 3.5020493268966675\n",
            "Running evaluation at iteration 17310...\n",
            "Iteration 17310, Train Loss: 3.1035540103912354 Eval Loss: 3.202443313598633\n",
            "Running evaluation at iteration 17320...\n",
            "Iteration 17320, Train Loss: 3.419287919998169 Eval Loss: 3.317619037628174\n",
            "Running evaluation at iteration 17330...\n",
            "Iteration 17330, Train Loss: 4.089054107666016 Eval Loss: 3.429040002822876\n",
            "Running evaluation at iteration 17340...\n",
            "Iteration 17340, Train Loss: 3.4402260780334473 Eval Loss: 3.393942141532898\n",
            "Running evaluation at iteration 17350...\n",
            "Iteration 17350, Train Loss: 3.186863660812378 Eval Loss: 3.265826153755188\n",
            "Running evaluation at iteration 17360...\n",
            "Iteration 17360, Train Loss: 3.739949941635132 Eval Loss: 3.392993664741516\n",
            "Running evaluation at iteration 17370...\n",
            "Iteration 17370, Train Loss: 3.3224422931671143 Eval Loss: 3.1847920179367066\n",
            "Running evaluation at iteration 17380...\n",
            "Iteration 17380, Train Loss: 3.4774200916290283 Eval Loss: 3.3662973403930665\n",
            "Running evaluation at iteration 17390...\n",
            "Iteration 17390, Train Loss: 3.331416130065918 Eval Loss: 3.5112977027893066\n",
            "Running evaluation at iteration 17400...\n",
            "Iteration 17400, Train Loss: 3.751206636428833 Eval Loss: 3.2598548889160157\n",
            "Running evaluation at iteration 17410...\n",
            "Iteration 17410, Train Loss: 3.8236265182495117 Eval Loss: 3.246017599105835\n",
            "Running evaluation at iteration 17420...\n",
            "Iteration 17420, Train Loss: 3.5077059268951416 Eval Loss: 3.4979031801223757\n",
            "Running evaluation at iteration 17430...\n",
            "Iteration 17430, Train Loss: 3.5564663410186768 Eval Loss: 3.1431663036346436\n",
            "Running evaluation at iteration 17440...\n",
            "Iteration 17440, Train Loss: 3.4490723609924316 Eval Loss: 3.3868374586105348\n",
            "Running evaluation at iteration 17450...\n",
            "Iteration 17450, Train Loss: 3.098513126373291 Eval Loss: 3.4038105964660645\n",
            "Running evaluation at iteration 17460...\n",
            "Iteration 17460, Train Loss: 3.436877489089966 Eval Loss: 3.402343916893005\n",
            "Running evaluation at iteration 17470...\n",
            "Iteration 17470, Train Loss: 3.4620919227600098 Eval Loss: 3.2817379236221313\n",
            "Running evaluation at iteration 17480...\n",
            "Iteration 17480, Train Loss: 3.465806007385254 Eval Loss: 3.5148318290710447\n",
            "Running evaluation at iteration 17490...\n",
            "Iteration 17490, Train Loss: 3.4800243377685547 Eval Loss: 3.248107600212097\n",
            "Running evaluation at iteration 17500...\n",
            "Iteration 17500, Train Loss: 3.6049611568450928 Eval Loss: 3.20398428440094\n",
            "Running evaluation at iteration 17510...\n",
            "Iteration 17510, Train Loss: 4.400490760803223 Eval Loss: 3.419728899002075\n",
            "Running evaluation at iteration 17520...\n",
            "Iteration 17520, Train Loss: 3.6276345252990723 Eval Loss: 3.539239192008972\n",
            "Running evaluation at iteration 17530...\n",
            "Iteration 17530, Train Loss: 2.8929996490478516 Eval Loss: 3.3398656845092773\n",
            "Running evaluation at iteration 17540...\n",
            "Iteration 17540, Train Loss: 3.76132869720459 Eval Loss: 3.553617024421692\n",
            "Running evaluation at iteration 17550...\n",
            "Iteration 17550, Train Loss: 3.279183864593506 Eval Loss: 3.6057999610900877\n",
            "Running evaluation at iteration 17560...\n",
            "Iteration 17560, Train Loss: 3.483307123184204 Eval Loss: 3.5384326457977293\n",
            "Running evaluation at iteration 17570...\n",
            "Iteration 17570, Train Loss: 3.710019111633301 Eval Loss: 3.2477927207946777\n",
            "Running evaluation at iteration 17580...\n",
            "Iteration 17580, Train Loss: 3.3966357707977295 Eval Loss: 3.29046630859375\n",
            "Running evaluation at iteration 17590...\n",
            "Iteration 17590, Train Loss: 3.0287139415740967 Eval Loss: 3.4158690929412843\n",
            "Running evaluation at iteration 17600...\n",
            "Iteration 17600, Train Loss: 3.143155813217163 Eval Loss: 3.1550217866897583\n",
            "Running evaluation at iteration 17610...\n",
            "Iteration 17610, Train Loss: 3.3866794109344482 Eval Loss: 3.349087429046631\n",
            "Running evaluation at iteration 17620...\n",
            "Iteration 17620, Train Loss: 4.377860069274902 Eval Loss: 3.2392255783081056\n",
            "Running evaluation at iteration 17630...\n",
            "Iteration 17630, Train Loss: 3.521832227706909 Eval Loss: 3.613297128677368\n",
            "Running evaluation at iteration 17640...\n",
            "Iteration 17640, Train Loss: 3.0406694412231445 Eval Loss: 3.5229955196380613\n",
            "Running evaluation at iteration 17650...\n",
            "Iteration 17650, Train Loss: 3.981567621231079 Eval Loss: 3.2525619983673097\n",
            "Running evaluation at iteration 17660...\n",
            "Iteration 17660, Train Loss: 3.057516574859619 Eval Loss: 3.5782703876495363\n",
            "Running evaluation at iteration 17670...\n",
            "Iteration 17670, Train Loss: 3.095308542251587 Eval Loss: 3.575897145271301\n",
            "Running evaluation at iteration 17680...\n",
            "Iteration 17680, Train Loss: 3.3591115474700928 Eval Loss: 3.1829731464385986\n",
            "Running evaluation at iteration 17690...\n",
            "Iteration 17690, Train Loss: 3.145650863647461 Eval Loss: 3.3457184553146364\n",
            "Running evaluation at iteration 17700...\n",
            "Iteration 17700, Train Loss: 3.632661819458008 Eval Loss: 3.3647958517074583\n",
            "Running evaluation at iteration 17710...\n",
            "Iteration 17710, Train Loss: 2.742706298828125 Eval Loss: 3.3738959074020385\n",
            "Running evaluation at iteration 17720...\n",
            "Iteration 17720, Train Loss: 3.1826236248016357 Eval Loss: 3.3443885326385496\n",
            "Running evaluation at iteration 17730...\n",
            "Iteration 17730, Train Loss: 3.036263942718506 Eval Loss: 3.5710529327392577\n",
            "Running evaluation at iteration 17740...\n",
            "Iteration 17740, Train Loss: 3.158411979675293 Eval Loss: 3.390190005302429\n",
            "Running evaluation at iteration 17750...\n",
            "Iteration 17750, Train Loss: 3.643178939819336 Eval Loss: 3.467561459541321\n",
            "Running evaluation at iteration 17760...\n",
            "Iteration 17760, Train Loss: 3.6231424808502197 Eval Loss: 3.391338515281677\n",
            "Running evaluation at iteration 17770...\n",
            "Iteration 17770, Train Loss: 3.719362258911133 Eval Loss: 3.403094506263733\n",
            "Running evaluation at iteration 17780...\n",
            "Iteration 17780, Train Loss: 3.2624335289001465 Eval Loss: 3.3897178888320925\n",
            "Running evaluation at iteration 17790...\n",
            "Iteration 17790, Train Loss: 3.2391462326049805 Eval Loss: 3.3374844551086427\n",
            "Running evaluation at iteration 17800...\n",
            "Iteration 17800, Train Loss: 3.5464870929718018 Eval Loss: 3.3283631801605225\n",
            "Running evaluation at iteration 17810...\n",
            "Iteration 17810, Train Loss: 3.859311819076538 Eval Loss: 3.4387038946151733\n",
            "Running evaluation at iteration 17820...\n",
            "Iteration 17820, Train Loss: 3.323873281478882 Eval Loss: 3.3002164125442506\n",
            "Running evaluation at iteration 17830...\n",
            "Iteration 17830, Train Loss: 3.7912917137145996 Eval Loss: 3.52478563785553\n",
            "Running evaluation at iteration 17840...\n",
            "Iteration 17840, Train Loss: 3.8968658447265625 Eval Loss: 3.3224848031997682\n",
            "Running evaluation at iteration 17850...\n",
            "Iteration 17850, Train Loss: 3.0735924243927 Eval Loss: 3.3964261054992675\n",
            "Running evaluation at iteration 17860...\n",
            "Iteration 17860, Train Loss: 3.660290479660034 Eval Loss: 3.1034806251525877\n",
            "Running evaluation at iteration 17870...\n",
            "Iteration 17870, Train Loss: 3.8742568492889404 Eval Loss: 3.375673842430115\n",
            "Running evaluation at iteration 17880...\n",
            "Iteration 17880, Train Loss: 3.3028817176818848 Eval Loss: 3.345263361930847\n",
            "Running evaluation at iteration 17890...\n",
            "Iteration 17890, Train Loss: 3.7130820751190186 Eval Loss: 3.31362898349762\n",
            "Running evaluation at iteration 17900...\n",
            "Iteration 17900, Train Loss: 3.295389413833618 Eval Loss: 3.5108246564865113\n",
            "Running evaluation at iteration 17910...\n",
            "Iteration 17910, Train Loss: 4.106700420379639 Eval Loss: 3.421265482902527\n",
            "Running evaluation at iteration 17920...\n",
            "Iteration 17920, Train Loss: 2.923055410385132 Eval Loss: 3.4119491577148438\n",
            "Running evaluation at iteration 17930...\n",
            "Iteration 17930, Train Loss: 3.5753893852233887 Eval Loss: 3.5731131076812743\n",
            "Running evaluation at iteration 17940...\n",
            "Iteration 17940, Train Loss: 3.1812915802001953 Eval Loss: 3.2587931156158447\n",
            "Running evaluation at iteration 17950...\n",
            "Iteration 17950, Train Loss: 3.1012682914733887 Eval Loss: 3.138026547431946\n",
            "Running evaluation at iteration 17960...\n",
            "Iteration 17960, Train Loss: 3.6001477241516113 Eval Loss: 3.2561227798461916\n",
            "Running evaluation at iteration 17970...\n",
            "Iteration 17970, Train Loss: 3.9382622241973877 Eval Loss: 3.4397433042526244\n",
            "Running evaluation at iteration 17980...\n",
            "Iteration 17980, Train Loss: 3.331088066101074 Eval Loss: 3.437602734565735\n",
            "Running evaluation at iteration 17990...\n",
            "Iteration 17990, Train Loss: 4.097246170043945 Eval Loss: 3.437141990661621\n",
            "Running evaluation at iteration 18000...\n",
            "Iteration 18000, Train Loss: 3.534883737564087 Eval Loss: 3.370125961303711\n",
            "Running evaluation at iteration 18010...\n",
            "Iteration 18010, Train Loss: 3.013493299484253 Eval Loss: 3.430654454231262\n",
            "Running evaluation at iteration 18020...\n",
            "Iteration 18020, Train Loss: 3.230950117111206 Eval Loss: 3.1972777366638185\n",
            "Running evaluation at iteration 18030...\n",
            "Iteration 18030, Train Loss: 3.672830581665039 Eval Loss: 3.31339373588562\n",
            "Running evaluation at iteration 18040...\n",
            "Iteration 18040, Train Loss: 2.9235339164733887 Eval Loss: 3.4968934774398805\n",
            "Running evaluation at iteration 18050...\n",
            "Iteration 18050, Train Loss: 3.401447057723999 Eval Loss: 3.0756983757019043\n",
            "Running evaluation at iteration 18060...\n",
            "Iteration 18060, Train Loss: 3.5730338096618652 Eval Loss: 3.5807140350341795\n",
            "Running evaluation at iteration 18070...\n",
            "Iteration 18070, Train Loss: 3.017789363861084 Eval Loss: 3.3243175029754637\n",
            "Running evaluation at iteration 18080...\n",
            "Iteration 18080, Train Loss: 3.615457057952881 Eval Loss: 3.516107988357544\n",
            "Running evaluation at iteration 18090...\n",
            "Iteration 18090, Train Loss: 3.5894711017608643 Eval Loss: 3.2867387533187866\n",
            "Running evaluation at iteration 18100...\n",
            "Iteration 18100, Train Loss: 3.411527156829834 Eval Loss: 3.5897983074188233\n",
            "Running evaluation at iteration 18110...\n",
            "Iteration 18110, Train Loss: 3.6933107376098633 Eval Loss: 3.260924816131592\n",
            "Running evaluation at iteration 18120...\n",
            "Iteration 18120, Train Loss: 3.3417510986328125 Eval Loss: 3.3795603036880495\n",
            "Running evaluation at iteration 18130...\n",
            "Iteration 18130, Train Loss: 3.7539684772491455 Eval Loss: 3.319840121269226\n",
            "Running evaluation at iteration 18140...\n",
            "Iteration 18140, Train Loss: 3.733139753341675 Eval Loss: 3.2796367406845093\n",
            "Running evaluation at iteration 18150...\n",
            "Iteration 18150, Train Loss: 3.6568541526794434 Eval Loss: 3.504253458976746\n",
            "Running evaluation at iteration 18160...\n",
            "Iteration 18160, Train Loss: 3.204469680786133 Eval Loss: 3.2882058382034303\n",
            "Running evaluation at iteration 18170...\n",
            "Iteration 18170, Train Loss: 3.7324719429016113 Eval Loss: 3.5009029388427733\n",
            "Running evaluation at iteration 18180...\n",
            "Iteration 18180, Train Loss: 3.0835962295532227 Eval Loss: 3.349444794654846\n",
            "Running evaluation at iteration 18190...\n",
            "Iteration 18190, Train Loss: 3.3911123275756836 Eval Loss: 3.483745002746582\n",
            "Running evaluation at iteration 18200...\n",
            "Iteration 18200, Train Loss: 3.6456918716430664 Eval Loss: 3.240840935707092\n",
            "Running evaluation at iteration 18210...\n",
            "Iteration 18210, Train Loss: 3.0062055587768555 Eval Loss: 3.403554916381836\n",
            "Running evaluation at iteration 18220...\n",
            "Iteration 18220, Train Loss: 3.4950647354125977 Eval Loss: 3.422190451622009\n",
            "Running evaluation at iteration 18230...\n",
            "Iteration 18230, Train Loss: 3.441711187362671 Eval Loss: 3.5239838361740112\n",
            "Running evaluation at iteration 18240...\n",
            "Iteration 18240, Train Loss: 3.2360053062438965 Eval Loss: 3.313408923149109\n",
            "Running evaluation at iteration 18250...\n",
            "Iteration 18250, Train Loss: 3.6279239654541016 Eval Loss: 3.3283362865447996\n",
            "Running evaluation at iteration 18260...\n",
            "Iteration 18260, Train Loss: 3.42318058013916 Eval Loss: 3.188118004798889\n",
            "Running evaluation at iteration 18270...\n",
            "Iteration 18270, Train Loss: 3.4475414752960205 Eval Loss: 3.3995417833328245\n",
            "Running evaluation at iteration 18280...\n",
            "Iteration 18280, Train Loss: 3.4523324966430664 Eval Loss: 3.4019467353820803\n",
            "Running evaluation at iteration 18290...\n",
            "Iteration 18290, Train Loss: 3.1294217109680176 Eval Loss: 3.3524463176727295\n",
            "Running evaluation at iteration 18300...\n",
            "Iteration 18300, Train Loss: 3.173832893371582 Eval Loss: 3.250179576873779\n",
            "Running evaluation at iteration 18310...\n",
            "Iteration 18310, Train Loss: 4.127986431121826 Eval Loss: 3.310524845123291\n",
            "Running evaluation at iteration 18320...\n",
            "Iteration 18320, Train Loss: 3.7351245880126953 Eval Loss: 3.5440651178359985\n",
            "Running evaluation at iteration 18330...\n",
            "Iteration 18330, Train Loss: 3.5306992530822754 Eval Loss: 3.542363095283508\n",
            "Running evaluation at iteration 18340...\n",
            "Iteration 18340, Train Loss: 3.058164358139038 Eval Loss: 3.545374941825867\n",
            "Running evaluation at iteration 18350...\n",
            "Iteration 18350, Train Loss: 3.3044216632843018 Eval Loss: 3.445104956626892\n",
            "Running evaluation at iteration 18360...\n",
            "Iteration 18360, Train Loss: 3.297956943511963 Eval Loss: 3.189291763305664\n",
            "Running evaluation at iteration 18370...\n",
            "Iteration 18370, Train Loss: 3.636842966079712 Eval Loss: 3.432278633117676\n",
            "Running evaluation at iteration 18380...\n",
            "Iteration 18380, Train Loss: 4.096601486206055 Eval Loss: 3.350457239151001\n",
            "Running evaluation at iteration 18390...\n",
            "Iteration 18390, Train Loss: 2.8795387744903564 Eval Loss: 3.2802966594696046\n",
            "Running evaluation at iteration 18400...\n",
            "Iteration 18400, Train Loss: 3.543799638748169 Eval Loss: 3.3811456680297853\n",
            "Running evaluation at iteration 18410...\n",
            "Iteration 18410, Train Loss: 3.9023637771606445 Eval Loss: 3.3034605026245116\n",
            "Running evaluation at iteration 18420...\n",
            "Iteration 18420, Train Loss: 3.5573010444641113 Eval Loss: 3.5264792680740356\n",
            "Running evaluation at iteration 18430...\n",
            "Iteration 18430, Train Loss: 3.816389799118042 Eval Loss: 3.4220286130905153\n",
            "Running evaluation at iteration 18440...\n",
            "Iteration 18440, Train Loss: 3.1026105880737305 Eval Loss: 3.520364856719971\n",
            "Running evaluation at iteration 18450...\n",
            "Iteration 18450, Train Loss: 3.6809678077697754 Eval Loss: 3.2265687704086305\n",
            "Running evaluation at iteration 18460...\n",
            "Iteration 18460, Train Loss: 4.119060039520264 Eval Loss: 3.425989580154419\n",
            "Running evaluation at iteration 18470...\n",
            "Iteration 18470, Train Loss: 4.06755256652832 Eval Loss: 3.4821054458618166\n",
            "Running evaluation at iteration 18480...\n",
            "Iteration 18480, Train Loss: 3.9872255325317383 Eval Loss: 3.257465934753418\n",
            "Running evaluation at iteration 18490...\n",
            "Iteration 18490, Train Loss: 3.3665719032287598 Eval Loss: 3.158412456512451\n",
            "Running evaluation at iteration 18500...\n",
            "Iteration 18500, Train Loss: 4.3139238357543945 Eval Loss: 3.4489373683929445\n",
            "Running evaluation at iteration 18510...\n",
            "Iteration 18510, Train Loss: 3.708456516265869 Eval Loss: 3.2577224016189574\n",
            "Running evaluation at iteration 18520...\n",
            "Iteration 18520, Train Loss: 3.6621413230895996 Eval Loss: 3.419035458564758\n",
            "Running evaluation at iteration 18530...\n",
            "Iteration 18530, Train Loss: 3.3318676948547363 Eval Loss: 3.625840735435486\n",
            "Running evaluation at iteration 18540...\n",
            "Iteration 18540, Train Loss: 3.5099053382873535 Eval Loss: 3.5180216312408445\n",
            "Running evaluation at iteration 18550...\n",
            "Iteration 18550, Train Loss: 4.20616340637207 Eval Loss: 3.4415560960769653\n",
            "Running evaluation at iteration 18560...\n",
            "Iteration 18560, Train Loss: 3.7501962184906006 Eval Loss: 3.406857657432556\n",
            "Running evaluation at iteration 18570...\n",
            "Iteration 18570, Train Loss: 3.6978037357330322 Eval Loss: 3.3012975692749023\n",
            "Running evaluation at iteration 18580...\n",
            "Iteration 18580, Train Loss: 3.3103225231170654 Eval Loss: 3.2693695068359374\n",
            "Running evaluation at iteration 18590...\n",
            "Iteration 18590, Train Loss: 3.826894521713257 Eval Loss: 3.1200775384902952\n",
            "Running evaluation at iteration 18600...\n",
            "Iteration 18600, Train Loss: 3.151301622390747 Eval Loss: 3.4441325187683107\n",
            "Running evaluation at iteration 18610...\n",
            "Iteration 18610, Train Loss: 3.2585906982421875 Eval Loss: 3.4383213043212892\n",
            "Running evaluation at iteration 18620...\n",
            "Iteration 18620, Train Loss: 3.1851842403411865 Eval Loss: 3.269811749458313\n",
            "Running evaluation at iteration 18630...\n",
            "Iteration 18630, Train Loss: 3.3503103256225586 Eval Loss: 3.3262281894683836\n",
            "Running evaluation at iteration 18640...\n",
            "Iteration 18640, Train Loss: 3.328914165496826 Eval Loss: 3.4375264167785646\n",
            "Running evaluation at iteration 18650...\n",
            "Iteration 18650, Train Loss: 3.1077632904052734 Eval Loss: 3.453029179573059\n",
            "Running evaluation at iteration 18660...\n",
            "Iteration 18660, Train Loss: 3.8380320072174072 Eval Loss: 3.403574514389038\n",
            "Running evaluation at iteration 18670...\n",
            "Iteration 18670, Train Loss: 3.67527437210083 Eval Loss: 3.399398612976074\n",
            "Running evaluation at iteration 18680...\n",
            "Iteration 18680, Train Loss: 3.4962410926818848 Eval Loss: 3.316733980178833\n",
            "Running evaluation at iteration 18690...\n",
            "Iteration 18690, Train Loss: 3.406716823577881 Eval Loss: 3.3261370420455934\n",
            "Running evaluation at iteration 18700...\n",
            "Iteration 18700, Train Loss: 3.799739360809326 Eval Loss: 3.4475045204162598\n",
            "Running evaluation at iteration 18710...\n",
            "Iteration 18710, Train Loss: 3.348202705383301 Eval Loss: 3.1851470470428467\n",
            "Running evaluation at iteration 18720...\n",
            "Iteration 18720, Train Loss: 3.670114517211914 Eval Loss: 3.4163639545440674\n",
            "Running evaluation at iteration 18730...\n",
            "Iteration 18730, Train Loss: 3.7646231651306152 Eval Loss: 3.4180419206619264\n",
            "Running evaluation at iteration 18740...\n",
            "Iteration 18740, Train Loss: 2.9141082763671875 Eval Loss: 3.6657434463500977\n",
            "Running evaluation at iteration 18750...\n",
            "Iteration 18750, Train Loss: 4.176828384399414 Eval Loss: 3.305681586265564\n",
            "Running evaluation at iteration 18760...\n",
            "Iteration 18760, Train Loss: 3.4284937381744385 Eval Loss: 3.4333203077316283\n",
            "Running evaluation at iteration 18770...\n",
            "Iteration 18770, Train Loss: 4.540731906890869 Eval Loss: 3.2414867162704466\n",
            "Running evaluation at iteration 18780...\n",
            "Iteration 18780, Train Loss: 3.1412386894226074 Eval Loss: 3.3355653524398803\n",
            "Running evaluation at iteration 18790...\n",
            "Iteration 18790, Train Loss: 3.8333704471588135 Eval Loss: 3.2777106761932373\n",
            "Running evaluation at iteration 18800...\n",
            "Iteration 18800, Train Loss: 2.7843430042266846 Eval Loss: 3.375652313232422\n",
            "Running evaluation at iteration 18810...\n",
            "Iteration 18810, Train Loss: 3.6167516708374023 Eval Loss: 3.230089259147644\n",
            "Running evaluation at iteration 18820...\n",
            "Iteration 18820, Train Loss: 3.7095468044281006 Eval Loss: 3.483525776863098\n",
            "Running evaluation at iteration 18830...\n",
            "Iteration 18830, Train Loss: 3.8785789012908936 Eval Loss: 3.382960057258606\n",
            "Running evaluation at iteration 18840...\n",
            "Iteration 18840, Train Loss: 3.636525869369507 Eval Loss: 3.1687512397766113\n",
            "Running evaluation at iteration 18850...\n",
            "Iteration 18850, Train Loss: 4.060286998748779 Eval Loss: 3.355170488357544\n",
            "Running evaluation at iteration 18860...\n",
            "Iteration 18860, Train Loss: 3.55281400680542 Eval Loss: 3.1798325300216677\n",
            "Running evaluation at iteration 18870...\n",
            "Iteration 18870, Train Loss: 3.8460519313812256 Eval Loss: 3.4067434072494507\n",
            "Running evaluation at iteration 18880...\n",
            "Iteration 18880, Train Loss: 3.257591962814331 Eval Loss: 3.5011376619338987\n",
            "Running evaluation at iteration 18890...\n",
            "Iteration 18890, Train Loss: 3.603470802307129 Eval Loss: 3.2973518133163453\n",
            "Running evaluation at iteration 18900...\n",
            "Iteration 18900, Train Loss: 3.1439905166625977 Eval Loss: 3.394235873222351\n",
            "Running evaluation at iteration 18910...\n",
            "Iteration 18910, Train Loss: 4.077579498291016 Eval Loss: 3.3470263481140137\n",
            "Running evaluation at iteration 18920...\n",
            "Iteration 18920, Train Loss: 2.7665648460388184 Eval Loss: 3.4912034273147583\n",
            "Running evaluation at iteration 18930...\n",
            "Iteration 18930, Train Loss: 2.8983263969421387 Eval Loss: 3.450772833824158\n",
            "Running evaluation at iteration 18940...\n",
            "Iteration 18940, Train Loss: 3.7845706939697266 Eval Loss: 3.3953381299972536\n",
            "Running evaluation at iteration 18950...\n",
            "Iteration 18950, Train Loss: 3.7237462997436523 Eval Loss: 3.30584397315979\n",
            "Running evaluation at iteration 18960...\n",
            "Iteration 18960, Train Loss: 3.2120766639709473 Eval Loss: 3.304937481880188\n",
            "Running evaluation at iteration 18970...\n",
            "Iteration 18970, Train Loss: 3.5276424884796143 Eval Loss: 3.280588221549988\n",
            "Running evaluation at iteration 18980...\n",
            "Iteration 18980, Train Loss: 3.0136990547180176 Eval Loss: 3.302759051322937\n",
            "Running evaluation at iteration 18990...\n",
            "Iteration 18990, Train Loss: 4.043465614318848 Eval Loss: 3.352819561958313\n",
            "Running evaluation at iteration 19000...\n",
            "Iteration 19000, Train Loss: 3.835348129272461 Eval Loss: 3.4880597591400146\n",
            "Running evaluation at iteration 19010...\n",
            "Iteration 19010, Train Loss: 3.720742702484131 Eval Loss: 3.3059168577194216\n",
            "Running evaluation at iteration 19020...\n",
            "Iteration 19020, Train Loss: 3.594167709350586 Eval Loss: 3.153447985649109\n",
            "Running evaluation at iteration 19030...\n",
            "Iteration 19030, Train Loss: 3.210930824279785 Eval Loss: 3.121047043800354\n",
            "Running evaluation at iteration 19040...\n",
            "Iteration 19040, Train Loss: 2.702166795730591 Eval Loss: 3.5316413402557374\n",
            "Running evaluation at iteration 19050...\n",
            "Iteration 19050, Train Loss: 3.007300615310669 Eval Loss: 3.3236382484436033\n",
            "Running evaluation at iteration 19060...\n",
            "Iteration 19060, Train Loss: 3.115964889526367 Eval Loss: 3.369385552406311\n",
            "Running evaluation at iteration 19070...\n",
            "Iteration 19070, Train Loss: 3.2476308345794678 Eval Loss: 3.245820641517639\n",
            "Running evaluation at iteration 19080...\n",
            "Iteration 19080, Train Loss: 4.28967809677124 Eval Loss: 3.430878210067749\n",
            "Running evaluation at iteration 19090...\n",
            "Iteration 19090, Train Loss: 4.062628269195557 Eval Loss: 3.2345948934555055\n",
            "Running evaluation at iteration 19100...\n",
            "Iteration 19100, Train Loss: 3.890104293823242 Eval Loss: 3.354554796218872\n",
            "Running evaluation at iteration 19110...\n",
            "Iteration 19110, Train Loss: 3.251843214035034 Eval Loss: 3.326407957077026\n",
            "Running evaluation at iteration 19120...\n",
            "Iteration 19120, Train Loss: 3.4041907787323 Eval Loss: 3.3890340089797975\n",
            "Running evaluation at iteration 19130...\n",
            "Iteration 19130, Train Loss: 3.2741763591766357 Eval Loss: 3.2548914432525633\n",
            "Running evaluation at iteration 19140...\n",
            "Iteration 19140, Train Loss: 2.9741907119750977 Eval Loss: 3.5651365756988525\n",
            "Running evaluation at iteration 19150...\n",
            "Iteration 19150, Train Loss: 3.490213394165039 Eval Loss: 3.158274531364441\n",
            "Running evaluation at iteration 19160...\n",
            "Iteration 19160, Train Loss: 3.7224607467651367 Eval Loss: 3.31523118019104\n",
            "Running evaluation at iteration 19170...\n",
            "Iteration 19170, Train Loss: 3.5967867374420166 Eval Loss: 3.4483397006988525\n",
            "Running evaluation at iteration 19180...\n",
            "Iteration 19180, Train Loss: 3.3598766326904297 Eval Loss: 3.3989593744277955\n",
            "Running evaluation at iteration 19190...\n",
            "Iteration 19190, Train Loss: 3.876516819000244 Eval Loss: 3.299338388442993\n",
            "Running evaluation at iteration 19200...\n",
            "Iteration 19200, Train Loss: 3.463120222091675 Eval Loss: 3.2927590131759645\n",
            "Running evaluation at iteration 19210...\n",
            "Iteration 19210, Train Loss: 3.7293448448181152 Eval Loss: 3.3681843280792236\n",
            "Running evaluation at iteration 19220...\n",
            "Iteration 19220, Train Loss: 3.1703760623931885 Eval Loss: 3.4425524711608886\n",
            "Running evaluation at iteration 19230...\n",
            "Iteration 19230, Train Loss: 2.5274057388305664 Eval Loss: 3.4372142791748046\n",
            "Running evaluation at iteration 19240...\n",
            "Iteration 19240, Train Loss: 3.6418752670288086 Eval Loss: 3.2312819957733154\n",
            "Running evaluation at iteration 19250...\n",
            "Iteration 19250, Train Loss: 3.5200490951538086 Eval Loss: 3.4308457136154176\n",
            "Running evaluation at iteration 19260...\n",
            "Iteration 19260, Train Loss: 3.8495092391967773 Eval Loss: 3.423959517478943\n",
            "Running evaluation at iteration 19270...\n",
            "Iteration 19270, Train Loss: 3.6776015758514404 Eval Loss: 3.217362904548645\n",
            "Running evaluation at iteration 19280...\n",
            "Iteration 19280, Train Loss: 3.798797369003296 Eval Loss: 3.606743836402893\n",
            "Running evaluation at iteration 19290...\n",
            "Iteration 19290, Train Loss: 3.2998404502868652 Eval Loss: 3.3380536794662476\n",
            "Running evaluation at iteration 19300...\n",
            "Iteration 19300, Train Loss: 3.4733657836914062 Eval Loss: 3.559528946876526\n",
            "Running evaluation at iteration 19310...\n",
            "Iteration 19310, Train Loss: 3.435236930847168 Eval Loss: 3.2146268129348754\n",
            "Running evaluation at iteration 19320...\n",
            "Iteration 19320, Train Loss: 3.1640307903289795 Eval Loss: 3.301816534996033\n",
            "Running evaluation at iteration 19330...\n",
            "Iteration 19330, Train Loss: 4.280758380889893 Eval Loss: 3.3324930906295775\n",
            "Running evaluation at iteration 19340...\n",
            "Iteration 19340, Train Loss: 3.7925221920013428 Eval Loss: 3.5362436294555666\n",
            "Running evaluation at iteration 19350...\n",
            "Iteration 19350, Train Loss: 3.5975615978240967 Eval Loss: 3.636859917640686\n",
            "Running evaluation at iteration 19360...\n",
            "Iteration 19360, Train Loss: 3.716339111328125 Eval Loss: 3.3774920463562013\n",
            "Running evaluation at iteration 19370...\n",
            "Iteration 19370, Train Loss: 3.2954578399658203 Eval Loss: 3.1487112045288086\n",
            "Running evaluation at iteration 19380...\n",
            "Iteration 19380, Train Loss: 2.9664101600646973 Eval Loss: 3.4421494722366335\n",
            "Running evaluation at iteration 19390...\n",
            "Iteration 19390, Train Loss: 3.9975931644439697 Eval Loss: 3.50441198348999\n",
            "Running evaluation at iteration 19400...\n",
            "Iteration 19400, Train Loss: 3.2700202465057373 Eval Loss: 3.1651764869689942\n",
            "Running evaluation at iteration 19410...\n",
            "Iteration 19410, Train Loss: 3.2427027225494385 Eval Loss: 3.474363851547241\n",
            "Running evaluation at iteration 19420...\n",
            "Iteration 19420, Train Loss: 3.4740684032440186 Eval Loss: 3.417112636566162\n",
            "Running evaluation at iteration 19430...\n",
            "Iteration 19430, Train Loss: 3.925130844116211 Eval Loss: 3.3043625831604\n",
            "Running evaluation at iteration 19440...\n",
            "Iteration 19440, Train Loss: 3.3592042922973633 Eval Loss: 3.3014580249786376\n",
            "Running evaluation at iteration 19450...\n",
            "Iteration 19450, Train Loss: 4.199491500854492 Eval Loss: 3.413690185546875\n",
            "Running evaluation at iteration 19460...\n",
            "Iteration 19460, Train Loss: 3.2294836044311523 Eval Loss: 3.3233834743499755\n",
            "Running evaluation at iteration 19470...\n",
            "Iteration 19470, Train Loss: 3.8270463943481445 Eval Loss: 3.413416337966919\n",
            "Running evaluation at iteration 19480...\n",
            "Iteration 19480, Train Loss: 2.989567756652832 Eval Loss: 3.430218744277954\n",
            "Running evaluation at iteration 19490...\n",
            "Iteration 19490, Train Loss: 3.0807669162750244 Eval Loss: 3.5287389278411867\n",
            "Running evaluation at iteration 19500...\n",
            "Iteration 19500, Train Loss: 3.8022449016571045 Eval Loss: 3.4935836791992188\n",
            "Running evaluation at iteration 19510...\n",
            "Iteration 19510, Train Loss: 3.5067598819732666 Eval Loss: 3.2615562438964845\n",
            "Running evaluation at iteration 19520...\n",
            "Iteration 19520, Train Loss: 3.9585604667663574 Eval Loss: 3.413456177711487\n",
            "Running evaluation at iteration 19530...\n",
            "Iteration 19530, Train Loss: 3.4810564517974854 Eval Loss: 3.3339709043502808\n",
            "Running evaluation at iteration 19540...\n",
            "Iteration 19540, Train Loss: 3.099151372909546 Eval Loss: 3.2976869106292725\n",
            "Running evaluation at iteration 19550...\n",
            "Iteration 19550, Train Loss: 3.7604384422302246 Eval Loss: 3.494450235366821\n",
            "Running evaluation at iteration 19560...\n",
            "Iteration 19560, Train Loss: 4.171511650085449 Eval Loss: 3.3537514448165893\n",
            "Running evaluation at iteration 19570...\n",
            "Iteration 19570, Train Loss: 3.450778007507324 Eval Loss: 3.296030616760254\n",
            "Running evaluation at iteration 19580...\n",
            "Iteration 19580, Train Loss: 3.2378363609313965 Eval Loss: 3.229654550552368\n",
            "Running evaluation at iteration 19590...\n",
            "Iteration 19590, Train Loss: 3.345438003540039 Eval Loss: 3.2363404989242555\n",
            "Running evaluation at iteration 19600...\n",
            "Iteration 19600, Train Loss: 3.2120325565338135 Eval Loss: 3.440813159942627\n",
            "Running evaluation at iteration 19610...\n",
            "Iteration 19610, Train Loss: 2.998474359512329 Eval Loss: 3.258608078956604\n",
            "Running evaluation at iteration 19620...\n",
            "Iteration 19620, Train Loss: 3.4662997722625732 Eval Loss: 3.3479347705841063\n",
            "Running evaluation at iteration 19630...\n",
            "Iteration 19630, Train Loss: 3.85567569732666 Eval Loss: 3.142859196662903\n",
            "Running evaluation at iteration 19640...\n",
            "Iteration 19640, Train Loss: 3.5940260887145996 Eval Loss: 3.5609057188034057\n",
            "Running evaluation at iteration 19650...\n",
            "Iteration 19650, Train Loss: 4.3029398918151855 Eval Loss: 3.3139978885650634\n",
            "Running evaluation at iteration 19660...\n",
            "Iteration 19660, Train Loss: 3.6010172367095947 Eval Loss: 3.332796001434326\n",
            "Running evaluation at iteration 19670...\n",
            "Iteration 19670, Train Loss: 3.2138261795043945 Eval Loss: 3.0377124547958374\n",
            "Running evaluation at iteration 19680...\n",
            "Iteration 19680, Train Loss: 3.657151699066162 Eval Loss: 3.6242082357406615\n",
            "Running evaluation at iteration 19690...\n",
            "Iteration 19690, Train Loss: 3.554927110671997 Eval Loss: 3.387117052078247\n",
            "Running evaluation at iteration 19700...\n",
            "Iteration 19700, Train Loss: 3.291188955307007 Eval Loss: 3.4154902935028075\n",
            "Running evaluation at iteration 19710...\n",
            "Iteration 19710, Train Loss: 3.7210745811462402 Eval Loss: 3.2761549234390257\n",
            "Running evaluation at iteration 19720...\n",
            "Iteration 19720, Train Loss: 3.2845354080200195 Eval Loss: 3.4059902906417845\n",
            "Running evaluation at iteration 19730...\n",
            "Iteration 19730, Train Loss: 3.5101428031921387 Eval Loss: 3.280418133735657\n",
            "Running evaluation at iteration 19740...\n",
            "Iteration 19740, Train Loss: 3.1447741985321045 Eval Loss: 3.474780774116516\n",
            "Running evaluation at iteration 19750...\n",
            "Iteration 19750, Train Loss: 3.9792468547821045 Eval Loss: 3.1836785078048706\n",
            "Running evaluation at iteration 19760...\n",
            "Iteration 19760, Train Loss: 3.879423141479492 Eval Loss: 3.44575514793396\n",
            "Running evaluation at iteration 19770...\n",
            "Iteration 19770, Train Loss: 3.3511552810668945 Eval Loss: 3.3100538730621336\n",
            "Running evaluation at iteration 19780...\n",
            "Iteration 19780, Train Loss: 3.342721939086914 Eval Loss: 3.424930953979492\n",
            "Running evaluation at iteration 19790...\n",
            "Iteration 19790, Train Loss: 4.5539703369140625 Eval Loss: 3.3913694858551025\n",
            "Running evaluation at iteration 19800...\n",
            "Iteration 19800, Train Loss: 3.6106133460998535 Eval Loss: 3.3870396852493285\n",
            "Running evaluation at iteration 19810...\n",
            "Iteration 19810, Train Loss: 3.3149008750915527 Eval Loss: 3.41192512512207\n",
            "Running evaluation at iteration 19820...\n",
            "Iteration 19820, Train Loss: 3.358426094055176 Eval Loss: 3.587655544281006\n",
            "Running evaluation at iteration 19830...\n",
            "Iteration 19830, Train Loss: 3.2353463172912598 Eval Loss: 3.089974021911621\n",
            "Running evaluation at iteration 19840...\n",
            "Iteration 19840, Train Loss: 3.7246222496032715 Eval Loss: 3.4665319442749025\n",
            "Running evaluation at iteration 19850...\n",
            "Iteration 19850, Train Loss: 3.5615367889404297 Eval Loss: 3.511473822593689\n",
            "Running evaluation at iteration 19860...\n",
            "Iteration 19860, Train Loss: 3.1777734756469727 Eval Loss: 3.4302931785583497\n",
            "Running evaluation at iteration 19870...\n",
            "Iteration 19870, Train Loss: 3.130403995513916 Eval Loss: 3.2737290143966673\n",
            "Running evaluation at iteration 19880...\n",
            "Iteration 19880, Train Loss: 3.5393378734588623 Eval Loss: 3.173550319671631\n",
            "Running evaluation at iteration 19890...\n",
            "Iteration 19890, Train Loss: 3.5457763671875 Eval Loss: 3.209763836860657\n",
            "Running evaluation at iteration 19900...\n",
            "Iteration 19900, Train Loss: 3.799923896789551 Eval Loss: 3.110822057723999\n",
            "Running evaluation at iteration 19910...\n",
            "Iteration 19910, Train Loss: 3.054521083831787 Eval Loss: 3.336955261230469\n",
            "Running evaluation at iteration 19920...\n",
            "Iteration 19920, Train Loss: 3.2881884574890137 Eval Loss: 3.5558931112289427\n",
            "Running evaluation at iteration 19930...\n",
            "Iteration 19930, Train Loss: 3.32858943939209 Eval Loss: 3.2389267683029175\n",
            "Running evaluation at iteration 19940...\n",
            "Iteration 19940, Train Loss: 3.4058399200439453 Eval Loss: 3.2547894954681396\n",
            "Running evaluation at iteration 19950...\n",
            "Iteration 19950, Train Loss: 3.3020687103271484 Eval Loss: 3.4927684545516966\n",
            "Running evaluation at iteration 19960...\n",
            "Iteration 19960, Train Loss: 3.498274564743042 Eval Loss: 3.551654052734375\n",
            "Running evaluation at iteration 19970...\n",
            "Iteration 19970, Train Loss: 3.082064151763916 Eval Loss: 3.4510626077651976\n",
            "Running evaluation at iteration 19980...\n",
            "Iteration 19980, Train Loss: 3.4213874340057373 Eval Loss: 3.336086964607239\n",
            "Running evaluation at iteration 19990...\n",
            "Iteration 19990, Train Loss: 4.375720977783203 Eval Loss: 3.270148181915283\n",
            "Running evaluation at iteration 20000...\n",
            "Iteration 20000, Train Loss: 3.547534704208374 Eval Loss: 3.432755637168884\n",
            "Running evaluation at iteration 20010...\n",
            "Iteration 20010, Train Loss: 3.2697010040283203 Eval Loss: 3.4556637525558473\n",
            "Running evaluation at iteration 20020...\n",
            "Iteration 20020, Train Loss: 3.482405424118042 Eval Loss: 3.22475745677948\n",
            "Running evaluation at iteration 20030...\n",
            "Iteration 20030, Train Loss: 3.417843818664551 Eval Loss: 3.2113353490829466\n",
            "Running evaluation at iteration 20040...\n",
            "Iteration 20040, Train Loss: 3.2752323150634766 Eval Loss: 3.285481572151184\n",
            "Running evaluation at iteration 20050...\n",
            "Iteration 20050, Train Loss: 3.524233341217041 Eval Loss: 3.2821887493133546\n",
            "Running evaluation at iteration 20060...\n",
            "Iteration 20060, Train Loss: 3.7505404949188232 Eval Loss: 3.422355127334595\n",
            "Running evaluation at iteration 20070...\n",
            "Iteration 20070, Train Loss: 3.6407711505889893 Eval Loss: 3.4845782279968263\n",
            "Running evaluation at iteration 20080...\n",
            "Iteration 20080, Train Loss: 2.662724494934082 Eval Loss: 3.2623546600341795\n",
            "Running evaluation at iteration 20090...\n",
            "Iteration 20090, Train Loss: 3.2135651111602783 Eval Loss: 3.2320288181304933\n",
            "Running evaluation at iteration 20100...\n",
            "Iteration 20100, Train Loss: 3.37263822555542 Eval Loss: 3.337147283554077\n",
            "Running evaluation at iteration 20110...\n",
            "Iteration 20110, Train Loss: 3.3488409519195557 Eval Loss: 3.4475069284439086\n",
            "Running evaluation at iteration 20120...\n",
            "Iteration 20120, Train Loss: 3.5552213191986084 Eval Loss: 3.2962631225585937\n",
            "Running evaluation at iteration 20130...\n",
            "Iteration 20130, Train Loss: 4.195237636566162 Eval Loss: 3.2252731800079344\n",
            "Running evaluation at iteration 20140...\n",
            "Iteration 20140, Train Loss: 3.212221622467041 Eval Loss: 3.354950737953186\n",
            "Running evaluation at iteration 20150...\n",
            "Iteration 20150, Train Loss: 3.3277056217193604 Eval Loss: 3.4749158143997194\n",
            "Running evaluation at iteration 20160...\n",
            "Iteration 20160, Train Loss: 3.201840400695801 Eval Loss: 3.434714508056641\n",
            "Running evaluation at iteration 20170...\n",
            "Iteration 20170, Train Loss: 2.8730385303497314 Eval Loss: 3.5511847734451294\n",
            "Running evaluation at iteration 20180...\n",
            "Iteration 20180, Train Loss: 3.149819850921631 Eval Loss: 3.3403307437896728\n",
            "Running evaluation at iteration 20190...\n",
            "Iteration 20190, Train Loss: 3.4376893043518066 Eval Loss: 3.4003458499908445\n",
            "Running evaluation at iteration 20200...\n",
            "Iteration 20200, Train Loss: 3.4546048641204834 Eval Loss: 3.517684054374695\n",
            "Running evaluation at iteration 20210...\n",
            "Iteration 20210, Train Loss: 2.942495346069336 Eval Loss: 3.386469507217407\n",
            "Running evaluation at iteration 20220...\n",
            "Iteration 20220, Train Loss: 3.4586408138275146 Eval Loss: 3.5541133642196656\n",
            "Running evaluation at iteration 20230...\n",
            "Iteration 20230, Train Loss: 3.4600679874420166 Eval Loss: 3.254695200920105\n",
            "Running evaluation at iteration 20240...\n",
            "Iteration 20240, Train Loss: 3.311505079269409 Eval Loss: 3.431773233413696\n",
            "Running evaluation at iteration 20250...\n",
            "Iteration 20250, Train Loss: 3.121349811553955 Eval Loss: 3.195471739768982\n",
            "Running evaluation at iteration 20260...\n",
            "Iteration 20260, Train Loss: 3.4274110794067383 Eval Loss: 3.3869770765304565\n",
            "Running evaluation at iteration 20270...\n",
            "Iteration 20270, Train Loss: 3.5539770126342773 Eval Loss: 3.267987513542175\n",
            "Running evaluation at iteration 20280...\n",
            "Iteration 20280, Train Loss: 3.200960636138916 Eval Loss: 3.1862135410308836\n",
            "Running evaluation at iteration 20290...\n",
            "Iteration 20290, Train Loss: 3.7888333797454834 Eval Loss: 3.295678639411926\n",
            "Running evaluation at iteration 20300...\n",
            "Iteration 20300, Train Loss: 3.5776305198669434 Eval Loss: 3.5095521926879885\n",
            "Running evaluation at iteration 20310...\n",
            "Iteration 20310, Train Loss: 3.8086133003234863 Eval Loss: 3.2269689559936525\n",
            "Running evaluation at iteration 20320...\n",
            "Iteration 20320, Train Loss: 3.3840019702911377 Eval Loss: 3.30378315448761\n",
            "Running evaluation at iteration 20330...\n",
            "Iteration 20330, Train Loss: 3.407092332839966 Eval Loss: 3.431582546234131\n",
            "Running evaluation at iteration 20340...\n",
            "Iteration 20340, Train Loss: 3.966562509536743 Eval Loss: 3.3775640964508056\n",
            "Running evaluation at iteration 20350...\n",
            "Iteration 20350, Train Loss: 3.189845085144043 Eval Loss: 3.499638867378235\n",
            "Running evaluation at iteration 20360...\n",
            "Iteration 20360, Train Loss: 3.330235004425049 Eval Loss: 3.4023113012313844\n",
            "Running evaluation at iteration 20370...\n",
            "Iteration 20370, Train Loss: 2.9913864135742188 Eval Loss: 3.3044206380844114\n",
            "Running evaluation at iteration 20380...\n",
            "Iteration 20380, Train Loss: 2.7616443634033203 Eval Loss: 3.2946624040603636\n",
            "Running evaluation at iteration 20390...\n",
            "Iteration 20390, Train Loss: 3.309185743331909 Eval Loss: 3.1898935794830323\n",
            "Running evaluation at iteration 20400...\n",
            "Iteration 20400, Train Loss: 3.7577197551727295 Eval Loss: 3.1603793621063234\n",
            "Running evaluation at iteration 20410...\n",
            "Iteration 20410, Train Loss: 2.8273730278015137 Eval Loss: 3.3662296295166017\n",
            "Running evaluation at iteration 20420...\n",
            "Iteration 20420, Train Loss: 2.918412208557129 Eval Loss: 3.3637951374053956\n",
            "Running evaluation at iteration 20430...\n",
            "Iteration 20430, Train Loss: 3.7764227390289307 Eval Loss: 3.3920112371444704\n",
            "Running evaluation at iteration 20440...\n",
            "Iteration 20440, Train Loss: 3.3383123874664307 Eval Loss: 3.458718109130859\n",
            "Running evaluation at iteration 20450...\n",
            "Iteration 20450, Train Loss: 3.2969651222229004 Eval Loss: 3.287553811073303\n",
            "Running evaluation at iteration 20460...\n",
            "Iteration 20460, Train Loss: 3.6437673568725586 Eval Loss: 3.35291268825531\n",
            "Running evaluation at iteration 20470...\n",
            "Iteration 20470, Train Loss: 2.8987479209899902 Eval Loss: 3.2868220806121826\n",
            "Running evaluation at iteration 20480...\n",
            "Iteration 20480, Train Loss: 3.568305253982544 Eval Loss: 3.2541404247283934\n",
            "Running evaluation at iteration 20490...\n",
            "Iteration 20490, Train Loss: 3.6380956172943115 Eval Loss: 3.3085240840911867\n",
            "Running evaluation at iteration 20500...\n",
            "Iteration 20500, Train Loss: 4.418294906616211 Eval Loss: 3.245063304901123\n",
            "Running evaluation at iteration 20510...\n",
            "Iteration 20510, Train Loss: 4.102632999420166 Eval Loss: 3.168656325340271\n",
            "Running evaluation at iteration 20520...\n",
            "Iteration 20520, Train Loss: 2.6504156589508057 Eval Loss: 3.2729864835739138\n",
            "Running evaluation at iteration 20530...\n",
            "Iteration 20530, Train Loss: 3.54008412361145 Eval Loss: 3.2986199378967287\n",
            "Running evaluation at iteration 20540...\n",
            "Iteration 20540, Train Loss: 3.4385814666748047 Eval Loss: 3.373276114463806\n",
            "Running evaluation at iteration 20550...\n",
            "Iteration 20550, Train Loss: 3.447890281677246 Eval Loss: 3.5844083786010743\n",
            "Running evaluation at iteration 20560...\n",
            "Iteration 20560, Train Loss: 3.3759195804595947 Eval Loss: 3.3941107273101805\n",
            "Running evaluation at iteration 20570...\n",
            "Iteration 20570, Train Loss: 4.100936412811279 Eval Loss: 3.340267467498779\n",
            "Running evaluation at iteration 20580...\n",
            "Iteration 20580, Train Loss: 3.917360782623291 Eval Loss: 3.2078809261322023\n",
            "Running evaluation at iteration 20590...\n",
            "Iteration 20590, Train Loss: 3.697360038757324 Eval Loss: 3.316878390312195\n",
            "Running evaluation at iteration 20600...\n",
            "Iteration 20600, Train Loss: 3.3523497581481934 Eval Loss: 3.200076770782471\n",
            "Running evaluation at iteration 20610...\n",
            "Iteration 20610, Train Loss: 3.558196783065796 Eval Loss: 3.17622971534729\n",
            "Running evaluation at iteration 20620...\n",
            "Iteration 20620, Train Loss: 3.608593463897705 Eval Loss: 3.5783481359481812\n",
            "Running evaluation at iteration 20630...\n",
            "Iteration 20630, Train Loss: 3.206294298171997 Eval Loss: 3.3875375270843504\n",
            "Running evaluation at iteration 20640...\n",
            "Iteration 20640, Train Loss: 3.6559195518493652 Eval Loss: 3.20204553604126\n",
            "Running evaluation at iteration 20650...\n",
            "Iteration 20650, Train Loss: 3.5755553245544434 Eval Loss: 3.3947779893875123\n",
            "Running evaluation at iteration 20660...\n",
            "Iteration 20660, Train Loss: 3.479780912399292 Eval Loss: 3.325620007514954\n",
            "Running evaluation at iteration 20670...\n",
            "Iteration 20670, Train Loss: 3.5547568798065186 Eval Loss: 3.512401747703552\n",
            "Running evaluation at iteration 20680...\n",
            "Iteration 20680, Train Loss: 3.1467559337615967 Eval Loss: 3.269490456581116\n",
            "Running evaluation at iteration 20690...\n",
            "Iteration 20690, Train Loss: 4.215433120727539 Eval Loss: 3.1835978746414186\n",
            "Running evaluation at iteration 20700...\n",
            "Iteration 20700, Train Loss: 3.8647420406341553 Eval Loss: 3.304726219177246\n",
            "Running evaluation at iteration 20710...\n",
            "Iteration 20710, Train Loss: 2.921558141708374 Eval Loss: 3.162434959411621\n",
            "Running evaluation at iteration 20720...\n",
            "Iteration 20720, Train Loss: 2.8042643070220947 Eval Loss: 3.3574568748474123\n",
            "Running evaluation at iteration 20730...\n",
            "Iteration 20730, Train Loss: 3.554689407348633 Eval Loss: 3.140720844268799\n",
            "Running evaluation at iteration 20740...\n",
            "Iteration 20740, Train Loss: 3.141688823699951 Eval Loss: 3.217901086807251\n",
            "Running evaluation at iteration 20750...\n",
            "Iteration 20750, Train Loss: 3.20626163482666 Eval Loss: 3.180356574058533\n",
            "Running evaluation at iteration 20760...\n",
            "Iteration 20760, Train Loss: 3.218752861022949 Eval Loss: 3.4402558326721193\n",
            "Running evaluation at iteration 20770...\n",
            "Iteration 20770, Train Loss: 3.344775438308716 Eval Loss: 3.455216336250305\n",
            "Running evaluation at iteration 20780...\n",
            "Iteration 20780, Train Loss: 3.0056543350219727 Eval Loss: 3.138364481925964\n",
            "Running evaluation at iteration 20790...\n",
            "Iteration 20790, Train Loss: 3.5274808406829834 Eval Loss: 3.3950021028518678\n",
            "Running evaluation at iteration 20800...\n",
            "Iteration 20800, Train Loss: 3.137570858001709 Eval Loss: 3.2376632690429688\n",
            "Running evaluation at iteration 20810...\n",
            "Iteration 20810, Train Loss: 3.227353572845459 Eval Loss: 3.3176392793655394\n",
            "Running evaluation at iteration 20820...\n",
            "Iteration 20820, Train Loss: 3.3328142166137695 Eval Loss: 3.336703085899353\n",
            "Running evaluation at iteration 20830...\n",
            "Iteration 20830, Train Loss: 3.4678795337677 Eval Loss: 3.3368737936019897\n",
            "Running evaluation at iteration 20840...\n",
            "Iteration 20840, Train Loss: 3.2537081241607666 Eval Loss: 3.533732295036316\n",
            "Running evaluation at iteration 20850...\n",
            "Iteration 20850, Train Loss: 3.3391191959381104 Eval Loss: 3.3608683586120605\n",
            "Running evaluation at iteration 20860...\n",
            "Iteration 20860, Train Loss: 3.2550432682037354 Eval Loss: 3.2531949043273927\n",
            "Running evaluation at iteration 20870...\n",
            "Iteration 20870, Train Loss: 3.250610589981079 Eval Loss: 3.327378821372986\n",
            "Running evaluation at iteration 20880...\n",
            "Iteration 20880, Train Loss: 3.5126616954803467 Eval Loss: 3.407092332839966\n",
            "Running evaluation at iteration 20890...\n",
            "Iteration 20890, Train Loss: 3.397251605987549 Eval Loss: 3.3535075902938845\n",
            "Running evaluation at iteration 20900...\n",
            "Iteration 20900, Train Loss: 3.2439610958099365 Eval Loss: 3.4408453702926636\n",
            "Running evaluation at iteration 20910...\n",
            "Iteration 20910, Train Loss: 2.715367078781128 Eval Loss: 3.322867178916931\n",
            "Running evaluation at iteration 20920...\n",
            "Iteration 20920, Train Loss: 3.3449888229370117 Eval Loss: 3.406071972846985\n",
            "Running evaluation at iteration 20930...\n",
            "Iteration 20930, Train Loss: 3.373849868774414 Eval Loss: 3.110128617286682\n",
            "Running evaluation at iteration 20940...\n",
            "Iteration 20940, Train Loss: 3.6830978393554688 Eval Loss: 3.4190297603607176\n",
            "Running evaluation at iteration 20950...\n",
            "Iteration 20950, Train Loss: 3.7247726917266846 Eval Loss: 3.3494857788085937\n",
            "Running evaluation at iteration 20960...\n",
            "Iteration 20960, Train Loss: 3.372819185256958 Eval Loss: 3.2134727239608765\n",
            "Running evaluation at iteration 20970...\n",
            "Iteration 20970, Train Loss: 3.213780403137207 Eval Loss: 3.2268334150314333\n",
            "Running evaluation at iteration 20980...\n",
            "Iteration 20980, Train Loss: 2.6944751739501953 Eval Loss: 3.3678887605667116\n",
            "Running evaluation at iteration 20990...\n",
            "Iteration 20990, Train Loss: 4.2384934425354 Eval Loss: 3.2075734615325926\n",
            "Running evaluation at iteration 21000...\n",
            "Iteration 21000, Train Loss: 3.859088182449341 Eval Loss: 3.2451531410217287\n",
            "Running evaluation at iteration 21010...\n",
            "Iteration 21010, Train Loss: 2.63364839553833 Eval Loss: 3.171583819389343\n",
            "Running evaluation at iteration 21020...\n",
            "Iteration 21020, Train Loss: 4.230803966522217 Eval Loss: 3.312021541595459\n",
            "Running evaluation at iteration 21030...\n",
            "Iteration 21030, Train Loss: 2.999568223953247 Eval Loss: 3.2765239000320436\n",
            "Running evaluation at iteration 21040...\n",
            "Iteration 21040, Train Loss: 2.763496160507202 Eval Loss: 3.347727870941162\n",
            "Running evaluation at iteration 21050...\n",
            "Iteration 21050, Train Loss: 3.7023937702178955 Eval Loss: 3.2623300313949586\n",
            "Running evaluation at iteration 21060...\n",
            "Iteration 21060, Train Loss: 3.457721471786499 Eval Loss: 3.172234034538269\n",
            "Running evaluation at iteration 21070...\n",
            "Iteration 21070, Train Loss: 2.9485387802124023 Eval Loss: 3.282954716682434\n",
            "Running evaluation at iteration 21080...\n",
            "Iteration 21080, Train Loss: 4.250982284545898 Eval Loss: 3.371081256866455\n",
            "Running evaluation at iteration 21090...\n",
            "Iteration 21090, Train Loss: 3.707684278488159 Eval Loss: 3.3150213956832886\n",
            "Running evaluation at iteration 21100...\n",
            "Iteration 21100, Train Loss: 3.622745990753174 Eval Loss: 3.4874919176101686\n",
            "Running evaluation at iteration 21110...\n",
            "Iteration 21110, Train Loss: 3.3793725967407227 Eval Loss: 3.5636081218719484\n",
            "Running evaluation at iteration 21120...\n",
            "Iteration 21120, Train Loss: 3.1731886863708496 Eval Loss: 3.2495545864105226\n",
            "Running evaluation at iteration 21130...\n",
            "Iteration 21130, Train Loss: 3.4263253211975098 Eval Loss: 3.3625790596008303\n",
            "Running evaluation at iteration 21140...\n",
            "Iteration 21140, Train Loss: 3.575300693511963 Eval Loss: 3.421596884727478\n",
            "Running evaluation at iteration 21150...\n",
            "Iteration 21150, Train Loss: 3.9119012355804443 Eval Loss: 3.2577426195144654\n",
            "Running evaluation at iteration 21160...\n",
            "Iteration 21160, Train Loss: 3.7756617069244385 Eval Loss: 3.326869535446167\n",
            "Running evaluation at iteration 21170...\n",
            "Iteration 21170, Train Loss: 3.0298643112182617 Eval Loss: 3.0854339599609375\n",
            "Running evaluation at iteration 21180...\n",
            "Iteration 21180, Train Loss: 3.0448970794677734 Eval Loss: 3.1375314235687255\n",
            "Running evaluation at iteration 21190...\n",
            "Iteration 21190, Train Loss: 3.463658332824707 Eval Loss: 3.436256241798401\n",
            "Running evaluation at iteration 21200...\n",
            "Iteration 21200, Train Loss: 3.722316265106201 Eval Loss: 3.270061802864075\n",
            "Running evaluation at iteration 21210...\n",
            "Iteration 21210, Train Loss: 3.0303869247436523 Eval Loss: 3.502331852912903\n",
            "Running evaluation at iteration 21220...\n",
            "Iteration 21220, Train Loss: 3.08958101272583 Eval Loss: 3.3485973834991456\n",
            "Running evaluation at iteration 21230...\n",
            "Iteration 21230, Train Loss: 3.011113166809082 Eval Loss: 3.4809029579162596\n",
            "Running evaluation at iteration 21240...\n",
            "Iteration 21240, Train Loss: 3.8664391040802 Eval Loss: 3.357118344306946\n",
            "Running evaluation at iteration 21250...\n",
            "Iteration 21250, Train Loss: 3.832937240600586 Eval Loss: 3.1960226774215696\n",
            "Running evaluation at iteration 21260...\n",
            "Iteration 21260, Train Loss: 3.3337950706481934 Eval Loss: 3.3570096492767334\n",
            "Running evaluation at iteration 21270...\n",
            "Iteration 21270, Train Loss: 2.802622079849243 Eval Loss: 3.3081685304641724\n",
            "Running evaluation at iteration 21280...\n",
            "Iteration 21280, Train Loss: 3.7473304271698 Eval Loss: 3.1338414430618284\n",
            "Running evaluation at iteration 21290...\n",
            "Iteration 21290, Train Loss: 3.8941333293914795 Eval Loss: 3.3394218921661376\n",
            "Running evaluation at iteration 21300...\n",
            "Iteration 21300, Train Loss: 2.9458706378936768 Eval Loss: 3.4250308990478517\n",
            "Running evaluation at iteration 21310...\n",
            "Iteration 21310, Train Loss: 3.8221988677978516 Eval Loss: 3.336525297164917\n",
            "Running evaluation at iteration 21320...\n",
            "Iteration 21320, Train Loss: 3.294983148574829 Eval Loss: 3.5641818046569824\n",
            "Running evaluation at iteration 21330...\n",
            "Iteration 21330, Train Loss: 3.7174551486968994 Eval Loss: 3.3379411458969117\n",
            "Running evaluation at iteration 21340...\n",
            "Iteration 21340, Train Loss: 3.0901083946228027 Eval Loss: 3.5340011358261108\n",
            "Running evaluation at iteration 21350...\n",
            "Iteration 21350, Train Loss: 3.1311593055725098 Eval Loss: 3.2207864999771116\n",
            "Running evaluation at iteration 21360...\n",
            "Iteration 21360, Train Loss: 3.6319963932037354 Eval Loss: 3.3227659702301025\n",
            "Running evaluation at iteration 21370...\n",
            "Iteration 21370, Train Loss: 3.115788459777832 Eval Loss: 3.5452157258987427\n",
            "Running evaluation at iteration 21380...\n",
            "Iteration 21380, Train Loss: 3.7182161808013916 Eval Loss: 3.5099467039108276\n",
            "Running evaluation at iteration 21390...\n",
            "Iteration 21390, Train Loss: 3.3507721424102783 Eval Loss: 3.353451442718506\n",
            "Running evaluation at iteration 21400...\n",
            "Iteration 21400, Train Loss: 2.749715566635132 Eval Loss: 3.3607298851013185\n",
            "Running evaluation at iteration 21410...\n",
            "Iteration 21410, Train Loss: 3.1307733058929443 Eval Loss: 3.221480917930603\n",
            "Running evaluation at iteration 21420...\n",
            "Iteration 21420, Train Loss: 3.488837957382202 Eval Loss: 3.225496768951416\n",
            "Running evaluation at iteration 21430...\n",
            "Iteration 21430, Train Loss: 3.5986251831054688 Eval Loss: 3.3955586671829225\n",
            "Running evaluation at iteration 21440...\n",
            "Iteration 21440, Train Loss: 3.46574068069458 Eval Loss: 3.2477587461471558\n",
            "Running evaluation at iteration 21450...\n",
            "Iteration 21450, Train Loss: 3.4590063095092773 Eval Loss: 3.35575852394104\n",
            "Running evaluation at iteration 21460...\n",
            "Iteration 21460, Train Loss: 3.9366681575775146 Eval Loss: 3.545352602005005\n",
            "Running evaluation at iteration 21470...\n",
            "Iteration 21470, Train Loss: 3.1746087074279785 Eval Loss: 3.493478870391846\n",
            "Running evaluation at iteration 21480...\n",
            "Iteration 21480, Train Loss: 3.485572576522827 Eval Loss: 3.325416350364685\n",
            "Running evaluation at iteration 21490...\n",
            "Iteration 21490, Train Loss: 2.5501701831817627 Eval Loss: 3.3285841226577757\n",
            "Running evaluation at iteration 21500...\n",
            "Iteration 21500, Train Loss: 4.239087104797363 Eval Loss: 3.3060444593429565\n",
            "Running evaluation at iteration 21510...\n",
            "Iteration 21510, Train Loss: 3.6164252758026123 Eval Loss: 3.3935844898223877\n",
            "Running evaluation at iteration 21520...\n",
            "Iteration 21520, Train Loss: 3.059218406677246 Eval Loss: 3.3570667266845704\n",
            "Running evaluation at iteration 21530...\n",
            "Iteration 21530, Train Loss: 3.31764554977417 Eval Loss: 3.513282036781311\n",
            "Running evaluation at iteration 21540...\n",
            "Iteration 21540, Train Loss: 3.816282272338867 Eval Loss: 3.1044854640960695\n",
            "Running evaluation at iteration 21550...\n",
            "Iteration 21550, Train Loss: 3.147381544113159 Eval Loss: 3.3962889432907106\n",
            "Running evaluation at iteration 21560...\n",
            "Iteration 21560, Train Loss: 3.4744460582733154 Eval Loss: 3.245150327682495\n",
            "Running evaluation at iteration 21570...\n",
            "Iteration 21570, Train Loss: 3.8140921592712402 Eval Loss: 3.1971682786941527\n",
            "Running evaluation at iteration 21580...\n",
            "Iteration 21580, Train Loss: 3.4754886627197266 Eval Loss: 3.375153398513794\n",
            "Running evaluation at iteration 21590...\n",
            "Iteration 21590, Train Loss: 2.897779941558838 Eval Loss: 3.478338193893433\n",
            "Running evaluation at iteration 21600...\n",
            "Iteration 21600, Train Loss: 3.7635583877563477 Eval Loss: 3.2812593460083006\n",
            "Running evaluation at iteration 21610...\n",
            "Iteration 21610, Train Loss: 3.4936108589172363 Eval Loss: 3.125937294960022\n",
            "Running evaluation at iteration 21620...\n",
            "Iteration 21620, Train Loss: 3.197908639907837 Eval Loss: 3.4153901815414427\n",
            "Running evaluation at iteration 21630...\n",
            "Iteration 21630, Train Loss: 2.6980679035186768 Eval Loss: 3.343675637245178\n",
            "Running evaluation at iteration 21640...\n",
            "Iteration 21640, Train Loss: 3.2354671955108643 Eval Loss: 3.1708072662353515\n",
            "Running evaluation at iteration 21650...\n",
            "Iteration 21650, Train Loss: 2.9035696983337402 Eval Loss: 3.4594003677368166\n",
            "Running evaluation at iteration 21660...\n",
            "Iteration 21660, Train Loss: 3.8457748889923096 Eval Loss: 3.3604392290115355\n",
            "Running evaluation at iteration 21670...\n",
            "Iteration 21670, Train Loss: 3.6742682456970215 Eval Loss: 3.5377762794494627\n",
            "Running evaluation at iteration 21680...\n",
            "Iteration 21680, Train Loss: 3.232456922531128 Eval Loss: 3.5537113428115843\n",
            "Running evaluation at iteration 21690...\n",
            "Iteration 21690, Train Loss: 3.363112688064575 Eval Loss: 3.267308211326599\n",
            "Running evaluation at iteration 21700...\n",
            "Iteration 21700, Train Loss: 3.8290841579437256 Eval Loss: 3.2742274045944213\n",
            "Running evaluation at iteration 21710...\n",
            "Iteration 21710, Train Loss: 3.5569446086883545 Eval Loss: 3.1380691528320312\n",
            "Running evaluation at iteration 21720...\n",
            "Iteration 21720, Train Loss: 3.9032955169677734 Eval Loss: 3.4044260740280152\n",
            "Running evaluation at iteration 21730...\n",
            "Iteration 21730, Train Loss: 3.662400484085083 Eval Loss: 3.3210777759552004\n",
            "Running evaluation at iteration 21740...\n",
            "Iteration 21740, Train Loss: 3.386214017868042 Eval Loss: 3.390579676628113\n",
            "Running evaluation at iteration 21750...\n",
            "Iteration 21750, Train Loss: 3.261500835418701 Eval Loss: 3.366647481918335\n",
            "Running evaluation at iteration 21760...\n",
            "Iteration 21760, Train Loss: 3.241793155670166 Eval Loss: 3.3401124238967896\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-f7ebce97d877>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minigpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/ECE_239AS.2/Project3_skeleton/train.py\u001b[0m in \u001b[0;36msolver\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# Optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Record the loss value for logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_graph_capture_health_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_cuda_graph_capture_health_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compiling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_built\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         ):\n\u001b[1;32m    436\u001b[0m             \u001b[0mcapturing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_current_stream_capturing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mis_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# be initialized. This uses the CUDA Runtime API `cudaGetDeviceCount` which in turn initializes the CUDA Driver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# API via `cuInit`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDeviceCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "solver(model_name=\"minigpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WznureI8iQ3"
      },
      "source": [
        "### Train and Valid Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "img1 = mpimg.imread('/content/drive/MyDrive/ECE_239AS.2/Project3_skeleton/Graphs/MiniGPT_TrainingLoss.png')\n",
        "plt.figure()\n",
        "plt.imshow(img1)\n",
        "plt.axis('off')     # hide axes\n",
        "plt.title('Training Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.show()\n",
        "\n",
        "img2 = mpimg.imread('/content/drive/MyDrive/ECE_239AS.2/Project3_skeleton/Graphs/MiniGPT_EvalLoss.png')\n",
        "plt.figure()\n",
        "plt.imshow(img2)\n",
        "plt.axis('off')\n",
        "plt.title('Validation Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "msVGo3i7M8vj",
        "outputId": "20ad5040-fad2-4339-94bd-807abe405d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj-B6T8w8iQ3"
      },
      "source": [
        "### Generation (5 points)\n",
        "\n",
        "\n",
        "Perform generation with the MiniGPT model that you trained. After that, copy over the generation function you used for the Bigram model and generate a mini story using the same seed sentence.\n",
        "\n",
        "    `\"once upon a time\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OADr6-6n8iQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa41037d-e3b8-49f4-e1b4-f8cb3ab70307"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# TODO: Specify the path to your trained model\n",
        "model_path = '/content/drive/MyDrive/ECE_239AS.2/Project3_skeleton/models/minigpt/mini_model_checkpoint_20000.pt'\n",
        "model = MiniGPT(MiniGPTConfig)\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhf3GKWI8iQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8affff92-ae25-4236-a210-d60dfbdf4114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating text starting with: torch.Size([4])\n",
            "Once upon a time, there was a little girl named Tim. Timmy loved to see Freddie on her eyes. One day, she heard a everywhere he swam, and the bird came to asleep.\n",
            "But when the cat cleaned toys and her dolls were having a forgot to the candy couldn't grow in the storm.\n",
            "Lily and Sam back to her mom, but the storm was there for a big new rainbow. Her mom said, \"Don't worry, can't fight at my toy!\"\n",
            "Lily was the street, she saw a white tiger too.\" \n",
            "Timmy felt sad and sad and the bird pushing her bed with her favorite poured onto the rainbow under the forest. It was a new dress with many their important house. He put up of the trunk off the cloth with his friend who loved to walk in the delicious ingredients in sorts of her house. She asked her friend and didn't drive and started to ruin her arm. She grabbed his hand and opened their fence. Sue would\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "gen_sent = \"Once upon a time\"\n",
        "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
        "print(\"Generating text starting with:\", gen_tokens.shape)\n",
        "gen_tokens = gen_tokens.to(device)\n",
        "model.eval()\n",
        "print(\n",
        "    tokenizer.decode(\n",
        "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWDSSsgH8iQ4"
      },
      "source": [
        "Please answer the following questions.\n",
        "\n",
        "1. What can we say about the generated text in terms of grammar and coherence?\n",
        "\n",
        "Many individual phrases are well‐formed. For example, “Once upon a time, there was a little girl named Tim.” “Her mom said, ‘Don’t worry, can’t fight at my toy!’”. This shows the transformer has learned basic word ordering and subject–verb pairing. While it starts like a fairy‐tale, the story quickly veers into odd sequences which aren't coherent. There’s no sustained plot arc or logical cause-and-effect and there are repeated words with occasional agreement mistakes. These point to limitations in modeling longer dependencies and rich semantics.\n",
        "\n",
        "2. If the model is scaled with more parameters do you expect the GPT model to get substantially better? Why or why not?\n",
        "\n",
        "Larger models can in principle learn subtler syntactic and semantic patterns, reducing errors like misplaced prepositions or broken noun phrases. They can also attend over longer contexts more effectively which helps maintain narrative coherence over dozens of tokens. Simply increasing parameters won’t fix everything. You also need enough training data to fit those extra weights, and a longer context window if you want the model to remember earlier plot points. Without those, a scaled-up model may still “forget” characters introduced many sentences ago or hallucinate implausible actions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny-4H7Lt8iQ4"
      },
      "source": [
        "### Scaling up the model (5 points)\n",
        "\n",
        "To show that scale indeed will help the model learn we have trained a scaled up version of the model you just implemented. We will load the weights of this model and generate a mini story using the same seed sentence. Note that if you have implemented the model correctly just scaling the parameters and adding a few bells and whistles to the training script will results in a model like the one we will load now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zNzCCGS98iQ4"
      },
      "outputs": [],
      "source": [
        "from model import MiniGPT\n",
        "from config import MiniGPTConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xoPvdZbC8iQ4"
      },
      "outputs": [],
      "source": [
        "path_to_trained_model = \"/content/drive/MyDrive/ECE_239AS.2/Project3_skeleton/pretrained_models/MiniGPT/best_train_loss_checkpoint.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WreHORZ38iQ4"
      },
      "outputs": [],
      "source": [
        "ckpt = torch.load(path_to_trained_model, map_location=device) # remove map location if using GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HFM4C1ue8iQ4"
      },
      "outputs": [],
      "source": [
        "# Set the configs for scaled model\n",
        "MiniGPTConfig.context_length = 512\n",
        "MiniGPTConfig.embed_dim = 256\n",
        "MiniGPTConfig.num_heads = 16\n",
        "MiniGPTConfig.num_layers = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vAmcykwP8iQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa53e85-7989-4e6a-9a38-4b7d4ead5352"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Load model from checkpoint\n",
        "model = MiniGPT(MiniGPTConfig)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Dl_rI0fe8iQ4"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8UGLFsqp8iQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceaefaeb-98e4-4c61-e741-4e782ba451a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating text starting with: torch.Size([4])\n",
            "Once upon a time, there was a little girl named Lily. She loved to wear her favorite dress and play outside with her friends. One day, she saw a butterfly and wanted to catch it. But she didn't have a net in her hand. \n",
            "Lily asked her friend Timmy if he wanted to help her catch the butterfly. Timmy was happy to help and together they caught the butterfly. Lily was so happy that she realized something important was important to her friend. \n",
            "From that day on, Lily and Timmy always looked for yummy things that would fit in their pockets. They knew that catching bugs too much can make them fly better and help them fly safely.Once upon a time, there was a big elephant. He liked to swim in the lake but it was too dirty. He ran to his friend, the elephant, and told him about a good swim. The elephant said, \"I can do it myself to swim in this clean lake.\" The elephant was happy and jumped\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "gen_sent = \"Once upon a time\"\n",
        "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
        "print(\"Generating text starting with:\", gen_tokens.shape)\n",
        "gen_tokens = gen_tokens.to(device)\n",
        "model.eval()\n",
        "print(\n",
        "    tokenizer.decode(\n",
        "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaaDhptB8iQ4"
      },
      "source": [
        "## Bonus (5 points)\n",
        "\n",
        "The following are some open ended questions that you can attempt if you have time. Feel free to propose your own as well if you have an interesting idea.\n",
        "\n",
        "1. The model we have implemented is a decoder only model. Can you implement the encoder part as well? This should not be too hard to do since most of the layers are already implemented.\n",
        "2. What are some improvements we can add to the training script to make training more efficient and faster? Can you concretely show that the improvements you made help in training the model better?\n",
        "3. Can you implement a beam search decoder to generate the text instead of greedy decoding? Does this help in generating better text?\n",
        "4. Can you further optimize the model architecture? For example, can you implement [Multi Query Attention](https://arxiv.org/abs/1911.02150) or [Grouped Query Attention](https://arxiv.org/pdf/2305.13245) to improve the model performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myTgmAu58iQ4"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}